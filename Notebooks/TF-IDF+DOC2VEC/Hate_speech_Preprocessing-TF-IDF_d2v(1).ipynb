{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1\">TF-IDF</a></span></li><li><span><a href=\"#In-86-start-here\" data-toc-modified-id=\"In-86-start-here-2\">In 86 start here</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3\">Word2Vec</a></span></li><li><span><a href=\"#Doc2Vec\" data-toc-modified-id=\"Doc2Vec-4\">Doc2Vec</a></span></li><li><span><a href=\"#Ablation\" data-toc-modified-id=\"Ablation-5\">Ablation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Splitting-the-data\" data-toc-modified-id=\"Splitting-the-data-5.1\">Splitting the data</a></span></li></ul></li><li><span><a href=\"#Training-Logistic-Regression\" data-toc-modified-id=\"Training-Logistic-Regression-6\">Training Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-SVC\" data-toc-modified-id=\"Training-SVC-6.1\">Training SVC</a></span></li><li><span><a href=\"#Training-DescisionTree\" data-toc-modified-id=\"Training-DescisionTree-6.2\">Training DescisionTree</a></span></li><li><span><a href=\"#Training-RandomForest\" data-toc-modified-id=\"Training-RandomForest-6.3\">Training RandomForest</a></span></li><li><span><a href=\"#Training-Multilayer-Perceptron\" data-toc-modified-id=\"Training-Multilayer-Perceptron-6.4\">Training Multilayer Perceptron</a></span></li><li><span><a href=\"#Training-Gradient-Boosting\" data-toc-modified-id=\"Training-Gradient-Boosting-6.5\">Training Gradient Boosting</a></span></li></ul></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-7\">Naive Bayes</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:35.582744Z",
     "start_time": "2022-04-19T12:29:34.302518Z"
    },
    "id": "qZmuEjZft6HC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:35.595845Z",
     "start_time": "2022-04-19T12:29:35.592664Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:35.767160Z",
     "start_time": "2022-04-19T12:29:35.604581Z"
    },
    "id": "2g-tlAV2xeg4"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/coea/Hatespeech/labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:35.802403Z",
     "start_time": "2022-04-19T12:29:35.795413Z"
    },
    "id": "DiarcwG6MxR9"
   },
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0',axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:35.853426Z",
     "start_time": "2022-04-19T12:29:35.833920Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "id": "7kT-aKnpzmz-",
    "outputId": "3be1c8ed-c133-4d9d-b2e7-cafd8bf75fde"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:35.884137Z",
     "start_time": "2022-04-19T12:29:35.878556Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GSFMzROzsym",
    "outputId": "11572627-7167-48d5-ab0f-a7aa63c8f654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['count', 'hate_speech', 'offensive_language', 'neither', 'class',\n",
       "       'tweet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:35.963291Z",
     "start_time": "2022-04-19T12:29:35.909306Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "PPI6y2mU3P9S",
    "outputId": "2dd30c6e-1363-4dfc-b93a-7d3b16970c9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.243473</td>\n",
       "      <td>0.280515</td>\n",
       "      <td>2.413711</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>1.110277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.883060</td>\n",
       "      <td>0.631851</td>\n",
       "      <td>1.399459</td>\n",
       "      <td>1.113299</td>\n",
       "      <td>0.462089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count   hate_speech  offensive_language       neither  \\\n",
       "count  24783.000000  24783.000000        24783.000000  24783.000000   \n",
       "mean       3.243473      0.280515            2.413711      0.549247   \n",
       "std        0.883060      0.631851            1.399459      1.113299   \n",
       "min        3.000000      0.000000            0.000000      0.000000   \n",
       "25%        3.000000      0.000000            2.000000      0.000000   \n",
       "50%        3.000000      0.000000            3.000000      0.000000   \n",
       "75%        3.000000      0.000000            3.000000      0.000000   \n",
       "max        9.000000      7.000000            9.000000      9.000000   \n",
       "\n",
       "              class  \n",
       "count  24783.000000  \n",
       "mean       1.110277  \n",
       "std        0.462089  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        2.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:36.152636Z",
     "start_time": "2022-04-19T12:29:35.991595Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "zoaS7B0m3fug",
    "outputId": "6d8f8e97-aebe-4a09-ce79-8c25f129d899"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">hate_speech</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">neither</th>\n",
       "      <th colspan=\"8\" halign=\"left\">offensive_language</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>3.108392</td>\n",
       "      <td>0.648084</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>2.256643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>0.755944</td>\n",
       "      <td>0.487653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19190.0</td>\n",
       "      <td>3.268890</td>\n",
       "      <td>0.923024</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19190.0</td>\n",
       "      <td>0.180459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19190.0</td>\n",
       "      <td>3.003544</td>\n",
       "      <td>0.954097</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4163.0</td>\n",
       "      <td>3.172712</td>\n",
       "      <td>0.746097</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4163.0</td>\n",
       "      <td>0.062935</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4163.0</td>\n",
       "      <td>0.264233</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         count                                              hate_speech  \\\n",
       "         count      mean       std  min  25%  50%  75%  max       count   \n",
       "class                                                                     \n",
       "0       1430.0  3.108392  0.648084  3.0  3.0  3.0  3.0  9.0      1430.0   \n",
       "1      19190.0  3.268890  0.923024  3.0  3.0  3.0  3.0  9.0     19190.0   \n",
       "2       4163.0  3.172712  0.746097  3.0  3.0  3.0  3.0  9.0      4163.0   \n",
       "\n",
       "                ...  neither      offensive_language                           \\\n",
       "           mean ...      75%  max              count      mean       std  min   \n",
       "class           ...                                                             \n",
       "0      2.256643 ...      0.0  4.0             1430.0  0.755944  0.487653  0.0   \n",
       "1      0.180459 ...      0.0  3.0            19190.0  3.003544  0.954097  2.0   \n",
       "2      0.062935 ...      3.0  9.0             4163.0  0.264233  0.461737  0.0   \n",
       "\n",
       "                           \n",
       "       25%  50%  75%  max  \n",
       "class                      \n",
       "0      0.0  1.0  1.0  4.0  \n",
       "1      3.0  3.0  3.0  9.0  \n",
       "2      0.0  0.0  1.0  4.0  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"class\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:36.168781Z",
     "start_time": "2022-04-19T12:29:36.155438Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlrXEEjfRpU-",
    "outputId": "b2d7f10c-544b-4008-c75f-b1610f707312"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1430"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.groupby(\"class\").describe()[\"class\"==0]\n",
    "sum(df[\"class\"]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:36.234104Z",
     "start_time": "2022-04-19T12:29:36.223232Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kd3x0b6kTR2C",
    "outputId": "0fd8fb36-a08a-4437-bc22-29dcb4bde58b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    19190\n",
       "2     4163\n",
       "0     1430\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:36.247088Z",
     "start_time": "2022-04-19T12:29:36.236769Z"
    },
    "id": "aQWMCpzRFbA4"
   },
   "outputs": [],
   "source": [
    "def percentage_class(idx):\n",
    "    Percentage = 100 * float(sum(df[\"class\"]==idx))/float(len(df[\"class\"]))\n",
    "    return Percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:36.262577Z",
     "start_time": "2022-04-19T12:29:36.249693Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIY1NFjBRJKU",
    "outputId": "ed2113a7-0550-437f-d8ba-09c6b50e785b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the percenatge of '0' class 5.770084332001775\n",
      "the percenatge of '1' class 77.43211072105879\n",
      "the percenatge of '2' class 16.797804946939433\n"
     ]
    }
   ],
   "source": [
    "print(\"the percenatge of '0' class\", percentage_class(0))\n",
    "print(\"the percenatge of '1' class\", percentage_class(1))\n",
    "print(\"the percenatge of '2' class\", percentage_class(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:36.543328Z",
     "start_time": "2022-04-19T12:29:36.320839Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "GtRcXmguS0zE",
    "outputId": "e97f821f-5f7a-40f2-c045-039d5561314d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2d1823a588>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d1823f6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ploting percentage of each class\n",
    "df[\"class\"].value_counts().plot(kind='pie',autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHSLZ2oHJEfo"
   },
   "source": [
    "Finding out the Maximum length tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:36.595978Z",
     "start_time": "2022-04-19T12:29:36.546664Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "oHh5QglEIwlt",
    "outputId": "06a1c4ef-ae14-4b03-b582-15ba202a90d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['tweet'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:37.016316Z",
     "start_time": "2022-04-19T12:29:36.598966Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "0mNOMylQKD8C",
    "outputId": "cd1e1db0-f49d-40a1-a43d-6e83cfd76f20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2d2876f550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d181e67b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['length'].plot(bins=50, kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxCU9bWANEWN"
   },
   "source": [
    "#Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTmX_141EwZR"
   },
   "source": [
    "Here copying the data from **df** to **dataF** and creating new variable with same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:38.441902Z",
     "start_time": "2022-04-19T12:29:38.435336Z"
    },
    "id": "a_fg58OnRuMx"
   },
   "outputs": [],
   "source": [
    "#copying data from original variable to another varible.\n",
    "\n",
    "dataF = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:38.944147Z",
     "start_time": "2022-04-19T12:29:38.929060Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "Y8nVur4mR5Hi",
    "outputId": "2b894f96-6e1a-4a66-e7bf-0353f5d0a86d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing top 3 rows in the data set\n",
    "\n",
    "dataF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:39.045896Z",
     "start_time": "2022-04-19T12:29:39.023112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:39.498092Z",
     "start_time": "2022-04-19T12:29:39.105079Z"
    },
    "id": "C13nkq6e5c4P"
   },
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords            ## it will check stop words present in data\n",
    "from nltk.stem.porter import PorterStemmer   ## it will use for stemmming the words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T07:10:38.244596Z",
     "start_time": "2022-04-16T07:10:38.240725Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T07:10:39.247702Z",
     "start_time": "2022-04-16T07:10:38.358234Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1ee3083fd02b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m#For expanding Contractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "def con(words):\n",
    "    return [contractions.fix(word) for word in words.split()]    #For expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:30.271965Z",
     "start_time": "2022-03-05T05:04:30.267829Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataF['Contractions']=dataF['tweet'].apply(lambda x: con(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:30.995790Z",
     "start_time": "2022-03-05T05:04:30.977843Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:31.708535Z",
     "start_time": "2022-03-05T05:04:31.705355Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install TextBlob\n",
    "#from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.249394Z",
     "start_time": "2022-03-05T05:04:32.245934Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gingerit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.391937Z",
     "start_time": "2022-03-05T05:04:32.388591Z"
    }
   },
   "outputs": [],
   "source": [
    "#from gingerit.gingerit import GingerIt\n",
    "#parser = GingerIt()\n",
    "#tweet=parser.parse(dataF['tweet'][23])\n",
    "#tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.503268Z",
     "start_time": "2022-03-05T05:04:32.500517Z"
    }
   },
   "outputs": [],
   "source": [
    "#blob=TextBlob(dataF['tweet'][23])\n",
    "#blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:33.089007Z",
     "start_time": "2022-03-05T05:04:33.079920Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Expanding whatsapp language slangs\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"./HateSpeechNew/PreProcessing/sms_slang_translator-master/slang.txt\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if str(_str).upper() == row[0]:\n",
    "                    # If match found replace it with its appropriate phrase in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    return ' '.join(user_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.087982Z",
     "start_time": "2022-03-05T05:04:33.545921Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataF['ExpandedSlangs']=dataF['tweet'].apply(lambda x: translator(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.127335Z",
     "start_time": "2022-03-05T05:05:28.118138Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.175036Z",
     "start_time": "2022-03-05T05:05:28.152537Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'con' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-20407348bd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ExpandedSlangs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Contractions for expanded slangs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3192\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-20407348bd53>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ExpandedSlangs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Contractions for expanded slangs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'con' is not defined"
     ]
    }
   ],
   "source": [
    "dataF['Preprocessed_Initial']=dataF['ExpandedSlangs'].apply(lambda x: con(x)) #Contractions for expanded slangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:03.502948Z",
     "start_time": "2022-03-05T05:07:03.483677Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:03.996685Z",
     "start_time": "2022-03-05T05:07:03.993840Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJO5rARp8Bb2",
    "outputId": "fc26ef1f-29b7-4db9-db52-9bcc2a77cbd2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.121200Z",
     "start_time": "2022-03-05T05:07:04.118575Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctTSRXQR0vO4",
    "outputId": "1a59d48e-acec-4839-8f8e-bf1fc8180a6b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.244346Z",
     "start_time": "2022-03-05T05:07:04.241803Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iG6koGQnpfOF",
    "outputId": "e9572c08-72e0-4cdb-a447-f4b9fc6d3fbe"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.382959Z",
     "start_time": "2022-03-05T05:07:04.375813Z"
    },
    "id": "6146Ji8QjJPt"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the tweets.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer,word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    #for text in sentence:\n",
    "    #  if text in ['html','www','http','https','rt']:\n",
    "    #sentence= [sentence.replace(w,'') for w in sentence if w in ['html','www','http','https','rt']]\n",
    "    sentence=sentence.replace('rt',\"\")\n",
    "    sentence=sentence.replace('www',\"\")\n",
    "    sentence=sentence.replace('http',\"\")\n",
    "    sentence=sentence.replace('https',\"\")\n",
    "    sentence=sentence.replace('html',\"\")\n",
    "    sentence=sentence.replace('*',\"\")\n",
    "    sentence=sentence.replace('#',\"\")\n",
    "    cleanr = re.compile('<.?>')\n",
    "    cleantext1 = re.sub(cleanr, '', sentence)\n",
    "    cleantext = re.sub('@[^\\s]+','',cleantext1)\n",
    "    #rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', cleantext)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    tokens = tokenizer.tokenize(rem_num)\n",
    "    #tokens = word_tokenize(rem_num)\n",
    "    filtered_words = [w for w in tokens if len(w) > 2] # if not w in stopwords.words('english')]\n",
    "    #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    #lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.605980Z",
     "start_time": "2022-03-05T05:07:04.477566Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnDBnHcyLRZk",
    "outputId": "c3638e8c-646c-4c09-b7e2-0a8fe26246e4",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Preprocessed_Initial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Preprocessed_Initial'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-a01c6738a8ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PreprocessedTweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#dataF['tweet_prepro'] = dataF['tweet'].apply(lambda x: preprocess(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#dataF['tweet_prepro'].tail()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Preprocessed_Initial'"
     ]
    }
   ],
   "source": [
    "dataF['PreprocessedTweet']=dataF['Preprocessed_Initial'].map(lambda s:preprocess(s))\n",
    "#dataF['tweet_prepro'] = dataF['tweet'].apply(lambda x: preprocess(x))\n",
    "#dataF['tweet_prepro'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:05.904217Z",
     "start_time": "2022-03-05T05:07:05.886888Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:33:06.476959Z",
     "start_time": "2022-03-05T09:33:06.472222Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split(\" \",text) \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.187016Z",
     "start_time": "2022-03-05T05:19:23.336Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['final_tweet_tokens'] = dataF['PreprocessedTweet'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.188320Z",
     "start_time": "2022-03-05T05:19:23.644Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.189637Z",
     "start_time": "2022-03-05T05:19:24.031Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF['PreprocessedTweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.190956Z",
     "start_time": "2022-03-05T05:19:24.453Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.192175Z",
     "start_time": "2022-03-05T05:19:24.910Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "word_c = WordCloud(collocations=False,background_color='white',mode='RGB',scale=4).generate(str(dataF['PreprocessedTweet']))\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(18,8),facecolor='w')\n",
    "plt.imshow(word_c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.193460Z",
     "start_time": "2022-03-05T05:19:25.528Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatizing(words):\n",
    "    lemmatizer =WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.194684Z",
     "start_time": "2022-03-05T05:19:25.996Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['LemmaWords']=dataF['final_tweet_tokens'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.195891Z",
     "start_time": "2022-03-05T05:19:26.615Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.197110Z",
     "start_time": "2022-03-05T05:19:26.972Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF['LemmaWords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.198387Z",
     "start_time": "2022-03-05T05:19:27.407Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.to_csv('Hate_spech_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.199640Z",
     "start_time": "2022-03-05T05:19:27.920Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.200844Z",
     "start_time": "2022-03-05T05:19:30.381Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.202042Z",
     "start_time": "2022-03-05T05:19:31.178Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.203266Z",
     "start_time": "2022-03-05T05:19:31.370Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['KerasTokens']=dataF['PreprocessedTweet'].apply(text_to_word_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.204501Z",
     "start_time": "2022-03-05T05:19:31.537Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.205742Z",
     "start_time": "2022-03-05T05:19:32.391Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "word_c = WordCloud(collocations=False,background_color='white',mode='RGB',scale=4).generate(str(dataF['LemmaWords']))\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(18,8),facecolor='w')\n",
    "plt.imshow(word_c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.206953Z",
     "start_time": "2022-03-05T05:19:32.619Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['Lemma_Preprocessed']=dataF['LemmaWords'].map(lambda s:preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.208149Z",
     "start_time": "2022-03-05T05:19:32.830Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T08:36:34.150352Z",
     "start_time": "2022-03-15T08:36:33.698741Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF = pd.read_csv(\"/home/coea/Hatespeech/preprocessing/Hate_spech_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T08:36:34.250170Z",
     "start_time": "2022-03-15T08:36:34.223693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "      <th>Preprocessed_Initial</th>\n",
       "      <th>PreprocessedTweet</th>\n",
       "      <th>final_tweet_tokens</th>\n",
       "      <th>LemmaWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>['!!!', 'RT', '@mayasolovely:', 'As', 'a', 'wo...</td>\n",
       "      <td>woman you should not complain about cleaning y...</td>\n",
       "      <td>['woman', 'you', 'should', 'not', 'complain', ...</td>\n",
       "      <td>['woman', 'you', 'should', 'not', 'complain', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "      <td>['!!!!!', 'RT', '@mleew17:', 'boy', 'That Is',...</td>\n",
       "      <td>boy that cold tyga down bad for cuffin that ho...</td>\n",
       "      <td>['boy', 'that', 'cold', 'tyga', 'down', 'bad',...</td>\n",
       "      <td>['boy', 'that', 'cold', 'tyga', 'down', 'bad',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "      <td>['!!!!!!!', 'RT', '@UrKindOfBrand', 'Dog', 'RT...</td>\n",
       "      <td>dog you ever fuck bitch and she sta cry you co...</td>\n",
       "      <td>['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...</td>\n",
       "      <td>['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>['!!!!!!!!!', 'RT', '@C_G_Anderson:', '@viva_b...</td>\n",
       "      <td>she look like tranny</td>\n",
       "      <td>['she', 'look', 'like', 'tranny']</td>\n",
       "      <td>['she', 'look', 'like', 'tranny']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>['!!!!!!!!!!!!!', 'RT', '@ShenikaRoberts:', 'T...</td>\n",
       "      <td>the shit you hear about might true might faker...</td>\n",
       "      <td>['the', 'shit', 'you', 'hear', 'about', 'might...</td>\n",
       "      <td>['the', 'shit', 'you', 'hear', 'about', 'might...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "\n",
       "                                Preprocessed_Initial  \\\n",
       "0  ['!!!', 'RT', '@mayasolovely:', 'As', 'a', 'wo...   \n",
       "1  ['!!!!!', 'RT', '@mleew17:', 'boy', 'That Is',...   \n",
       "2  ['!!!!!!!', 'RT', '@UrKindOfBrand', 'Dog', 'RT...   \n",
       "3  ['!!!!!!!!!', 'RT', '@C_G_Anderson:', '@viva_b...   \n",
       "4  ['!!!!!!!!!!!!!', 'RT', '@ShenikaRoberts:', 'T...   \n",
       "\n",
       "                                   PreprocessedTweet  \\\n",
       "0  woman you should not complain about cleaning y...   \n",
       "1  boy that cold tyga down bad for cuffin that ho...   \n",
       "2  dog you ever fuck bitch and she sta cry you co...   \n",
       "3                               she look like tranny   \n",
       "4  the shit you hear about might true might faker...   \n",
       "\n",
       "                                  final_tweet_tokens  \\\n",
       "0  ['woman', 'you', 'should', 'not', 'complain', ...   \n",
       "1  ['boy', 'that', 'cold', 'tyga', 'down', 'bad',...   \n",
       "2  ['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...   \n",
       "3                  ['she', 'look', 'like', 'tranny']   \n",
       "4  ['the', 'shit', 'you', 'hear', 'about', 'might...   \n",
       "\n",
       "                                          LemmaWords  \n",
       "0  ['woman', 'you', 'should', 'not', 'complain', ...  \n",
       "1  ['boy', 'that', 'cold', 'tyga', 'down', 'bad',...  \n",
       "2  ['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...  \n",
       "3                  ['she', 'look', 'like', 'tranny']  \n",
       "4  ['the', 'shit', 'you', 'hear', 'about', 'might...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:33:35.291436Z",
     "start_time": "2022-03-05T09:33:35.287464Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T08:36:39.761456Z",
     "start_time": "2022-03-15T08:36:38.002766Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T03:37:56.594816Z",
     "start_time": "2022-03-07T03:37:56.228628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 590)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=50, max_features=1000, stop_words=None,tokenizer=tokenize,analyzer='word')\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataF['PreprocessedTweet'].values.astype('U'))\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In 86 start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:07.726410Z",
     "start_time": "2022-04-19T12:30:04.336867Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.read_csv('/home/coea/Hatespeech/TF-IDF_features_l.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:07.882142Z",
     "start_time": "2022-04-19T12:30:07.872442Z"
    }
   },
   "outputs": [],
   "source": [
    "# tfidf_df = pd.DataFrame(tfidf_df.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:08.122871Z",
     "start_time": "2022-04-19T12:30:07.998438Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df.drop('class.1',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:08.277302Z",
     "start_time": "2022-04-19T12:30:08.268279Z"
    }
   },
   "outputs": [],
   "source": [
    "# tfidf_df = tfidf_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:08.390234Z",
     "start_time": "2022-04-19T12:30:08.382066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 590)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 1.06 s, total: 3min 34s\n",
      "Wall time: 17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3628394, 5058200)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_tweet = dataF['Lemma_Preprocessed'].apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            vector_size=500, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 32, # no.of cores\n",
    "            seed = 34\n",
    ") \n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(dataF['Lemma_Preprocessed']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 500)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 500)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 500 )\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pandas==0.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/dstuser/anaconda3/lib/python3.7/site-packages (4.32.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LabeledSentence' from 'gensim.models.doc2vec' (/home/dstuser/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-8d3be766ad86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#tqdm.pandas(desc=\"progress-bar\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabeledSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LabeledSentence' from 'gensim.models.doc2vec' (/home/dstuser/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "#tqdm.pandas(desc=\"progress-bar\") \n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(TaggedDocument(s, [\"tweet_\" + str(i)]))\n",
    "    return output\n",
    "\n",
    "labeled_tweets = add_label(tokenized_tweet) # label all the tweets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time \n",
    "model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for â€˜distributed memoryâ€™ model\n",
    "                                  dm_mean=1, # dm_mean = 1 for using mean of the context word vectors\n",
    "                                  vector_size=1000, # no. of desired features\n",
    "                                  window=5, # width of the context window                                  \n",
    "                                  negative=7, # if > 0 then negative sampling will be used\n",
    "                                  min_count=5, # Ignores all words with total frequency lower than 5.                                  \n",
    "                                  workers=32, # no. of cores                                  \n",
    "                                  alpha=0.1, # learning rate                                  \n",
    "                                  seed = 23, # for reproducibility\n",
    "                                 ) \n",
    "\n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n",
    "\n",
    "model_d2v.train(labeled_tweets, total_examples= len(dataF['Lemma_Preprocessed']), epochs=15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docvec_arrays = np.zeros((len(tokenized_tweet), 1000)) \n",
    "for i in range(len(dataF)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,1000))    \n",
    "\n",
    "docvec_df = pd.DataFrame(docvec_arrays) \n",
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:24.973154Z",
     "start_time": "2022-04-19T12:30:11.446563Z"
    }
   },
   "outputs": [],
   "source": [
    "docvec_df = pd.read_csv(\"/home/coea/Hatespeech/doc2vec_features_l.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:25.616047Z",
     "start_time": "2022-04-19T12:30:25.403898Z"
    }
   },
   "outputs": [],
   "source": [
    "docvec_df.drop('class.1',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:25.869931Z",
     "start_time": "2022-04-19T12:30:25.863317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:26.526381Z",
     "start_time": "2022-04-19T12:30:26.046912Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_d2v = pd.concat([tfidf_df,docvec_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:26.880223Z",
     "start_time": "2022-04-19T12:30:26.873438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 1590)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_d2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:29.126023Z",
     "start_time": "2022-04-19T12:30:27.243931Z"
    },
    "id": "53bWppiTiWGx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_d2v.values, dataF['class'].values, \\\n",
    "                                                    stratify = dataF['class'].values,shuffle = True,\\\n",
    "                                                    random_state=1,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:29.557051Z",
     "start_time": "2022-04-19T12:30:29.477003Z"
    },
    "id": "Obk37BzJsRwW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing necessary librarys machine learning algorthms\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# PNN algorithm not imported\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#importing all necessary metrics\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:30:29.918392Z",
     "start_time": "2022-04-19T12:30:29.909410Z"
    }
   },
   "outputs": [],
   "source": [
    "technique='TF_D2V'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:30:10.665545Z",
     "start_time": "2022-04-13T04:26:25.834590Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.893, Val_Acc: 0.886, est=0.888, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8930804922332055\n",
      "Test_macro_f1: 0.7182438924851229\n",
      "Fold: 2, acc=0.893, Val_Acc: 0.891, est=0.890, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8932822271535203\n",
      "Test_macro_f1: 0.7133514398369889\n",
      "Fold: 3, acc=0.894, Val_Acc: 0.893, est=0.888, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8944926366754085\n",
      "Test_macro_f1: 0.7257073053659443\n",
      "Fold: 4, acc=0.894, Val_Acc: 0.892, est=0.887, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8944926366754085\n",
      "Test_macro_f1: 0.717678266011732\n",
      "Fold: 5, acc=0.895, Val_Acc: 0.890, est=0.888, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8946943715957232\n",
      "Test_macro_f1: 0.7115323682419481\n",
      "Fold: 6, acc=0.894, Val_Acc: 0.896, est=0.887, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8938874319144644\n",
      "Test_macro_f1: 0.7154475766845767\n",
      "Fold: 7, acc=0.897, Val_Acc: 0.889, est=0.889, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.896913455719185\n",
      "Test_macro_f1: 0.7165350526173938\n",
      "Fold: 8, acc=0.895, Val_Acc: 0.897, est=0.889, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8952995763566673\n",
      "Test_macro_f1: 0.7185512668423361\n",
      "Fold: 9, acc=0.892, Val_Acc: 0.881, est=0.888, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8916683477910026\n",
      "Test_macro_f1: 0.7098491027796726\n",
      "Fold:10, acc=0.896, Val_Acc: 0.899, est=0.886, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8961065160379261\n",
      "Test_macro_f1: 0.720148352453852\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    LR_model = LogisticRegression(random_state=1,C=100)\n",
    "    \n",
    "    # define gridsearch CV\n",
    "    p_grid = {'penalty': ['l1', 'l2']}\n",
    "    \n",
    "    clf = GridSearchCV(estimator = LR_model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:31:34.412530Z",
     "start_time": "2022-04-13T04:31:34.389691Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.891 +/- 0.005\n",
      "\n",
      "CV accuracy: 0.894 +/- 0.001\n",
      "\n",
      "CV micro F1: 0.894 +/- 0.001\n",
      "\n",
      "CV macro F1: 0.717 +/- 0.004\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.541 +/- 0.013\n",
      "\n",
      "CV Class : Hate speech Recall: 0.295 +/- 0.013\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.382 +/- 0.012\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.926 +/- 0.002\n",
      "\n",
      "CV Class : Offensive language Recall: 0.949 +/- 0.002\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.938 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.814 +/- 0.002\n",
      "\n",
      "CV Class : Neither Recall: 0.848 +/- 0.006\n",
      "\n",
      "CV Class : Neither F1 score: 0.831 +/- 0.003\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:31:40.354858Z",
     "start_time": "2022-04-13T04:31:40.320228Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_D2V_LR.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.893080</td>\n",
       "      <td>0.893080</td>\n",
       "      <td>0.718244</td>\n",
       "      <td>0.550633</td>\n",
       "      <td>0.304196</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.923877</td>\n",
       "      <td>0.948671</td>\n",
       "      <td>0.936110</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.923877</td>\n",
       "      <td>0.839136</td>\n",
       "      <td>0.826730</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.893282</td>\n",
       "      <td>0.893282</td>\n",
       "      <td>0.713351</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.290210</td>\n",
       "      <td>0.373034</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.926624</td>\n",
       "      <td>0.947629</td>\n",
       "      <td>0.937009</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.926624</td>\n",
       "      <td>0.849940</td>\n",
       "      <td>0.830012</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.894493</td>\n",
       "      <td>0.894493</td>\n",
       "      <td>0.725707</td>\n",
       "      <td>0.537143</td>\n",
       "      <td>0.328671</td>\n",
       "      <td>0.407809</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.928407</td>\n",
       "      <td>0.946066</td>\n",
       "      <td>0.937153</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.928407</td>\n",
       "      <td>0.851140</td>\n",
       "      <td>0.832160</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.894493</td>\n",
       "      <td>0.894493</td>\n",
       "      <td>0.717678</td>\n",
       "      <td>0.544872</td>\n",
       "      <td>0.297203</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.927134</td>\n",
       "      <td>0.948150</td>\n",
       "      <td>0.937524</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.927134</td>\n",
       "      <td>0.852341</td>\n",
       "      <td>0.830895</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.894694</td>\n",
       "      <td>0.894694</td>\n",
       "      <td>0.711532</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.279720</td>\n",
       "      <td>0.365297</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.950495</td>\n",
       "      <td>0.938513</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.830787</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.893080  0.893080  0.718244               0.550633            0.304196   \n",
       "1  0.893282  0.893282  0.713351               0.522013            0.290210   \n",
       "2  0.894493  0.894493  0.725707               0.537143            0.328671   \n",
       "3  0.894493  0.894493  0.717678               0.544872            0.297203   \n",
       "4  0.894694  0.894694  0.711532               0.526316            0.279720   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.391892                286.0                      0.923877   \n",
       "1             0.373034                286.0                      0.926624   \n",
       "2             0.407809                286.0                      0.928407   \n",
       "3             0.384615                286.0                      0.927134   \n",
       "4             0.365297                286.0                      0.926829   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.948671                    0.936110   \n",
       "1                   0.947629                    0.937009   \n",
       "2                   0.946066                    0.937153   \n",
       "3                   0.948150                    0.937524   \n",
       "4                   0.950495                    0.938513   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.923877        0.839136   \n",
       "1                      3838.0           0.926624        0.849940   \n",
       "2                      3838.0           0.928407        0.851140   \n",
       "3                      3838.0           0.927134        0.852341   \n",
       "4                      3838.0           0.926829        0.848739   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.826730            833.0  \n",
       "1         0.830012            833.0  \n",
       "2         0.832160            833.0  \n",
       "3         0.830895            833.0  \n",
       "4         0.830787            833.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"LR.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHwC6nvnrbkq"
   },
   "source": [
    "## Training SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T20:59:49.612379Z",
     "start_time": "2022-04-19T12:31:14.156263Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.896, Val_Acc: 0.896, est=0.898, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.8961065160379261\n",
      "Test_macro_f1: 0.6568521346588759\n",
      "Fold: 2, acc=0.898, Val_Acc: 0.896, est=0.897, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.8975186604801291\n",
      "Test_macro_f1: 0.6512163381434483\n",
      "Fold: 3, acc=0.898, Val_Acc: 0.894, est=0.897, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.8977203954004438\n",
      "Test_macro_f1: 0.6554436301119753\n",
      "Fold: 4, acc=0.900, Val_Acc: 0.900, est=0.897, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.8999394795239056\n",
      "Test_macro_f1: 0.667397735405225\n",
      "Fold: 5, acc=0.899, Val_Acc: 0.898, est=0.898, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.8987290700020173\n",
      "Test_macro_f1: 0.6618494275358603\n",
      "Fold: 6, acc=0.900, Val_Acc: 0.900, est=0.898, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.8999394795239056\n",
      "Test_macro_f1: 0.6628696753064464\n",
      "Fold: 7, acc=0.898, Val_Acc: 0.894, est=0.898, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.8981238652410732\n",
      "Test_macro_f1: 0.6536706416360379\n",
      "Fold: 8, acc=0.898, Val_Acc: 0.903, est=0.896, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.898325600161388\n",
      "Test_macro_f1: 0.6595409779647791\n",
      "Fold: 9, acc=0.898, Val_Acc: 0.895, est=0.897, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.898325600161388\n",
      "Test_macro_f1: 0.6613208068168159\n",
      "Fold:10, acc=0.899, Val_Acc: 0.912, est=0.895, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.8987290700020173\n",
      "Test_macro_f1: 0.662866474700088\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = SVC(random_state=42,max_iter=-1)\n",
    "    param_range = [0.1,0.5,1.0]\n",
    "    \n",
    "    p_grid =  {'C': param_range, \n",
    "               #'loss':['hinge', 'squared_hinge'],\n",
    "               'kernel':['linear','poly', 'rbf', 'sigmoid']\n",
    "               }   \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T20:59:49.986421Z",
     "start_time": "2022-04-19T20:59:49.962216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.899 +/- 0.005\n",
      "\n",
      "CV accuracy: 0.898 +/- 0.001\n",
      "\n",
      "CV micro F1: 0.898 +/- 0.001\n",
      "\n",
      "CV macro F1: 0.659 +/- 0.005\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.457 +/- 0.030\n",
      "\n",
      "CV Class : Hate speech Recall: 0.115 +/- 0.009\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.183 +/- 0.013\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.926 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Recall: 0.957 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.941 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.814 +/- 0.003\n",
      "\n",
      "CV Class : Neither Recall: 0.897 +/- 0.005\n",
      "\n",
      "CV Class : Neither F1 score: 0.853 +/- 0.002\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T20:59:50.354203Z",
     "start_time": "2022-04-19T20:59:50.307925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_D2V_SVM.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.896107</td>\n",
       "      <td>0.896107</td>\n",
       "      <td>0.656852</td>\n",
       "      <td>0.417722</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.180822</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.923968</td>\n",
       "      <td>0.956227</td>\n",
       "      <td>0.939821</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.923968</td>\n",
       "      <td>0.887155</td>\n",
       "      <td>0.849914</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.897519</td>\n",
       "      <td>0.897519</td>\n",
       "      <td>0.651216</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.097902</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.925599</td>\n",
       "      <td>0.956227</td>\n",
       "      <td>0.940664</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.925599</td>\n",
       "      <td>0.901561</td>\n",
       "      <td>0.853894</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.897720</td>\n",
       "      <td>0.897720</td>\n",
       "      <td>0.655444</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.108392</td>\n",
       "      <td>0.175637</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.925674</td>\n",
       "      <td>0.957269</td>\n",
       "      <td>0.941207</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.925674</td>\n",
       "      <td>0.894358</td>\n",
       "      <td>0.849487</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.899939</td>\n",
       "      <td>0.899939</td>\n",
       "      <td>0.667398</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.129371</td>\n",
       "      <td>0.203857</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.927544</td>\n",
       "      <td>0.957269</td>\n",
       "      <td>0.942172</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.927544</td>\n",
       "      <td>0.900360</td>\n",
       "      <td>0.856164</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.898729</td>\n",
       "      <td>0.898729</td>\n",
       "      <td>0.661849</td>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.118881</td>\n",
       "      <td>0.189415</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.926768</td>\n",
       "      <td>0.956227</td>\n",
       "      <td>0.941267</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.926768</td>\n",
       "      <td>0.901561</td>\n",
       "      <td>0.854866</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.896107  0.896107  0.656852               0.417722            0.115385   \n",
       "1  0.897519  0.897519  0.651216               0.424242            0.097902   \n",
       "2  0.897720  0.897720  0.655444               0.462687            0.108392   \n",
       "3  0.899939  0.899939  0.667398               0.480519            0.129371   \n",
       "4  0.898729  0.898729  0.661849               0.465753            0.118881   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.180822                286.0                      0.923968   \n",
       "1             0.159091                286.0                      0.925599   \n",
       "2             0.175637                286.0                      0.925674   \n",
       "3             0.203857                286.0                      0.927544   \n",
       "4             0.189415                286.0                      0.926768   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.956227                    0.939821   \n",
       "1                   0.956227                    0.940664   \n",
       "2                   0.957269                    0.941207   \n",
       "3                   0.957269                    0.942172   \n",
       "4                   0.956227                    0.941267   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.923968        0.887155   \n",
       "1                      3838.0           0.925599        0.901561   \n",
       "2                      3838.0           0.925674        0.894358   \n",
       "3                      3838.0           0.927544        0.900360   \n",
       "4                      3838.0           0.926768        0.901561   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.849914            833.0  \n",
       "1         0.853894            833.0  \n",
       "2         0.849487            833.0  \n",
       "3         0.856164            833.0  \n",
       "4         0.854866            833.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"SVM.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIXIwRTzeUiD"
   },
   "source": [
    "## Training DescisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T22:07:22.311772Z",
     "start_time": "2022-04-13T04:44:13.148078Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.785, Val_Acc: 0.782, est=0.785, cfg={'max_depth': 6, 'min_samples_split': 6, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'criterion': 'entropy'}\n",
      "Test_micro_f1: 0.7851523098648376\n",
      "Test_macro_f1: 0.40191989200229505\n",
      "Fold: 2, acc=0.784, Val_Acc: 0.774, est=0.788, cfg={'max_depth': 8, 'min_samples_split': 2, 'max_features': 'sqrt', 'min_samples_leaf': 8, 'criterion': 'gini'}\n",
      "Test_micro_f1: 0.7837401654226347\n",
      "Test_macro_f1: 0.44853747750925493\n",
      "Fold: 3, acc=0.786, Val_Acc: 0.774, est=0.785, cfg={'max_depth': 6, 'min_samples_split': 7, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'criterion': 'entropy'}\n",
      "Test_micro_f1: 0.7861609844664111\n",
      "Test_macro_f1: 0.39131763446495826\n",
      "Fold: 4, acc=0.786, Val_Acc: 0.789, est=0.786, cfg={'max_depth': 6, 'min_samples_split': 2, 'max_features': 'sqrt', 'min_samples_leaf': 6, 'criterion': 'gini'}\n",
      "Test_micro_f1: 0.7863627193867259\n",
      "Test_macro_f1: 0.36768821567437865\n",
      "Fold: 5, acc=0.784, Val_Acc: 0.789, est=0.783, cfg={'max_depth': 8, 'min_samples_split': 2, 'max_features': 'sqrt', 'min_samples_leaf': 9, 'criterion': 'gini'}\n",
      "Test_micro_f1: 0.7843453701835787\n",
      "Test_macro_f1: 0.4194425645435083\n",
      "Fold: 6, acc=0.780, Val_Acc: 0.792, est=0.787, cfg={'max_depth': 6, 'min_samples_split': 2, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'criterion': 'gini'}\n",
      "Test_micro_f1: 0.7801089368569699\n",
      "Test_macro_f1: 0.40782728905377663\n",
      "Fold: 7, acc=0.781, Val_Acc: 0.774, est=0.785, cfg={'max_depth': 8, 'min_samples_split': 2, 'max_features': 'sqrt', 'min_samples_leaf': 6, 'criterion': 'gini'}\n",
      "Test_micro_f1: 0.7809158765382288\n",
      "Test_macro_f1: 0.4201610776755286\n",
      "Fold: 8, acc=0.787, Val_Acc: 0.782, est=0.789, cfg={'max_depth': 6, 'min_samples_split': 2, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'criterion': 'gini'}\n",
      "Test_micro_f1: 0.7867661892273553\n",
      "Test_macro_f1: 0.40405006656252923\n",
      "Fold: 9, acc=0.778, Val_Acc: 0.780, est=0.787, cfg={'max_depth': 8, 'min_samples_split': 2, 'max_features': 'sqrt', 'min_samples_leaf': 7, 'criterion': 'gini'}\n",
      "Test_micro_f1: 0.7780915876538229\n",
      "Test_macro_f1: 0.40402304012776646\n",
      "Fold:10, acc=0.778, Val_Acc: 0.801, est=0.789, cfg={'max_depth': 8, 'min_samples_split': 2, 'max_features': 'sqrt', 'min_samples_leaf': 9, 'criterion': 'gini'}\n",
      "Test_micro_f1: 0.7784950574944522\n",
      "Test_macro_f1: 0.4224763387992927\n",
      "CPU times: user 2h 1min, sys: 17min 49s, total: 2h 18min 49s\n",
      "Wall time: 1d 17h 23min 8s\n",
      "Parser   : 311 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "#writing a loop for nested 10-fold cross-validation & parameter tuning\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    model = DecisionTreeClassifier(random_state=1)\n",
    "    \n",
    "    params_range = np.arange(2, 20, 2)\n",
    "    min_split_param = np.arange(2, 10)\n",
    "    \n",
    "    # define gridsearch CV\n",
    "    p_grid={\n",
    "            'min_samples_split': min_split_param,\n",
    "            'criterion':['gini','entropy'], \n",
    "            'max_depth': np.arange(2, 250, 2),\n",
    "            'max_features':['sqrt','log2'],\n",
    "            'min_samples_leaf': np.arange(2, 10)\n",
    "               \n",
    "               } \n",
    "\n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T22:07:23.164077Z",
     "start_time": "2022-04-14T22:07:23.142009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.784 +/- 0.009\n",
      "\n",
      "CV accuracy: 0.783 +/- 0.003\n",
      "\n",
      "CV micro F1: 0.783 +/- 0.003\n",
      "\n",
      "CV macro F1: 0.409 +/- 0.020\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.175 +/- 0.142\n",
      "\n",
      "CV Class : Hate speech Recall: 0.009 +/- 0.011\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.017 +/- 0.020\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.807 +/- 0.007\n",
      "\n",
      "CV Class : Offensive language Recall: 0.957 +/- 0.015\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.875 +/- 0.002\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.539 +/- 0.041\n",
      "\n",
      "CV Class : Neither Recall: 0.248 +/- 0.055\n",
      "\n",
      "CV Class : Neither F1 score: 0.334 +/- 0.049\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T22:07:24.007971Z",
     "start_time": "2022-04-14T22:07:23.962462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_D2V_DT.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.785152</td>\n",
       "      <td>0.785152</td>\n",
       "      <td>0.401920</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.802515</td>\n",
       "      <td>0.964565</td>\n",
       "      <td>0.876109</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.802515</td>\n",
       "      <td>0.226891</td>\n",
       "      <td>0.322801</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.783740</td>\n",
       "      <td>0.783740</td>\n",
       "      <td>0.448537</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.067692</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.817893</td>\n",
       "      <td>0.938510</td>\n",
       "      <td>0.874060</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.817893</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.403860</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.786161</td>\n",
       "      <td>0.786161</td>\n",
       "      <td>0.391318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.800429</td>\n",
       "      <td>0.971860</td>\n",
       "      <td>0.877854</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.800429</td>\n",
       "      <td>0.200480</td>\n",
       "      <td>0.296099</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.786363</td>\n",
       "      <td>0.786363</td>\n",
       "      <td>0.367688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>0.985930</td>\n",
       "      <td>0.878876</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>0.136855</td>\n",
       "      <td>0.224189</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.784345</td>\n",
       "      <td>0.784345</td>\n",
       "      <td>0.419443</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.811081</td>\n",
       "      <td>0.949713</td>\n",
       "      <td>0.874940</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.811081</td>\n",
       "      <td>0.290516</td>\n",
       "      <td>0.376654</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.785152  0.785152  0.401920               0.166667            0.003497   \n",
       "1  0.783740  0.783740  0.448537               0.282051            0.038462   \n",
       "2  0.786161  0.786161  0.391318               0.000000            0.000000   \n",
       "3  0.786363  0.786363  0.367688               0.000000            0.000000   \n",
       "4  0.784345  0.784345  0.419443               0.090909            0.003497   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.006849                286.0                      0.802515   \n",
       "1             0.067692                286.0                      0.817893   \n",
       "2             0.000000                286.0                      0.800429   \n",
       "3             0.000000                286.0                      0.792793   \n",
       "4             0.006734                286.0                      0.811081   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.964565                    0.876109   \n",
       "1                   0.938510                    0.874060   \n",
       "2                   0.971860                    0.877854   \n",
       "3                   0.985930                    0.878876   \n",
       "4                   0.949713                    0.874940   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.802515        0.226891   \n",
       "1                      3838.0           0.817893        0.326531   \n",
       "2                      3838.0           0.800429        0.200480   \n",
       "3                      3838.0           0.792793        0.136855   \n",
       "4                      3838.0           0.811081        0.290516   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.322801            833.0  \n",
       "1         0.403860            833.0  \n",
       "2         0.296099            833.0  \n",
       "3         0.224189            833.0  \n",
       "4         0.376654            833.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"DT.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfzInD9wC1jl"
   },
   "source": [
    "## Training RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T10:48:10.329567Z",
     "start_time": "2022-04-18T03:44:20.082954Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.809, Val_Acc: 0.812, est=0.813, cfg={'min_samples_split': 5, 'max_depth': 10, 'criterion': 'entropy', 'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8085535606213435\n",
      "Test_macro_f1: 0.4274057689668207\n",
      "Fold: 2, acc=0.807, Val_Acc: 0.808, est=0.815, cfg={'min_samples_split': 9, 'max_depth': 10, 'criterion': 'gini', 'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8073431510994553\n",
      "Test_macro_f1: 0.4204968935848194\n",
      "Fold: 3, acc=0.808, Val_Acc: 0.801, est=0.816, cfg={'min_samples_split': 2, 'max_depth': 10, 'criterion': 'entropy', 'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8081500907807141\n",
      "Test_macro_f1: 0.43215001980897955\n",
      "Fold: 4, acc=0.810, Val_Acc: 0.820, est=0.813, cfg={'min_samples_split': 6, 'max_depth': 10, 'criterion': 'entropy', 'n_estimators': 100, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8103691749041759\n",
      "Test_macro_f1: 0.4242185272871441\n",
      "Fold: 5, acc=0.808, Val_Acc: 0.809, est=0.814, cfg={'min_samples_split': 5, 'max_depth': 10, 'criterion': 'entropy', 'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8083518257010288\n",
      "Test_macro_f1: 0.4297847710591863\n",
      "Fold: 6, acc=0.811, Val_Acc: 0.817, est=0.815, cfg={'min_samples_split': 7, 'max_depth': 10, 'criterion': 'entropy', 'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8105709098244906\n",
      "Test_macro_f1: 0.4285693844712816\n",
      "Fold: 7, acc=0.809, Val_Acc: 0.816, est=0.814, cfg={'min_samples_split': 8, 'max_depth': 10, 'criterion': 'gini', 'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8091587653822876\n",
      "Test_macro_f1: 0.4259127948227072\n",
      "Fold: 8, acc=0.813, Val_Acc: 0.808, est=0.815, cfg={'min_samples_split': 3, 'max_depth': 10, 'criterion': 'gini', 'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8127899939479524\n",
      "Test_macro_f1: 0.4355309218508932\n",
      "Fold: 9, acc=0.811, Val_Acc: 0.825, est=0.814, cfg={'min_samples_split': 6, 'max_depth': 10, 'criterion': 'entropy', 'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8107726447448054\n",
      "Test_macro_f1: 0.4310161624193059\n",
      "Fold:10, acc=0.808, Val_Acc: 0.824, est=0.813, cfg={'min_samples_split': 5, 'max_depth': 10, 'criterion': 'entropy', 'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8083518257010288\n",
      "Test_macro_f1: 0.42310720267192353\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    #cv_inner = StratifiedKFold(n_splits=5, random_state=42)\n",
    "    p_grid = {'n_estimators': [50,100],\n",
    "              'max_depth' : np.arange(2, 12, 2),\n",
    "              'min_samples_split': np.arange(2, 10),\n",
    "              'max_features': ['sqrt', 'log2'],\n",
    "              'criterion' :['gini', 'entropy'],\n",
    "              \n",
    "                }\n",
    "    \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T10:48:10.981464Z",
     "start_time": "2022-04-18T10:48:10.937242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.814 +/- 0.007\n",
      "\n",
      "CV accuracy: 0.809 +/- 0.002\n",
      "\n",
      "CV micro F1: 0.809 +/- 0.002\n",
      "\n",
      "CV macro F1: 0.428 +/- 0.004\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.480 +/- 0.483\n",
      "\n",
      "CV Class : Hate speech Recall: 0.004 +/- 0.005\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.008 +/- 0.010\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.810 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Recall: 0.990 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.891 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.805 +/- 0.013\n",
      "\n",
      "CV Class : Neither Recall: 0.252 +/- 0.010\n",
      "\n",
      "CV Class : Neither F1 score: 0.384 +/- 0.012\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-18T12:35:13.321417Z",
     "start_time": "2022-04-18T12:35:13.187490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_D2V_RF.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.808554</td>\n",
       "      <td>0.808554</td>\n",
       "      <td>0.427406</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.808855</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.890347</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.808855</td>\n",
       "      <td>0.247299</td>\n",
       "      <td>0.377982</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.807343</td>\n",
       "      <td>0.807343</td>\n",
       "      <td>0.420497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.807995</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>0.889826</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.807995</td>\n",
       "      <td>0.242497</td>\n",
       "      <td>0.371665</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.808150</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>0.432150</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.027491</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.809473</td>\n",
       "      <td>0.988536</td>\n",
       "      <td>0.890088</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.809473</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.378871</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.810369</td>\n",
       "      <td>0.810369</td>\n",
       "      <td>0.424219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.809473</td>\n",
       "      <td>0.992965</td>\n",
       "      <td>0.891879</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.809473</td>\n",
       "      <td>0.247299</td>\n",
       "      <td>0.380776</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.808352</td>\n",
       "      <td>0.808352</td>\n",
       "      <td>0.429785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010490</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.809077</td>\n",
       "      <td>0.989317</td>\n",
       "      <td>0.890165</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.809077</td>\n",
       "      <td>0.248499</td>\n",
       "      <td>0.378428</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.808554  0.808554  0.427406                    1.0            0.006993   \n",
       "1  0.807343  0.807343  0.420497                    0.0            0.000000   \n",
       "2  0.808150  0.808150  0.432150                    0.8            0.013986   \n",
       "3  0.810369  0.810369  0.424219                    0.0            0.000000   \n",
       "4  0.808352  0.808352  0.429785                    1.0            0.010490   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.013889                286.0                      0.808855   \n",
       "1             0.000000                286.0                      0.807995   \n",
       "2             0.027491                286.0                      0.809473   \n",
       "3             0.000000                286.0                      0.809473   \n",
       "4             0.020761                286.0                      0.809077   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.990099                    0.890347   \n",
       "1                   0.990099                    0.889826   \n",
       "2                   0.988536                    0.890088   \n",
       "3                   0.992965                    0.891879   \n",
       "4                   0.989317                    0.890165   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.808855        0.247299   \n",
       "1                      3838.0           0.807995        0.242497   \n",
       "2                      3838.0           0.809473        0.249700   \n",
       "3                      3838.0           0.809473        0.247299   \n",
       "4                      3838.0           0.809077        0.248499   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.377982            833.0  \n",
       "1         0.371665            833.0  \n",
       "2         0.378871            833.0  \n",
       "3         0.380776            833.0  \n",
       "4         0.378428            833.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"RF.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwmNqojeGz5s"
   },
   "source": [
    "## Training Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T03:58:10.049287Z",
     "start_time": "2022-04-20T03:44:10.256636Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/joblib/parallel.py\", line 833, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/joblib/_parallel_backends.py\", line 521, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/usr/lib/python3.5/concurrent/futures/_base.py\", line 400, in result\n",
      "    self._condition.wait(timeout)\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 293, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-36-fdd8a32f950c>\", line 40, in <module>\n",
      "    gd_search = clf.fit(X_train[train], y_train[train])\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\", line 710, in fit\n",
      "    self._run_search(evaluate_candidates)\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\", line 1151, in _run_search\n",
      "    evaluate_candidates(ParameterGrid(self.param_grid))\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\", line 689, in evaluate_candidates\n",
      "    cv.split(X, y, groups)))\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/joblib/parallel.py\", line 934, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/joblib/parallel.py\", line 855, in retrieve\n",
      "    backend.abort_everything(ensure_ready=ensure_ready)\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/joblib/_parallel_backends.py\", line 538, in abort_everything\n",
      "    self._workers.shutdown(kill_workers=True)\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/joblib/externals/loky/process_executor.py\", line 1096, in shutdown\n",
      "    qmt.join()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1054, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1414, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/coea/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 226, in findsource\n",
      "    pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n",
      "  File \"/usr/lib/python3.5/re.py\", line 224, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"/usr/lib/python3.5/re.py\", line 293, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"/usr/lib/python3.5/sre_compile.py\", line 536, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"/usr/lib/python3.5/sre_parse.py\", line 829, in parse\n",
      "    p = _parse_sub(source, pattern, 0)\n",
      "  File \"/usr/lib/python3.5/sre_parse.py\", line 437, in _parse_sub\n",
      "    itemsappend(_parse(source, state))\n",
      "  File \"/usr/lib/python3.5/sre_parse.py\", line 778, in _parse\n",
      "    p = _parse_sub(source, state)\n",
      "  File \"/usr/lib/python3.5/sre_parse.py\", line 437, in _parse_sub\n",
      "    itemsappend(_parse(source, state))\n",
      "  File \"/usr/lib/python3.5/sre_parse.py\", line 633, in _parse\n",
      "    item = subpattern[-1:]\n",
      "  File \"/usr/lib/python3.5/sre_parse.py\", line 159, in __getitem__\n",
      "    return SubPattern(self.pattern, self.data[index])\n",
      "  File \"/usr/lib/python3.5/sre_parse.py\", line 106, in __init__\n",
      "    self.pattern = pattern\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'list' object to str implicitly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3326\u001b[0;31m                     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3327\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-fdd8a32f950c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# fitting model on parameter search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mgd_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0mdelete_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 \u001b[0mqmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2039\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2041\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3341\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3342\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3344\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3345\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2041\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2043\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1385\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1288\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m             )\n\u001b[1;32m   1290\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m             \u001b[0mformatted_exceptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m             \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert 'list' object to str implicitly"
     ]
    }
   ],
   "source": [
    "  \n",
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = MLPClassifier(random_state=1,max_iter=1000)\n",
    "\n",
    "    p_grid = { 'solver':['sgd', 'adam'],\n",
    "               'activation':['logistic', 'tanh', 'relu'],\n",
    "               'alpha':[0.01, 0.1],\n",
    "               'hidden_layer_sizes':[(50,),(100,),(150,)]\n",
    "             }    \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=20)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T03:58:10.053030Z",
     "start_time": "2022-04-20T03:59:38.300Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T03:58:10.056257Z",
     "start_time": "2022-04-20T03:59:39.672Z"
    }
   },
   "outputs": [],
   "source": [
    "model=\"MLP.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-22T05:47:15.289Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    model = GradientBoostingClassifier(random_state=1)\n",
    "    \n",
    "    p_grid = {  \"learning_rate\": [0.001, 0.01,0.1],\n",
    "                \"min_samples_split\": np.arange(2, 10),\n",
    "                \"max_depth\":np.arange(2, 200, 5),\n",
    "                \"n_estimators\": [50, 100, 200],\n",
    "                }\n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"GBT.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T05:18:42.866076Z",
     "start_time": "2022-03-21T06:13:23.490Z"
    }
   },
   "outputs": [],
   "source": [
    "# t_test = pd.DataFrame(t_test_tfidf_accuracy_gds,columns = ['Fold1','Fold2','Fold3','Fold4','Fold5','Fold6','Fold7','Fold8','Fold9','Fold10',])\n",
    "# res_metrics = pd.DataFrame(tfidf_result_metrics_gds, columns = ['accuracy','micro_f1','macro_f1'])#,'Specificity','Precision','Sensitivity'])\n",
    "\n",
    "# t_test.to_csv('t_test_tfidf_accuracy_10fold_grid_cv.csv', index = False)\n",
    "# res_metrics.to_csv('tfidf_result_metrics_10fold_grid_cv.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:36:16.000845Z",
     "start_time": "2022-04-13T04:35:31.772543Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1, acc=0.423, Val_Acc: 0.405, est=0.385, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: [0.42323986282025416]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.75      0.14       286\n",
      "           1       0.94      0.34      0.50      3838\n",
      "           2       0.64      0.69      0.67       833\n",
      "\n",
      "    accuracy                           0.42      4957\n",
      "   macro avg       0.55      0.59      0.44      4957\n",
      "weighted avg       0.84      0.42      0.51      4957\n",
      "\n",
      "Fold:  2, acc=0.425, Val_Acc: 0.412, est=0.384, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: [0.42323986282025416, 0.4246520072624571]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.74      0.15       286\n",
      "           1       0.94      0.34      0.50      3838\n",
      "           2       0.62      0.69      0.65       833\n",
      "\n",
      "    accuracy                           0.42      4957\n",
      "   macro avg       0.55      0.59      0.43      4957\n",
      "weighted avg       0.83      0.42      0.51      4957\n",
      "\n",
      "Fold:  3, acc=0.429, Val_Acc: 0.437, est=0.387, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: [0.42323986282025416, 0.4246520072624571, 0.4294936453500101]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.76      0.15       286\n",
      "           1       0.94      0.35      0.51      3838\n",
      "           2       0.65      0.68      0.67       833\n",
      "\n",
      "    accuracy                           0.43      4957\n",
      "   macro avg       0.56      0.60      0.44      4957\n",
      "weighted avg       0.84      0.43      0.52      4957\n",
      "\n",
      "Fold:  4, acc=0.446, Val_Acc: 0.444, est=0.398, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: [0.42323986282025416, 0.4246520072624571, 0.4294936453500101, 0.44623764373613073]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.76      0.15       286\n",
      "           1       0.95      0.37      0.53      3838\n",
      "           2       0.66      0.70      0.68       833\n",
      "\n",
      "    accuracy                           0.45      4957\n",
      "   macro avg       0.56      0.61      0.45      4957\n",
      "weighted avg       0.85      0.45      0.53      4957\n",
      "\n",
      "Fold:  5, acc=0.430, Val_Acc: 0.431, est=0.388, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: [0.42323986282025416, 0.4246520072624571, 0.4294936453500101, 0.44623764373613073, 0.4298971151906395]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.76      0.15       286\n",
      "           1       0.95      0.35      0.51      3838\n",
      "           2       0.64      0.70      0.67       833\n",
      "\n",
      "    accuracy                           0.43      4957\n",
      "   macro avg       0.56      0.60      0.44      4957\n",
      "weighted avg       0.84      0.43      0.51      4957\n",
      "\n",
      "Fold:  6, acc=0.426, Val_Acc: 0.435, est=0.381, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: [0.42323986282025416, 0.4246520072624571, 0.4294936453500101, 0.44623764373613073, 0.4298971151906395, 0.4262658866249748]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.74      0.15       286\n",
      "           1       0.94      0.35      0.51      3838\n",
      "           2       0.63      0.68      0.66       833\n",
      "\n",
      "    accuracy                           0.43      4957\n",
      "   macro avg       0.55      0.59      0.44      4957\n",
      "weighted avg       0.84      0.43      0.51      4957\n",
      "\n",
      "Fold:  7, acc=0.416, Val_Acc: 0.410, est=0.377, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: [0.42323986282025416, 0.4246520072624571, 0.4294936453500101, 0.44623764373613073, 0.4298971151906395, 0.4262658866249748, 0.41577567076861005]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.73      0.14       286\n",
      "           1       0.94      0.34      0.50      3838\n",
      "           2       0.64      0.67      0.65       833\n",
      "\n",
      "    accuracy                           0.42      4957\n",
      "   macro avg       0.55      0.58      0.43      4957\n",
      "weighted avg       0.84      0.42      0.50      4957\n",
      "\n",
      "Fold:  8, acc=0.428, Val_Acc: 0.406, est=0.376, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: [0.42323986282025416, 0.4246520072624571, 0.4294936453500101, 0.44623764373613073, 0.4298971151906395, 0.4262658866249748, 0.41577567076861005, 0.4282832358281218]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.77      0.15       286\n",
      "           1       0.95      0.35      0.51      3838\n",
      "           2       0.64      0.69      0.67       833\n",
      "\n",
      "    accuracy                           0.43      4957\n",
      "   macro avg       0.56      0.60      0.44      4957\n",
      "weighted avg       0.84      0.43      0.51      4957\n",
      "\n",
      "Fold:  9, acc=0.416, Val_Acc: 0.423, est=0.375, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: [0.42323986282025416, 0.4246520072624571, 0.4294936453500101, 0.44623764373613073, 0.4298971151906395, 0.4262658866249748, 0.41577567076861005, 0.4282832358281218, 0.4155739358482953]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.76      0.14       286\n",
      "           1       0.94      0.33      0.49      3838\n",
      "           2       0.64      0.67      0.66       833\n",
      "\n",
      "    accuracy                           0.42      4957\n",
      "   macro avg       0.55      0.59      0.43      4957\n",
      "weighted avg       0.84      0.42      0.50      4957\n",
      "\n",
      "Fold: 10, acc=0.417, Val_Acc: 0.405, est=0.380, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: [0.42323986282025416, 0.4246520072624571, 0.4294936453500101, 0.44623764373613073, 0.4298971151906395, 0.4262658866249748, 0.41577567076861005, 0.4282832358281218, 0.4155739358482953, 0.4167843453701836]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.77      0.15       286\n",
      "           1       0.94      0.33      0.49      3838\n",
      "           2       0.65      0.68      0.66       833\n",
      "\n",
      "    accuracy                           0.42      4957\n",
      "   macro avg       0.56      0.59      0.43      4957\n",
      "weighted avg       0.84      0.42      0.50      4957\n",
      "\n",
      "\n",
      "CV vali_accuracy: 0.421 +/- 0.014\n",
      "\n",
      "CV accuracy: 0.426 +/- 0.009\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "#writing a loop for nested 10-fold cross-validation & parameter tuning\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = GaussianNB()\n",
    "    #cv_inner = StratifiedKFold(n_splits=5, random_state=42)\n",
    "    p_grid = [\n",
    "      {'var_smoothing':[1e-9,1e-8,1e-7,1e-6,1e-5]\n",
    "       }]\n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"NB.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hate_speech_TF-IDF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "211px",
    "width": "228px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
