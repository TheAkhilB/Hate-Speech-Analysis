{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1\">TF-IDF</a></span></li><li><span><a href=\"#In-86-start-here\" data-toc-modified-id=\"In-86-start-here-2\">In 86 start here</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3\">Word2Vec</a></span></li><li><span><a href=\"#In-86-start-here-for-Word2vec\" data-toc-modified-id=\"In-86-start-here-for-Word2vec-4\">In 86 start here for Word2vec</a></span></li><li><span><a href=\"#Doc2Vec\" data-toc-modified-id=\"Doc2Vec-5\">Doc2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Splitting-the-data\" data-toc-modified-id=\"Splitting-the-data-5.1\">Splitting the data</a></span></li></ul></li><li><span><a href=\"#Training-Logistic-Regression\" data-toc-modified-id=\"Training-Logistic-Regression-6\">Training Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-SVC\" data-toc-modified-id=\"Training-SVC-6.1\">Training SVC</a></span></li><li><span><a href=\"#Training-DescisionTree\" data-toc-modified-id=\"Training-DescisionTree-6.2\">Training DescisionTree</a></span></li><li><span><a href=\"#Training-RandomForest\" data-toc-modified-id=\"Training-RandomForest-6.3\">Training RandomForest</a></span></li><li><span><a href=\"#Training-Multilayer-Perceptron\" data-toc-modified-id=\"Training-Multilayer-Perceptron-6.4\">Training Multilayer Perceptron</a></span></li><li><span><a href=\"#Training-Gradient-Boosting\" data-toc-modified-id=\"Training-Gradient-Boosting-6.5\">Training Gradient Boosting</a></span></li></ul></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-7\">Naive Bayes</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:29.962246Z",
     "start_time": "2022-04-19T12:27:26.118012Z"
    },
    "id": "qZmuEjZft6HC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:29.967877Z",
     "start_time": "2022-04-19T12:27:29.965078Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.142107Z",
     "start_time": "2022-04-19T12:27:29.979963Z"
    },
    "id": "2g-tlAV2xeg4"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/coea/Hatespeech/labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.163936Z",
     "start_time": "2022-04-19T12:27:30.159874Z"
    },
    "id": "DiarcwG6MxR9"
   },
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0',axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.192604Z",
     "start_time": "2022-04-19T12:27:30.179492Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "id": "7kT-aKnpzmz-",
    "outputId": "3be1c8ed-c133-4d9d-b2e7-cafd8bf75fde"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.211822Z",
     "start_time": "2022-04-19T12:27:30.207945Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GSFMzROzsym",
    "outputId": "11572627-7167-48d5-ab0f-a7aa63c8f654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['count', 'hate_speech', 'offensive_language', 'neither', 'class',\n",
       "       'tweet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.277123Z",
     "start_time": "2022-04-19T12:27:30.226915Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "PPI6y2mU3P9S",
    "outputId": "2dd30c6e-1363-4dfc-b93a-7d3b16970c9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.243473</td>\n",
       "      <td>0.280515</td>\n",
       "      <td>2.413711</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>1.110277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.883060</td>\n",
       "      <td>0.631851</td>\n",
       "      <td>1.399459</td>\n",
       "      <td>1.113299</td>\n",
       "      <td>0.462089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count   hate_speech  offensive_language       neither  \\\n",
       "count  24783.000000  24783.000000        24783.000000  24783.000000   \n",
       "mean       3.243473      0.280515            2.413711      0.549247   \n",
       "std        0.883060      0.631851            1.399459      1.113299   \n",
       "min        3.000000      0.000000            0.000000      0.000000   \n",
       "25%        3.000000      0.000000            2.000000      0.000000   \n",
       "50%        3.000000      0.000000            3.000000      0.000000   \n",
       "75%        3.000000      0.000000            3.000000      0.000000   \n",
       "max        9.000000      7.000000            9.000000      9.000000   \n",
       "\n",
       "              class  \n",
       "count  24783.000000  \n",
       "mean       1.110277  \n",
       "std        0.462089  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        2.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.395803Z",
     "start_time": "2022-04-19T12:27:30.295345Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "zoaS7B0m3fug",
    "outputId": "6d8f8e97-aebe-4a09-ce79-8c25f129d899"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">hate_speech</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">neither</th>\n",
       "      <th colspan=\"8\" halign=\"left\">offensive_language</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>3.108392</td>\n",
       "      <td>0.648084</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>2.256643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>0.755944</td>\n",
       "      <td>0.487653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19190.0</td>\n",
       "      <td>3.268890</td>\n",
       "      <td>0.923024</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19190.0</td>\n",
       "      <td>0.180459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19190.0</td>\n",
       "      <td>3.003544</td>\n",
       "      <td>0.954097</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4163.0</td>\n",
       "      <td>3.172712</td>\n",
       "      <td>0.746097</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4163.0</td>\n",
       "      <td>0.062935</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4163.0</td>\n",
       "      <td>0.264233</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         count                                              hate_speech  \\\n",
       "         count      mean       std  min  25%  50%  75%  max       count   \n",
       "class                                                                     \n",
       "0       1430.0  3.108392  0.648084  3.0  3.0  3.0  3.0  9.0      1430.0   \n",
       "1      19190.0  3.268890  0.923024  3.0  3.0  3.0  3.0  9.0     19190.0   \n",
       "2       4163.0  3.172712  0.746097  3.0  3.0  3.0  3.0  9.0      4163.0   \n",
       "\n",
       "                ...  neither      offensive_language                           \\\n",
       "           mean ...      75%  max              count      mean       std  min   \n",
       "class           ...                                                             \n",
       "0      2.256643 ...      0.0  4.0             1430.0  0.755944  0.487653  0.0   \n",
       "1      0.180459 ...      0.0  3.0            19190.0  3.003544  0.954097  2.0   \n",
       "2      0.062935 ...      3.0  9.0             4163.0  0.264233  0.461737  0.0   \n",
       "\n",
       "                           \n",
       "       25%  50%  75%  max  \n",
       "class                      \n",
       "0      0.0  1.0  1.0  4.0  \n",
       "1      3.0  3.0  3.0  9.0  \n",
       "2      0.0  0.0  1.0  4.0  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"class\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.417891Z",
     "start_time": "2022-04-19T12:27:30.411928Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlrXEEjfRpU-",
    "outputId": "b2d7f10c-544b-4008-c75f-b1610f707312"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1430"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.groupby(\"class\").describe()[\"class\"==0]\n",
    "sum(df[\"class\"]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.438981Z",
     "start_time": "2022-04-19T12:27:30.433738Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kd3x0b6kTR2C",
    "outputId": "0fd8fb36-a08a-4437-bc22-29dcb4bde58b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    19190\n",
       "2     4163\n",
       "0     1430\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.456601Z",
     "start_time": "2022-04-19T12:27:30.453921Z"
    },
    "id": "aQWMCpzRFbA4"
   },
   "outputs": [],
   "source": [
    "def percentage_class(idx):\n",
    "    Percentage = 100 * float(sum(df[\"class\"]==idx))/float(len(df[\"class\"]))\n",
    "    return Percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.478958Z",
     "start_time": "2022-04-19T12:27:30.471678Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIY1NFjBRJKU",
    "outputId": "ed2113a7-0550-437f-d8ba-09c6b50e785b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the percenatge of '0' class 5.770084332001775\n",
      "the percenatge of '1' class 77.43211072105879\n",
      "the percenatge of '2' class 16.797804946939433\n"
     ]
    }
   ],
   "source": [
    "print(\"the percenatge of '0' class\", percentage_class(0))\n",
    "print(\"the percenatge of '1' class\", percentage_class(1))\n",
    "print(\"the percenatge of '2' class\", percentage_class(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.655878Z",
     "start_time": "2022-04-19T12:27:30.495216Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "GtRcXmguS0zE",
    "outputId": "e97f821f-5f7a-40f2-c045-039d5561314d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fef0f30d710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef079acef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ploting percentage of each class\n",
    "df[\"class\"].value_counts().plot(kind='pie',autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHSLZ2oHJEfo"
   },
   "source": [
    "Finding out the Maximum length tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.693556Z",
     "start_time": "2022-04-19T12:27:30.670798Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "oHh5QglEIwlt",
    "outputId": "06a1c4ef-ae14-4b03-b582-15ba202a90d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['tweet'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.920669Z",
     "start_time": "2022-04-19T12:27:30.709923Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "0mNOMylQKD8C",
    "outputId": "cd1e1db0-f49d-40a1-a43d-6e83cfd76f20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fef0f2b2b38>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef2c052ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['length'].plot(bins=50, kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxCU9bWANEWN"
   },
   "source": [
    "#Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTmX_141EwZR"
   },
   "source": [
    "Here copying the data from **df** to **dataF** and creating new variable with same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:30.938754Z",
     "start_time": "2022-04-19T12:27:30.934536Z"
    },
    "id": "a_fg58OnRuMx"
   },
   "outputs": [],
   "source": [
    "#copying data from original variable to another varible.\n",
    "\n",
    "dataF = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T03:22:48.583433Z",
     "start_time": "2022-04-19T03:22:48.570300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "Y8nVur4mR5Hi",
    "outputId": "2b894f96-6e1a-4a66-e7bf-0353f5d0a86d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing top 3 rows in the data set\n",
    "\n",
    "dataF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T03:22:48.677642Z",
     "start_time": "2022-04-19T03:22:48.664155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T03:22:50.442725Z",
     "start_time": "2022-04-19T03:22:48.757084Z"
    },
    "id": "C13nkq6e5c4P"
   },
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords            ## it will check stop words present in data\n",
    "from nltk.stem.porter import PorterStemmer   ## it will use for stemmming the words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T03:22:50.539950Z",
     "start_time": "2022-04-19T03:22:50.536465Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T03:22:51.737271Z",
     "start_time": "2022-04-19T03:22:50.543292Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1ee3083fd02b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m#For expanding Contractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "def con(words):\n",
    "    return [contractions.fix(word) for word in words.split()]    #For expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T03:22:51.739055Z",
     "start_time": "2022-04-19T03:37:57.072Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataF['Contractions']=dataF['tweet'].apply(lambda x: con(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T03:22:51.740841Z",
     "start_time": "2022-04-19T03:37:57.156Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T03:22:51.748813Z",
     "start_time": "2022-04-19T03:37:57.219Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install TextBlob\n",
    "#from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.249394Z",
     "start_time": "2022-03-05T05:04:32.245934Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gingerit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.391937Z",
     "start_time": "2022-03-05T05:04:32.388591Z"
    }
   },
   "outputs": [],
   "source": [
    "#from gingerit.gingerit import GingerIt\n",
    "#parser = GingerIt()\n",
    "#tweet=parser.parse(dataF['tweet'][23])\n",
    "#tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.503268Z",
     "start_time": "2022-03-05T05:04:32.500517Z"
    }
   },
   "outputs": [],
   "source": [
    "#blob=TextBlob(dataF['tweet'][23])\n",
    "#blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:33.089007Z",
     "start_time": "2022-03-05T05:04:33.079920Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Expanding whatsapp language slangs\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"./HateSpeechNew/PreProcessing/sms_slang_translator-master/slang.txt\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if str(_str).upper() == row[0]:\n",
    "                    # If match found replace it with its appropriate phrase in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    return ' '.join(user_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.087982Z",
     "start_time": "2022-03-05T05:04:33.545921Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataF['ExpandedSlangs']=dataF['tweet'].apply(lambda x: translator(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.127335Z",
     "start_time": "2022-03-05T05:05:28.118138Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.175036Z",
     "start_time": "2022-03-05T05:05:28.152537Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'con' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-20407348bd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ExpandedSlangs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Contractions for expanded slangs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3192\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-20407348bd53>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ExpandedSlangs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Contractions for expanded slangs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'con' is not defined"
     ]
    }
   ],
   "source": [
    "dataF['Preprocessed_Initial']=dataF['ExpandedSlangs'].apply(lambda x: con(x)) #Contractions for expanded slangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:03.502948Z",
     "start_time": "2022-03-05T05:07:03.483677Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:03.996685Z",
     "start_time": "2022-03-05T05:07:03.993840Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJO5rARp8Bb2",
    "outputId": "fc26ef1f-29b7-4db9-db52-9bcc2a77cbd2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.121200Z",
     "start_time": "2022-03-05T05:07:04.118575Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctTSRXQR0vO4",
    "outputId": "1a59d48e-acec-4839-8f8e-bf1fc8180a6b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.244346Z",
     "start_time": "2022-03-05T05:07:04.241803Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iG6koGQnpfOF",
    "outputId": "e9572c08-72e0-4cdb-a447-f4b9fc6d3fbe"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.382959Z",
     "start_time": "2022-03-05T05:07:04.375813Z"
    },
    "id": "6146Ji8QjJPt"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the tweets.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer,word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    #for text in sentence:\n",
    "    #  if text in ['html','www','http','https','rt']:\n",
    "    #sentence= [sentence.replace(w,'') for w in sentence if w in ['html','www','http','https','rt']]\n",
    "    sentence=sentence.replace('rt',\"\")\n",
    "    sentence=sentence.replace('www',\"\")\n",
    "    sentence=sentence.replace('http',\"\")\n",
    "    sentence=sentence.replace('https',\"\")\n",
    "    sentence=sentence.replace('html',\"\")\n",
    "    sentence=sentence.replace('*',\"\")\n",
    "    sentence=sentence.replace('#',\"\")\n",
    "    cleanr = re.compile('<.?>')\n",
    "    cleantext1 = re.sub(cleanr, '', sentence)\n",
    "    cleantext = re.sub('@[^\\s]+','',cleantext1)\n",
    "    #rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', cleantext)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    tokens = tokenizer.tokenize(rem_num)\n",
    "    #tokens = word_tokenize(rem_num)\n",
    "    filtered_words = [w for w in tokens if len(w) > 2] # if not w in stopwords.words('english')]\n",
    "    #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    #lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.605980Z",
     "start_time": "2022-03-05T05:07:04.477566Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnDBnHcyLRZk",
    "outputId": "c3638e8c-646c-4c09-b7e2-0a8fe26246e4",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Preprocessed_Initial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Preprocessed_Initial'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-a01c6738a8ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PreprocessedTweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#dataF['tweet_prepro'] = dataF['tweet'].apply(lambda x: preprocess(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#dataF['tweet_prepro'].tail()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Preprocessed_Initial'"
     ]
    }
   ],
   "source": [
    "dataF['PreprocessedTweet']=dataF['Preprocessed_Initial'].map(lambda s:preprocess(s))\n",
    "#dataF['tweet_prepro'] = dataF['tweet'].apply(lambda x: preprocess(x))\n",
    "#dataF['tweet_prepro'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:05.904217Z",
     "start_time": "2022-03-05T05:07:05.886888Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:33:06.476959Z",
     "start_time": "2022-03-05T09:33:06.472222Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split(\" \",text) \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.187016Z",
     "start_time": "2022-03-05T05:19:23.336Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['final_tweet_tokens'] = dataF['PreprocessedTweet'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.188320Z",
     "start_time": "2022-03-05T05:19:23.644Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.189637Z",
     "start_time": "2022-03-05T05:19:24.031Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF['PreprocessedTweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.190956Z",
     "start_time": "2022-03-05T05:19:24.453Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.192175Z",
     "start_time": "2022-03-05T05:19:24.910Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "word_c = WordCloud(collocations=False,background_color='white',mode='RGB',scale=4).generate(str(dataF['PreprocessedTweet']))\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(18,8),facecolor='w')\n",
    "plt.imshow(word_c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.193460Z",
     "start_time": "2022-03-05T05:19:25.528Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatizing(words):\n",
    "    lemmatizer =WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.194684Z",
     "start_time": "2022-03-05T05:19:25.996Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['LemmaWords']=dataF['final_tweet_tokens'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.195891Z",
     "start_time": "2022-03-05T05:19:26.615Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.197110Z",
     "start_time": "2022-03-05T05:19:26.972Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF['LemmaWords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.198387Z",
     "start_time": "2022-03-05T05:19:27.407Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.to_csv('Hate_spech_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.199640Z",
     "start_time": "2022-03-05T05:19:27.920Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.200844Z",
     "start_time": "2022-03-05T05:19:30.381Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.202042Z",
     "start_time": "2022-03-05T05:19:31.178Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.203266Z",
     "start_time": "2022-03-05T05:19:31.370Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['KerasTokens']=dataF['PreprocessedTweet'].apply(text_to_word_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.204501Z",
     "start_time": "2022-03-05T05:19:31.537Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.205742Z",
     "start_time": "2022-03-05T05:19:32.391Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "word_c = WordCloud(collocations=False,background_color='white',mode='RGB',scale=4).generate(str(dataF['LemmaWords']))\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(18,8),facecolor='w')\n",
    "plt.imshow(word_c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.206953Z",
     "start_time": "2022-03-05T05:19:32.619Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['Lemma_Preprocessed']=dataF['LemmaWords'].map(lambda s:preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.208149Z",
     "start_time": "2022-03-05T05:19:32.830Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T11:35:00.346119Z",
     "start_time": "2022-03-17T11:34:59.770591Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF = pd.read_csv(\"/home/coea/Hatespeech/preprocessing/Hate_spech_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T11:35:00.495188Z",
     "start_time": "2022-03-17T11:35:00.444766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "      <th>Preprocessed_Initial</th>\n",
       "      <th>PreprocessedTweet</th>\n",
       "      <th>final_tweet_tokens</th>\n",
       "      <th>LemmaWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>['!!!', 'RT', '@mayasolovely:', 'As', 'a', 'wo...</td>\n",
       "      <td>woman you should not complain about cleaning y...</td>\n",
       "      <td>['woman', 'you', 'should', 'not', 'complain', ...</td>\n",
       "      <td>['woman', 'you', 'should', 'not', 'complain', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "      <td>['!!!!!', 'RT', '@mleew17:', 'boy', 'That Is',...</td>\n",
       "      <td>boy that cold tyga down bad for cuffin that ho...</td>\n",
       "      <td>['boy', 'that', 'cold', 'tyga', 'down', 'bad',...</td>\n",
       "      <td>['boy', 'that', 'cold', 'tyga', 'down', 'bad',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "      <td>['!!!!!!!', 'RT', '@UrKindOfBrand', 'Dog', 'RT...</td>\n",
       "      <td>dog you ever fuck bitch and she sta cry you co...</td>\n",
       "      <td>['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...</td>\n",
       "      <td>['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>['!!!!!!!!!', 'RT', '@C_G_Anderson:', '@viva_b...</td>\n",
       "      <td>she look like tranny</td>\n",
       "      <td>['she', 'look', 'like', 'tranny']</td>\n",
       "      <td>['she', 'look', 'like', 'tranny']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>['!!!!!!!!!!!!!', 'RT', '@ShenikaRoberts:', 'T...</td>\n",
       "      <td>the shit you hear about might true might faker...</td>\n",
       "      <td>['the', 'shit', 'you', 'hear', 'about', 'might...</td>\n",
       "      <td>['the', 'shit', 'you', 'hear', 'about', 'might...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "\n",
       "                                Preprocessed_Initial  \\\n",
       "0  ['!!!', 'RT', '@mayasolovely:', 'As', 'a', 'wo...   \n",
       "1  ['!!!!!', 'RT', '@mleew17:', 'boy', 'That Is',...   \n",
       "2  ['!!!!!!!', 'RT', '@UrKindOfBrand', 'Dog', 'RT...   \n",
       "3  ['!!!!!!!!!', 'RT', '@C_G_Anderson:', '@viva_b...   \n",
       "4  ['!!!!!!!!!!!!!', 'RT', '@ShenikaRoberts:', 'T...   \n",
       "\n",
       "                                   PreprocessedTweet  \\\n",
       "0  woman you should not complain about cleaning y...   \n",
       "1  boy that cold tyga down bad for cuffin that ho...   \n",
       "2  dog you ever fuck bitch and she sta cry you co...   \n",
       "3                               she look like tranny   \n",
       "4  the shit you hear about might true might faker...   \n",
       "\n",
       "                                  final_tweet_tokens  \\\n",
       "0  ['woman', 'you', 'should', 'not', 'complain', ...   \n",
       "1  ['boy', 'that', 'cold', 'tyga', 'down', 'bad',...   \n",
       "2  ['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...   \n",
       "3                  ['she', 'look', 'like', 'tranny']   \n",
       "4  ['the', 'shit', 'you', 'hear', 'about', 'might...   \n",
       "\n",
       "                                          LemmaWords  \n",
       "0  ['woman', 'you', 'should', 'not', 'complain', ...  \n",
       "1  ['boy', 'that', 'cold', 'tyga', 'down', 'bad',...  \n",
       "2  ['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...  \n",
       "3                  ['she', 'look', 'like', 'tranny']  \n",
       "4  ['the', 'shit', 'you', 'hear', 'about', 'might...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:33:35.291436Z",
     "start_time": "2022-03-05T09:33:35.287464Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T08:36:39.761456Z",
     "start_time": "2022-03-15T08:36:38.002766Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T03:37:56.594816Z",
     "start_time": "2022-03-07T03:37:56.228628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 590)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=50, max_features=1000, stop_words=None,tokenizer=tokenize,analyzer='word')\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataF['PreprocessedTweet'].values.astype('U'))\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In 86 start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T11:25:32.934985Z",
     "start_time": "2022-03-17T11:25:29.418640Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.read_csv('/home/coea/Hatespeech/TF-IDF/TF-IDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T11:25:33.127845Z",
     "start_time": "2022-03-17T11:25:33.122489Z"
    }
   },
   "outputs": [],
   "source": [
    "# tfidf_df = pd.DataFrame(tfidf_df.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T11:25:33.280287Z",
     "start_time": "2022-03-17T11:25:33.268848Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T11:25:33.371694Z",
     "start_time": "2022-03-17T11:25:33.361403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 589)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 1.06 s, total: 3min 34s\n",
      "Wall time: 17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3628394, 5058200)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_tweet = dataF['Lemma_Preprocessed'].apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            vector_size=500, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 32, # no.of cores\n",
    "            seed = 34\n",
    ") \n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(dataF['Lemma_Preprocessed']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 500)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 500)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 500 )\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In 86 start here for Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:43.845674Z",
     "start_time": "2022-04-19T12:27:40.253413Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_df = pd.read_csv(\"/home/coea/Hatespeech/Word2Vec_Vectors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:43.905000Z",
     "start_time": "2022-04-19T12:27:43.901814Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_df = word2vec_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pandas==0.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/dstuser/anaconda3/lib/python3.7/site-packages (4.32.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LabeledSentence' from 'gensim.models.doc2vec' (/home/dstuser/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-8d3be766ad86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#tqdm.pandas(desc=\"progress-bar\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabeledSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LabeledSentence' from 'gensim.models.doc2vec' (/home/dstuser/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "#tqdm.pandas(desc=\"progress-bar\") \n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(TaggedDocument(s, [\"tweet_\" + str(i)]))\n",
    "    return output\n",
    "\n",
    "labeled_tweets = add_label(tokenized_tweet) # label all the tweets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time \n",
    "model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model\n",
    "                                  dm_mean=1, # dm_mean = 1 for using mean of the context word vectors\n",
    "                                  vector_size=1000, # no. of desired features\n",
    "                                  window=5, # width of the context window                                  \n",
    "                                  negative=7, # if > 0 then negative sampling will be used\n",
    "                                  min_count=5, # Ignores all words with total frequency lower than 5.                                  \n",
    "                                  workers=32, # no. of cores                                  \n",
    "                                  alpha=0.1, # learning rate                                  \n",
    "                                  seed = 23, # for reproducibility\n",
    "                                 ) \n",
    "\n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n",
    "\n",
    "model_d2v.train(labeled_tweets, total_examples= len(dataF['Lemma_Preprocessed']), epochs=15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docvec_arrays = np.zeros((len(tokenized_tweet), 1000)) \n",
    "for i in range(len(dataF)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,1000))    \n",
    "\n",
    "docvec_df = pd.DataFrame(docvec_arrays) \n",
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:44.422461Z",
     "start_time": "2022-04-19T12:27:43.938531Z"
    },
    "id": "53bWppiTiWGx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(word2vec_df, dataF['class'].values, \\\n",
    "                                                    stratify = dataF['class'].values,shuffle = True,\\\n",
    "                                                    random_state=1,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:44.905015Z",
     "start_time": "2022-04-19T12:27:44.454617Z"
    },
    "id": "Obk37BzJsRwW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing necessary librarys machine learning algorthms\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# PNN algorithm not imported\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#importing all necessary metrics\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:27:46.021946Z",
     "start_time": "2022-04-19T12:27:46.018800Z"
    }
   },
   "outputs": [],
   "source": [
    "technique='word2vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-21T05:44:37.500886Z",
     "start_time": "2022-03-21T05:33:25.902028Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.878, Val_Acc: 0.873, est=0.875, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8783538430502321\n",
      "Test_macro_f1: 0.6402863540755339\n",
      "Fold: 2, acc=0.877, Val_Acc: 0.866, est=0.877, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8773451684486585\n",
      "Test_macro_f1: 0.6398970234135787\n",
      "Fold: 3, acc=0.876, Val_Acc: 0.871, est=0.875, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8755295541658261\n",
      "Test_macro_f1: 0.6383455269669928\n",
      "Fold: 4, acc=0.875, Val_Acc: 0.877, est=0.876, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8753278192455114\n",
      "Test_macro_f1: 0.6302563600873944\n",
      "Fold: 5, acc=0.877, Val_Acc: 0.871, est=0.874, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8771434335283438\n",
      "Test_macro_f1: 0.6381315203005776\n",
      "Fold: 6, acc=0.876, Val_Acc: 0.878, est=0.874, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8761347589267702\n",
      "Test_macro_f1: 0.6357567595452036\n",
      "Fold: 7, acc=0.876, Val_Acc: 0.878, est=0.876, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8763364938470849\n",
      "Test_macro_f1: 0.6280295501837286\n",
      "Fold: 8, acc=0.877, Val_Acc: 0.873, est=0.876, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8773451684486585\n",
      "Test_macro_f1: 0.6341832916946123\n",
      "Fold: 9, acc=0.878, Val_Acc: 0.876, est=0.875, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8777486382892878\n",
      "Test_macro_f1: 0.6377089785866775\n",
      "Fold:10, acc=0.877, Val_Acc: 0.893, est=0.872, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8773451684486585\n",
      "Test_macro_f1: 0.6342002646301753\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    LR_model = LogisticRegression(random_state=1,C=100)\n",
    "    \n",
    "    # define gridsearch CV\n",
    "    p_grid = {'penalty': ['l1', 'l2']}\n",
    "    \n",
    "    clf = GridSearchCV(estimator = LR_model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-21T05:44:37.783468Z",
     "start_time": "2022-03-21T05:44:37.748023Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.876 +/- 0.007\n",
      "\n",
      "CV accuracy: 0.877 +/- 0.001\n",
      "\n",
      "CV micro F1: 0.877 +/- 0.001\n",
      "\n",
      "CV macro F1: 0.636 +/- 0.004\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.619 +/- 0.024\n",
      "\n",
      "CV Class : Hate speech Recall: 0.127 +/- 0.008\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.211 +/- 0.011\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.907 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Recall: 0.953 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.930 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.752 +/- 0.004\n",
      "\n",
      "CV Class : Neither Recall: 0.782 +/- 0.005\n",
      "\n",
      "CV Class : Neither F1 score: 0.767 +/- 0.002\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-21T05:49:10.343322Z",
     "start_time": "2022-03-21T05:49:10.235768Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec_LR.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.878354</td>\n",
       "      <td>0.878354</td>\n",
       "      <td>0.640286</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.132867</td>\n",
       "      <td>0.220290</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.907807</td>\n",
       "      <td>0.954403</td>\n",
       "      <td>0.930522</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.907807</td>\n",
       "      <td>0.783914</td>\n",
       "      <td>0.770047</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.877345</td>\n",
       "      <td>0.877345</td>\n",
       "      <td>0.639897</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.223496</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.906080</td>\n",
       "      <td>0.955185</td>\n",
       "      <td>0.929985</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.906080</td>\n",
       "      <td>0.773109</td>\n",
       "      <td>0.766211</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.875530</td>\n",
       "      <td>0.875530</td>\n",
       "      <td>0.638346</td>\n",
       "      <td>0.629032</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.952058</td>\n",
       "      <td>0.928826</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.776711</td>\n",
       "      <td>0.762073</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.875328</td>\n",
       "      <td>0.875328</td>\n",
       "      <td>0.630256</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.118881</td>\n",
       "      <td>0.197101</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.906723</td>\n",
       "      <td>0.952319</td>\n",
       "      <td>0.928962</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.906723</td>\n",
       "      <td>0.780312</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.877143</td>\n",
       "      <td>0.877143</td>\n",
       "      <td>0.638132</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.129371</td>\n",
       "      <td>0.215116</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.908887</td>\n",
       "      <td>0.951277</td>\n",
       "      <td>0.929599</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.908887</td>\n",
       "      <td>0.792317</td>\n",
       "      <td>0.769679</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.878354  0.878354  0.640286               0.644068            0.132867   \n",
       "1  0.877345  0.877345  0.639897               0.619048            0.136364   \n",
       "2  0.875530  0.875530  0.638346               0.629032            0.136364   \n",
       "3  0.875328  0.875328  0.630256               0.576271            0.118881   \n",
       "4  0.877143  0.877143  0.638132               0.637931            0.129371   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.220290                286.0                      0.907807   \n",
       "1             0.223496                286.0                      0.906080   \n",
       "2             0.224138                286.0                      0.906700   \n",
       "3             0.197101                286.0                      0.906723   \n",
       "4             0.215116                286.0                      0.908887   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.954403                    0.930522   \n",
       "1                   0.955185                    0.929985   \n",
       "2                   0.952058                    0.928826   \n",
       "3                   0.952319                    0.928962   \n",
       "4                   0.951277                    0.929599   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.907807        0.783914   \n",
       "1                      3838.0           0.906080        0.773109   \n",
       "2                      3838.0           0.906700        0.776711   \n",
       "3                      3838.0           0.906723        0.780312   \n",
       "4                      3838.0           0.908887        0.792317   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.770047            833.0  \n",
       "1         0.766211            833.0  \n",
       "2         0.762073            833.0  \n",
       "3         0.764706            833.0  \n",
       "4         0.769679            833.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"LR.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHwC6nvnrbkq"
   },
   "source": [
    "## Training SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T05:52:13.943026Z",
     "start_time": "2022-04-19T03:23:23.265170Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.881, Val_Acc: 0.876, est=0.883, cfg={'C': 1.0, 'kernel': 'rbf'}\n",
      "Test_micro_f1: 0.8813798668549526\n",
      "Test_macro_f1: 0.5959379440915232\n",
      "Fold: 2, acc=0.882, Val_Acc: 0.876, est=0.884, cfg={'C': 1.0, 'kernel': 'rbf'}\n",
      "Test_micro_f1: 0.881783336695582\n",
      "Test_macro_f1: 0.5976153725708755\n",
      "Fold: 3, acc=0.881, Val_Acc: 0.879, est=0.883, cfg={'C': 1.0, 'kernel': 'rbf'}\n",
      "Test_micro_f1: 0.8805729271736937\n",
      "Test_macro_f1: 0.5960584111590714\n",
      "Fold: 4, acc=0.882, Val_Acc: 0.886, est=0.882, cfg={'C': 1.0, 'kernel': 'rbf'}\n",
      "Test_micro_f1: 0.8823885414565261\n",
      "Test_macro_f1: 0.6024936584169592\n",
      "Fold: 5, acc=0.882, Val_Acc: 0.882, est=0.883, cfg={'C': 1.0, 'kernel': 'rbf'}\n",
      "Test_micro_f1: 0.8819850716158967\n",
      "Test_macro_f1: 0.5936560218682218\n",
      "Fold: 6, acc=0.882, Val_Acc: 0.891, est=0.883, cfg={'C': 1.0, 'kernel': 'rbf'}\n",
      "Test_micro_f1: 0.8821868065362114\n",
      "Test_macro_f1: 0.5979377418526138\n",
      "Fold: 7, acc=0.882, Val_Acc: 0.880, est=0.884, cfg={'C': 1.0, 'kernel': 'rbf'}\n",
      "Test_micro_f1: 0.8823885414565261\n",
      "Test_macro_f1: 0.5996533702806937\n",
      "Fold: 8, acc=0.883, Val_Acc: 0.883, est=0.883, cfg={'C': 1.0, 'kernel': 'rbf'}\n",
      "Test_micro_f1: 0.8825902763768408\n",
      "Test_macro_f1: 0.5940193803491652\n",
      "Fold: 9, acc=0.883, Val_Acc: 0.884, est=0.882, cfg={'C': 1.0, 'kernel': 'rbf'}\n",
      "Test_micro_f1: 0.8831954811377849\n",
      "Test_macro_f1: 0.5986487474661026\n",
      "Fold:10, acc=0.882, Val_Acc: 0.901, est=0.881, cfg={'C': 1.0, 'kernel': 'rbf'}\n",
      "Test_micro_f1: 0.8821868065362114\n",
      "Test_macro_f1: 0.5941354216145088\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = SVC(random_state=42,max_iter=-1)\n",
    "    param_range = [0.1,0.5,1.0]\n",
    "    \n",
    "    p_grid =  {'C': param_range, \n",
    "               #'loss':['hinge', 'squared_hinge'],\n",
    "               'kernel':['linear','poly', 'rbf', 'sigmoid']\n",
    "               }   \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T05:52:14.123287Z",
     "start_time": "2022-04-19T05:52:14.100135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.884 +/- 0.007\n",
      "\n",
      "CV accuracy: 0.882 +/- 0.001\n",
      "\n",
      "CV micro F1: 0.882 +/- 0.001\n",
      "\n",
      "CV macro F1: 0.597 +/- 0.003\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.548 +/- 0.075\n",
      "\n",
      "CV Class : Hate speech Recall: 0.034 +/- 0.004\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.063 +/- 0.008\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.915 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Recall: 0.953 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.934 +/- 0.000\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.748 +/- 0.003\n",
      "\n",
      "CV Class : Neither Recall: 0.846 +/- 0.004\n",
      "\n",
      "CV Class : Neither F1 score: 0.794 +/- 0.002\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T05:52:14.303146Z",
     "start_time": "2022-04-19T05:52:14.259305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec_SVM.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.881380</td>\n",
       "      <td>0.881380</td>\n",
       "      <td>0.595938</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.031469</td>\n",
       "      <td>0.057878</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.915539</td>\n",
       "      <td>0.951798</td>\n",
       "      <td>0.933316</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.915539</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>0.796620</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.881783</td>\n",
       "      <td>0.881783</td>\n",
       "      <td>0.597615</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.034965</td>\n",
       "      <td>0.066007</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.914915</td>\n",
       "      <td>0.952579</td>\n",
       "      <td>0.933367</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.914915</td>\n",
       "      <td>0.846339</td>\n",
       "      <td>0.793472</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.880573</td>\n",
       "      <td>0.880573</td>\n",
       "      <td>0.596058</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.034965</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.915059</td>\n",
       "      <td>0.951537</td>\n",
       "      <td>0.932942</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.915059</td>\n",
       "      <td>0.843938</td>\n",
       "      <td>0.789444</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.882389</td>\n",
       "      <td>0.882389</td>\n",
       "      <td>0.602494</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>0.077170</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.917567</td>\n",
       "      <td>0.951277</td>\n",
       "      <td>0.934118</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.917567</td>\n",
       "      <td>0.853541</td>\n",
       "      <td>0.796193</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.881985</td>\n",
       "      <td>0.881985</td>\n",
       "      <td>0.593656</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>0.053512</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.914521</td>\n",
       "      <td>0.953361</td>\n",
       "      <td>0.933537</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.914521</td>\n",
       "      <td>0.846339</td>\n",
       "      <td>0.793919</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.881380  0.881380  0.595938               0.360000            0.031469   \n",
       "1  0.881783  0.881783  0.597615               0.588235            0.034965   \n",
       "2  0.880573  0.880573  0.596058               0.555556            0.034965   \n",
       "3  0.882389  0.882389  0.602494               0.480000            0.041958   \n",
       "4  0.881985  0.881985  0.593656               0.615385            0.027972   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.057878                286.0                      0.915539   \n",
       "1             0.066007                286.0                      0.914915   \n",
       "2             0.065789                286.0                      0.915059   \n",
       "3             0.077170                286.0                      0.917567   \n",
       "4             0.053512                286.0                      0.914521   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.951798                    0.933316   \n",
       "1                   0.952579                    0.933367   \n",
       "2                   0.951537                    0.932942   \n",
       "3                   0.951277                    0.934118   \n",
       "4                   0.953361                    0.933537   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.915539        0.848739   \n",
       "1                      3838.0           0.914915        0.846339   \n",
       "2                      3838.0           0.915059        0.843938   \n",
       "3                      3838.0           0.917567        0.853541   \n",
       "4                      3838.0           0.914521        0.846339   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.796620            833.0  \n",
       "1         0.793472            833.0  \n",
       "2         0.789444            833.0  \n",
       "3         0.796193            833.0  \n",
       "4         0.793919            833.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"SVM.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIXIwRTzeUiD"
   },
   "source": [
    "## Training DescisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-22T08:44:45.735Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "#writing a loop for nested 10-fold cross-validation & parameter tuning\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    model = DecisionTreeClassifier(random_state=1)\n",
    "    \n",
    "    params_range = np.arange(2, 20, 2)\n",
    "    min_split_param = np.arange(2, 10)\n",
    "    \n",
    "    # define gridsearch CV\n",
    "    p_grid={\n",
    "            'min_samples_split': min_split_param,\n",
    "            'criterion':['gini','entropy'], \n",
    "            'max_depth': np.arange(2, 250, 2),\n",
    "            'max_features':['sqrt','log2'],\n",
    "            'min_samples_leaf': np.arange(2, 10)\n",
    "               \n",
    "               } \n",
    "\n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-22T08:44:47.140Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-21T06:04:10.819Z"
    }
   },
   "outputs": [],
   "source": [
    "model=\"DT.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfzInD9wC1jl"
   },
   "source": [
    "## Training RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-16T07:12:14.617Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.892, Val_Acc: 0.893, est=0.893, cfg={'min_samples_split': 9, 'criterion': 'entropy', 'max_depth': 172, 'n_estimators': 200, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8922735525519467\n",
      "Test_macro_f1: 0.6529699264654975\n",
      "Fold: 2, acc=0.892, Val_Acc: 0.888, est=0.893, cfg={'min_samples_split': 9, 'criterion': 'entropy', 'max_depth': 182, 'n_estimators': 100, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8922735525519467\n",
      "Test_macro_f1: 0.6572266248349027\n",
      "Fold: 3, acc=0.893, Val_Acc: 0.896, est=0.893, cfg={'min_samples_split': 9, 'criterion': 'entropy', 'max_depth': 117, 'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8928787573128909\n",
      "Test_macro_f1: 0.6552646742500361\n",
      "Fold: 4, acc=0.888, Val_Acc: 0.895, est=0.893, cfg={'min_samples_split': 8, 'criterion': 'entropy', 'max_depth': 82, 'n_estimators': 200, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8884405890659673\n",
      "Test_macro_f1: 0.6487868394318691\n",
      "Fold: 5, acc=0.891, Val_Acc: 0.884, est=0.893, cfg={'min_samples_split': 9, 'criterion': 'entropy', 'max_depth': 87, 'n_estimators': 200, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8912648779503732\n",
      "Test_macro_f1: 0.6565160835181449\n",
      "Fold: 6, acc=0.891, Val_Acc: 0.898, est=0.891, cfg={'min_samples_split': 8, 'criterion': 'entropy', 'max_depth': 122, 'n_estimators': 100, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8912648779503732\n",
      "Test_macro_f1: 0.647578040665735\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    #cv_inner = StratifiedKFold(n_splits=5, random_state=42)\n",
    "    p_grid = {'n_estimators': [50,100,200],\n",
    "              'max_depth' : np.arange(2, 200, 5),\n",
    "              'min_samples_split': np.arange(2, 10),\n",
    "              'max_features': ['sqrt', 'log2'],\n",
    "              'criterion' :['gini', 'entropy'],\n",
    "              \n",
    "                }\n",
    "    \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-23T13:02:01.416Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-23T06:08:52.209Z"
    }
   },
   "outputs": [],
   "source": [
    "model=\"RF.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwmNqojeGz5s"
   },
   "source": [
    "## Training Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-19T12:43:23.187Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.878, Val_Acc: 0.880, est=0.880, cfg={'activation': 'tanh', 'solver': 'adam', 'hidden_layer_sizes': (100,), 'alpha': 0.1}\n",
      "Test_micro_f1: 0.8781521081299173\n",
      "Test_macro_f1: 0.6368086898135864\n",
      "Fold: 2, acc=0.882, Val_Acc: 0.873, est=0.880, cfg={'activation': 'tanh', 'solver': 'adam', 'hidden_layer_sizes': (50,), 'alpha': 0.1}\n",
      "Test_micro_f1: 0.8821868065362114\n",
      "Test_macro_f1: 0.6513706686773264\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = MLPClassifier(random_state=1,max_iter=1000)\n",
    "\n",
    "    p_grid = { 'solver':['sgd', 'adam'],\n",
    "               'activation':['logistic', 'tanh', 'relu'],\n",
    "               'alpha':[0.01,0.1],\n",
    "               'hidden_layer_sizes':[(50,),(100,)]\n",
    "             }    \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=20)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-19T12:43:24.202Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-19T12:43:24.464Z"
    }
   },
   "outputs": [],
   "source": [
    "model=\"MLP.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-17T11:40:44.443Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    model = GradientBoostingClassifier(random_state=1)\n",
    "    \n",
    "    p_grid = {  \"learning_rate\": [0.001, 0.01,0.1],\n",
    "                \"min_samples_split\": np.arange(2, 10),\n",
    "                \"max_depth\":np.arange(2, 200, 5),\n",
    "                \"n_estimators\": [50, 100, 200],\n",
    "                }\n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"GBT.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test = pd.DataFrame(t_test_tfidf_accuracy_gds,columns = ['Fold1','Fold2','Fold3','Fold4','Fold5','Fold6','Fold7','Fold8','Fold9','Fold10',])\n",
    "res_metrics = pd.DataFrame(tfidf_result_metrics_gds, columns = ['accuracy','micro_f1','macro_f1'])#,'Specificity','Precision','Sensitivity'])\n",
    "\n",
    "t_test.to_csv('t_test_tfidf_accuracy_10fold_grid_cv.csv', index = False)\n",
    "res_metrics.to_csv('tfidf_result_metrics_10fold_grid_cv.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T05:37:34.337514Z",
     "start_time": "2022-04-13T05:37:07.815967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.769, Val_Acc: 0.756, est=0.766, cfg={'var_smoothing': 1e-09}\n",
      "Test_micro_f1: 0.7686100463990316\n",
      "Test_macro_f1: 0.5928272663672685\n",
      "Fold: 2, acc=0.776, Val_Acc: 0.763, est=0.775, cfg={'var_smoothing': 1e-09}\n",
      "Test_micro_f1: 0.7762759733709905\n",
      "Test_macro_f1: 0.5985542919537808\n",
      "Fold: 3, acc=0.767, Val_Acc: 0.752, est=0.764, cfg={'var_smoothing': 1e-09}\n",
      "Test_micro_f1: 0.7667944321161992\n",
      "Test_macro_f1: 0.592415505894233\n",
      "Fold: 4, acc=0.765, Val_Acc: 0.758, est=0.761, cfg={'var_smoothing': 1e-09}\n",
      "Test_micro_f1: 0.7647770829130521\n",
      "Test_macro_f1: 0.5920497488597819\n",
      "Fold: 5, acc=0.773, Val_Acc: 0.776, est=0.772, cfg={'var_smoothing': 1e-09}\n",
      "Test_micro_f1: 0.7728464797256404\n",
      "Test_macro_f1: 0.5960970234387957\n",
      "Fold: 6, acc=0.770, Val_Acc: 0.781, est=0.765, cfg={'var_smoothing': 1e-09}\n",
      "Test_micro_f1: 0.7702239257615493\n",
      "Test_macro_f1: 0.5958297614853351\n",
      "Fold: 7, acc=0.773, Val_Acc: 0.767, est=0.772, cfg={'var_smoothing': 1e-09}\n",
      "Test_micro_f1: 0.7732499495662699\n",
      "Test_macro_f1: 0.596317679789593\n",
      "Fold: 8, acc=0.773, Val_Acc: 0.780, est=0.771, cfg={'var_smoothing': 1e-09}\n",
      "Test_micro_f1: 0.7726447448053259\n",
      "Test_macro_f1: 0.5956396617901373\n",
      "Fold: 9, acc=0.775, Val_Acc: 0.779, est=0.776, cfg={'var_smoothing': 1e-09}\n",
      "Test_micro_f1: 0.7754690336897317\n",
      "Test_macro_f1: 0.598792460191444\n",
      "Fold:10, acc=0.770, Val_Acc: 0.779, est=0.765, cfg={'var_smoothing': 1e-09}\n",
      "Test_micro_f1: 0.7700221908412346\n",
      "Test_macro_f1: 0.5961238746416978\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "#writing a loop for nested 10-fold cross-validation & parameter tuning\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = GaussianNB()\n",
    "    #cv_inner = StratifiedKFold(n_splits=5, random_state=42)\n",
    "    p_grid = [\n",
    "      {'var_smoothing':[1e-9,1e-8,1e-7,1e-6,1e-5]\n",
    "       }]\n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T05:37:54.970718Z",
     "start_time": "2022-04-13T05:37:54.945110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.769 +/- 0.010\n",
      "\n",
      "CV accuracy: 0.771 +/- 0.004\n",
      "\n",
      "CV micro F1: 0.771 +/- 0.004\n",
      "\n",
      "CV macro F1: 0.595 +/- 0.002\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.201 +/- 0.004\n",
      "\n",
      "CV Class : Hate speech Recall: 0.497 +/- 0.014\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.286 +/- 0.003\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.948 +/- 0.003\n",
      "\n",
      "CV Class : Offensive language Recall: 0.813 +/- 0.004\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.875 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.585 +/- 0.002\n",
      "\n",
      "CV Class : Neither Recall: 0.671 +/- 0.007\n",
      "\n",
      "CV Class : Neither F1 score: 0.625 +/- 0.004\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T05:37:58.311759Z",
     "start_time": "2022-04-13T05:37:58.256904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec_NB.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.768610</td>\n",
       "      <td>0.768610</td>\n",
       "      <td>0.592827</td>\n",
       "      <td>0.196159</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.281773</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.947673</td>\n",
       "      <td>0.811621</td>\n",
       "      <td>0.874386</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.947673</td>\n",
       "      <td>0.662665</td>\n",
       "      <td>0.622322</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776276</td>\n",
       "      <td>0.776276</td>\n",
       "      <td>0.598554</td>\n",
       "      <td>0.206015</td>\n",
       "      <td>0.479021</td>\n",
       "      <td>0.288118</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.944461</td>\n",
       "      <td>0.819698</td>\n",
       "      <td>0.877668</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.944461</td>\n",
       "      <td>0.678271</td>\n",
       "      <td>0.629877</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.766794</td>\n",
       "      <td>0.766794</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.196286</td>\n",
       "      <td>0.517483</td>\n",
       "      <td>0.284615</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.952439</td>\n",
       "      <td>0.808755</td>\n",
       "      <td>0.874736</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.952439</td>\n",
       "      <td>0.659064</td>\n",
       "      <td>0.617895</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.764777</td>\n",
       "      <td>0.764777</td>\n",
       "      <td>0.592050</td>\n",
       "      <td>0.194263</td>\n",
       "      <td>0.520979</td>\n",
       "      <td>0.283001</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.953395</td>\n",
       "      <td>0.804846</td>\n",
       "      <td>0.872845</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.953395</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.620303</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.772846</td>\n",
       "      <td>0.772846</td>\n",
       "      <td>0.596097</td>\n",
       "      <td>0.201767</td>\n",
       "      <td>0.479021</td>\n",
       "      <td>0.283938</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.944998</td>\n",
       "      <td>0.814747</td>\n",
       "      <td>0.875052</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.944998</td>\n",
       "      <td>0.680672</td>\n",
       "      <td>0.629301</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.768610  0.768610  0.592827               0.196159            0.500000   \n",
       "1  0.776276  0.776276  0.598554               0.206015            0.479021   \n",
       "2  0.766794  0.766794  0.592416               0.196286            0.517483   \n",
       "3  0.764777  0.764777  0.592050               0.194263            0.520979   \n",
       "4  0.772846  0.772846  0.596097               0.201767            0.479021   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.281773                286.0                      0.947673   \n",
       "1             0.288118                286.0                      0.944461   \n",
       "2             0.284615                286.0                      0.952439   \n",
       "3             0.283001                286.0                      0.953395   \n",
       "4             0.283938                286.0                      0.944998   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.811621                    0.874386   \n",
       "1                   0.819698                    0.877668   \n",
       "2                   0.808755                    0.874736   \n",
       "3                   0.804846                    0.872845   \n",
       "4                   0.814747                    0.875052   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.947673        0.662665   \n",
       "1                      3838.0           0.944461        0.678271   \n",
       "2                      3838.0           0.952439        0.659064   \n",
       "3                      3838.0           0.953395        0.663866   \n",
       "4                      3838.0           0.944998        0.680672   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.622322            833.0  \n",
       "1         0.629877            833.0  \n",
       "2         0.617895            833.0  \n",
       "3         0.620303            833.0  \n",
       "4         0.629301            833.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"NB.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hate_speech_TF-IDF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "211px",
    "width": "228px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
