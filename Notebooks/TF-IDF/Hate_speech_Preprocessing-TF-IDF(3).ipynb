{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1\">TF-IDF</a></span></li><li><span><a href=\"#In-86-start-here\" data-toc-modified-id=\"In-86-start-here-2\">In 86 start here</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3\">Word2Vec</a></span></li><li><span><a href=\"#Doc2Vec\" data-toc-modified-id=\"Doc2Vec-4\">Doc2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Splitting-the-data\" data-toc-modified-id=\"Splitting-the-data-4.1\">Splitting the data</a></span></li></ul></li><li><span><a href=\"#Training-Logistic-Regression\" data-toc-modified-id=\"Training-Logistic-Regression-5\">Training Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-SVC\" data-toc-modified-id=\"Training-SVC-5.1\">Training SVC</a></span></li><li><span><a href=\"#Training-DescisionTree\" data-toc-modified-id=\"Training-DescisionTree-5.2\">Training DescisionTree</a></span></li><li><span><a href=\"#Training-RandomForest\" data-toc-modified-id=\"Training-RandomForest-5.3\">Training RandomForest</a></span></li><li><span><a href=\"#Training-Multilayer-Perceptron\" data-toc-modified-id=\"Training-Multilayer-Perceptron-5.4\">Training Multilayer Perceptron</a></span></li><li><span><a href=\"#Training-Gradient-Boosting\" data-toc-modified-id=\"Training-Gradient-Boosting-5.5\">Training Gradient Boosting</a></span></li></ul></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-6\">Naive Bayes</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:12:25.064043Z",
     "start_time": "2022-04-11T11:12:22.472141Z"
    },
    "id": "qZmuEjZft6HC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:12:25.075794Z",
     "start_time": "2022-04-11T11:12:25.072919Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:12:25.258653Z",
     "start_time": "2022-04-11T11:12:25.082199Z"
    },
    "id": "2g-tlAV2xeg4"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/coea/Hatespeech/labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:12:25.281464Z",
     "start_time": "2022-04-11T11:12:25.276585Z"
    },
    "id": "DiarcwG6MxR9"
   },
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0',axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:12:25.312275Z",
     "start_time": "2022-04-11T11:12:25.297766Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "id": "7kT-aKnpzmz-",
    "outputId": "3be1c8ed-c133-4d9d-b2e7-cafd8bf75fde"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:12:26.071051Z",
     "start_time": "2022-04-11T11:12:26.060459Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GSFMzROzsym",
    "outputId": "11572627-7167-48d5-ab0f-a7aa63c8f654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['count', 'hate_speech', 'offensive_language', 'neither', 'class',\n",
       "       'tweet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:12:26.299691Z",
     "start_time": "2022-04-11T11:12:26.260142Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "PPI6y2mU3P9S",
    "outputId": "2dd30c6e-1363-4dfc-b93a-7d3b16970c9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.243473</td>\n",
       "      <td>0.280515</td>\n",
       "      <td>2.413711</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>1.110277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.883060</td>\n",
       "      <td>0.631851</td>\n",
       "      <td>1.399459</td>\n",
       "      <td>1.113299</td>\n",
       "      <td>0.462089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count   hate_speech  offensive_language       neither  \\\n",
       "count  24783.000000  24783.000000        24783.000000  24783.000000   \n",
       "mean       3.243473      0.280515            2.413711      0.549247   \n",
       "std        0.883060      0.631851            1.399459      1.113299   \n",
       "min        3.000000      0.000000            0.000000      0.000000   \n",
       "25%        3.000000      0.000000            2.000000      0.000000   \n",
       "50%        3.000000      0.000000            3.000000      0.000000   \n",
       "75%        3.000000      0.000000            3.000000      0.000000   \n",
       "max        9.000000      7.000000            9.000000      9.000000   \n",
       "\n",
       "              class  \n",
       "count  24783.000000  \n",
       "mean       1.110277  \n",
       "std        0.462089  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        2.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:12:26.582207Z",
     "start_time": "2022-04-11T11:12:26.476186Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "zoaS7B0m3fug",
    "outputId": "6d8f8e97-aebe-4a09-ce79-8c25f129d899"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">hate_speech</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">neither</th>\n",
       "      <th colspan=\"8\" halign=\"left\">offensive_language</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>3.108392</td>\n",
       "      <td>0.648084</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>2.256643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>0.755944</td>\n",
       "      <td>0.487653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19190.0</td>\n",
       "      <td>3.268890</td>\n",
       "      <td>0.923024</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19190.0</td>\n",
       "      <td>0.180459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19190.0</td>\n",
       "      <td>3.003544</td>\n",
       "      <td>0.954097</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4163.0</td>\n",
       "      <td>3.172712</td>\n",
       "      <td>0.746097</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4163.0</td>\n",
       "      <td>0.062935</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4163.0</td>\n",
       "      <td>0.264233</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         count                                              hate_speech  \\\n",
       "         count      mean       std  min  25%  50%  75%  max       count   \n",
       "class                                                                     \n",
       "0       1430.0  3.108392  0.648084  3.0  3.0  3.0  3.0  9.0      1430.0   \n",
       "1      19190.0  3.268890  0.923024  3.0  3.0  3.0  3.0  9.0     19190.0   \n",
       "2       4163.0  3.172712  0.746097  3.0  3.0  3.0  3.0  9.0      4163.0   \n",
       "\n",
       "                ...  neither      offensive_language                           \\\n",
       "           mean ...      75%  max              count      mean       std  min   \n",
       "class           ...                                                             \n",
       "0      2.256643 ...      0.0  4.0             1430.0  0.755944  0.487653  0.0   \n",
       "1      0.180459 ...      0.0  3.0            19190.0  3.003544  0.954097  2.0   \n",
       "2      0.062935 ...      3.0  9.0             4163.0  0.264233  0.461737  0.0   \n",
       "\n",
       "                           \n",
       "       25%  50%  75%  max  \n",
       "class                      \n",
       "0      0.0  1.0  1.0  4.0  \n",
       "1      3.0  3.0  3.0  9.0  \n",
       "2      0.0  0.0  1.0  4.0  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"class\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:12:26.659531Z",
     "start_time": "2022-04-11T11:12:26.653741Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlrXEEjfRpU-",
    "outputId": "b2d7f10c-544b-4008-c75f-b1610f707312"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1430"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.groupby(\"class\").describe()[\"class\"==0]\n",
    "sum(df[\"class\"]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:12:28.081393Z",
     "start_time": "2022-04-11T11:12:28.072411Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kd3x0b6kTR2C",
    "outputId": "0fd8fb36-a08a-4437-bc22-29dcb4bde58b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    19190\n",
       "2     4163\n",
       "0     1430\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:37.778331Z",
     "start_time": "2022-04-11T11:13:37.773392Z"
    },
    "id": "aQWMCpzRFbA4"
   },
   "outputs": [],
   "source": [
    "def percentage_class(idx):\n",
    "    Percentage = 100 * float(sum(df[\"class\"]==idx))/float(len(df[\"class\"]))\n",
    "    return Percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:38.024634Z",
     "start_time": "2022-04-11T11:13:38.011235Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIY1NFjBRJKU",
    "outputId": "ed2113a7-0550-437f-d8ba-09c6b50e785b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the percenatge of '0' class 5.770084332001775\n",
      "the percenatge of '1' class 77.43211072105879\n",
      "the percenatge of '2' class 16.797804946939433\n"
     ]
    }
   ],
   "source": [
    "print(\"the percenatge of '0' class\", percentage_class(0))\n",
    "print(\"the percenatge of '1' class\", percentage_class(1))\n",
    "print(\"the percenatge of '2' class\", percentage_class(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:38.272694Z",
     "start_time": "2022-04-11T11:13:38.175960Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "GtRcXmguS0zE",
    "outputId": "e97f821f-5f7a-40f2-c045-039d5561314d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd57dc61f60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADuCAYAAAAZZe3jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3XmYXFWB/vHvqe7q6u7qJJ2EbGS7CYEQ2WRLCCBkQBy0FIFRRkQJIG6gooNL6c/BdhznV4obIoiIjLgwOC4zLJewCIQAsoQlpNlCIFRIIBtJp9L7UnXmj1uShSbp7lT1qVv1fp6nnu6udFW9Bcnbp+899xxjrUVERMIj4jqAiIgMjopbRCRkVNwiIiGj4hYRCRkVt4hIyKi4RURCRsUtIhIyKm4RkZBRcYuIhIyKW0QkZFTcIiIho+IWEQkZFbeISMiouEVEQkbFLSISMipuEZGQUXGLiISMiltEJGRU3CIiIaPiFhEJGRW3iEjIqLhFREJGxS0iEjIqbhGRkFFxi4iEjIpbRCRkVNwiIiGj4hYRCRkVt4hIyKi4RURCptp1AJG94SX9CNAIjM7f4kA0f6ve4bbr1wboBbqBnvytC+jI39rzH9uAlnQqYYftTYnsgbFWfx+ltHhJfwQwbYfbVGAC28t5x9tIghIupj5gM7Bpl9vGHT7fAKSB19KpRK7IeaTCqbhl2HlJvwY4AJgD7MdbS7rRXbq91g2sBlb1c3s5nUq0OcwmZULFLUXjJf0oMBs4FDgEOIigrGcAVQ6jubQBaAaW5W9PAy+kU4k+p6kkVFTcUhD5Y83vAI4B5gNHAQcCNS5zhUQ38Czbi3wZsCydSmxzmkpKlopbhsRL+o1sL+n5wFxglNNQ5SUHPAM89PdbOpVIO00kJUPFLQPiJf0xwCnAu4HjCEbTxT4pKDtbA9wPLAYWp1OJl93GEVdU3NIvL+lXEYyiTwX+ETgazfsvNWuAO4HbgLvTqUSH4zwyTFTc8iYv6U8hKOlTgZMJpttJOHQB9wG3ArelU4k1jvNIEam4K5yX9KcBZwH/THBCUcrDcvIlDjyqC4jKi4q7AnlJf1/gwwRlfQw6Vl3u1gD/Bfw2nUo84zqM7D0Vd4Xwkv544EMEZX08Ol5dqZ4GfgfcmE4lXncdRoZGxV3GvKRfDbwfuJDguHWlXvQib5UjOCb+W+Av6VSi1XEeGQQVdxnykv4M4JPAecAkt2kkBDqAPwA/TacSy1yHkT1TcZeJ/JWLpwIX5z/qUIgMxYPAT4H/0WX4pUvFHXL5lfQ+BVwEzHQcR8rHWuDnwLXpVOIN12FkZyrukPKS/ljgEuBzaL61FE8XcBNwhQ6jlA4Vd8jkp/JdCnyaYNMAkeHiA/+WTiUecx2k0qm4Q8JL+jOBrwELgZjjOFLZ7iIo8IdcB6lUKu4Sly/sbwNno+l8UlruAr6ZTiWWug5SaVTcJSq/bOo3gc+jNa2ltN1CUODNroNUChX3Hhhjrie4iGWjtfbgYr9efteYi4B/BcYW+/VECiQHXEtQ4Jtdhyl3Ku49MMacQLDT92+KXdxe0j8D+B6wfzFfR6SItgCXAdekU4ms6zDlSsU9AMYYD7itWMXtJf0jgR8BJxTj+UUceBr4QjqVWOI6SDlScQ9AsYrbS/p1wL8TzMfWiUcpR38AvpxOJda6DlJOVNwDUIzi9pL+AuA6YL9CPadIiWonOGfzE60LXhgq7gEoZHF7SX8k8AOCFfu0DrZUkvuB87Tp8d7TQkTDyEv67weeI1i5T6UtleZEYLmX9C90HSTsNOLeA2PMfwELgH2ADcC3rLW/GsxzeEl/FHA18NGCBxQJp9uBC9OpxDrXQcJIxV1kXtKfS7BIzwzXWURKzBbg4nQqcZPrIGGj4i4SL+kbgsWg/gOIOo4jUsquBy5KpxLdroOEhYq7CPKHRn4DnOY6i0hILAXO1LTBgVFxF5iX9A8F/oKm+YkM1kbgrHQqcb/rIKVOs0oKyEv6HwEeQaUtMhTjgb96Sf8S10FKnUbcBeIl/a8CKTTNT6QQfgd8Kp1KdLoOUopU3Hspv0nvFQRbiIlI4TwOvC+dSmxyHaTUqLj3gpf0a4HfA2e6ziJSplYC79HVljtTcQ+Rl/THALcCx7rOIlLm1gGnplOJ5a6DlAoV9xB4SX86cCcw23UWkQrRQlDe2qgYFfegeUl/KrAE8BxHEak0rcD7tca3pgMOipf0JwL3oNIWcWEEsMhL+ie7DuKaRtwD5CX9fYDFwEGOo4hUujbgH9KpxOOug7ii4h6A/CXs9wJHuM4iIgBsAo5LpxIrXQdxQYdK9sBL+g3AIlTaIqVkHHCXl/QnuQ7igop7N7ykXw3cDMx3nUVE3sIjOOY9ynWQ4abi3r0fAye5DiEib+sw4GYv6cdcBxlOKu634SX989Fl7CJhcCLwa9chhpNOTvYjv2vNEqCifoqLhNzn0qnEVa5DDAcV9y68pD8BeAKY7DqLiAxKD3BsOpV4wnWQYlNx78BL+lHgPuA411kGo3fzWjbd8r03v+7bup7G4z9G9+sv0Lsl2FAk19VOpDbOvudf2e9z2FyWdTd8ieoRYxn/oW8BsOnWy+ndtJq6/Y5m9IkLAdj6t5uo2Wc69QfofK2UpFeAI9KpxFbXQYqp2nWAEvN9QlbaANGxU94sZJvLsvbqhdQfMJ+RR3/wze/Zcu91RGLxt32O1sdvITp2KranA4Ceja8QqY6x7wU/Y8NN3yTX3U6ut5ue11fQeOxHivuGRIZuBvCfwBmugxSTTk7meUn/JCD0O290rX6aaOMkqkeNf/M+ay0dLzxIfM4J/T6mb9sbdK5aSsNh73nzPhOpJtfXjbU5bK4PTITMA79j1PHnFP09iOyl072k/0XXIYpJxQ14SX8kwU/p0O9e0/78Eup3Kejutc9SFW8kOqb/w/Yt91xL44ILMGb724/uM5WqulGs+/Ul1M+aS1/LOqy1xCbOKmp+kQL5vpf0D3EdolhU3IEfAtNch9hbNttL50uPET/w+J3ub3/u/rcdbXe89BiReGO/hTzm3Z9i3/OvZOTcM9n6wG9pfNfHyPztD2z63xSty+4oynsQKZAocI2X9EM/GOtPxRe3l/QXABe6zlEInaueoGbCflTFR795n81l6XjxYeoP7L+4u197js6Vj7L25xew6Zbv07V6OW/c+oOdvqdj5SPUTJyF7e2id+s6xp2epGPFQ+R6u4r6fkT20rHAJ1yHKIaKLu781mPXus5RKP2NrLvSy4iOnUL1yH36fczoE89jysU3MOWz1zPutK9SO/1Q9vnAl9/8c5vtY9vjNzNy3j9h+7p582iSzUG2r1hvRaRQvpdf2bOsVHRxA18D9ncdohByPV10pZdRP3vnndTan1/yljLva93Mhj9+a0DP2/qkT8PBJxOJ1hIdNwPb183rv7qYmomziNQ2FCy/SJGMAS53HaLQKnYet5f0xwGrALWPSHmzwIJy2jmnkkfcX0elLVIJDHCVl/TLpu/K5o0MRn7fyItc5xCRYXMwcJbrEIVSkcUNXIYWkBKpNJeVy6i7LN7EYHhJ/wDgPNc5RGTYzQH+2XWIQqi44gaa0BotIpXqa64DFEJFzSrxkv5EYA0qbpFKdmo6lbjTdYi9UWkj7vNQaYtUuq+4DrC3KmbEnV+zYCWwn+ssIuKUBWakU4nVroMMVSWNuE9GpS0iwbzuUK9PXEnF/SnXAUSkZIS6uCviUImX9McDawmWehQRgWCLs6dchxiKShlxn45KW0R2FtpRd6UU9z+6DiAiJefssF5JGcrQg+El/SqCE5MiIjvaF5jrOsRQlH1xA/OAUa5DiEhJepfrAENRCcWtwyQi8naO3/O3lJ5KKO73uA4gIiXruDBuKFzWxe0l/ZHA0a5ziEjJGkuwamColHVxA4cAVa5DiEhJC91x7koobhGR3TnOdYDBGlBxG2P2M8bE8p8vMMZ8wRjTWNxoBaHiFpE92d91gMEa6Ij7z0DWGDMLuBaYCtxYtFSFM9t1ABEpedNcBxisgRZ3zlrbB5wBXGmt/QowqXixCkarAYrInkz0kn6olsQYaHH3GmPOBhYCt+XvK+k3mv8fMdV1DhEpeRFgiusQgzHQ4j4fmA9811r7ijFmBvDb4sUqiHFoRomIDEyoDpcMaBsva+1zwBcAjDGjgRHW2u8VM1gBxF0HEJHQCFVxD3RWyWJjzEhjzBjgSeCXxpgfFTfaXqtzHUBEQmOC6wCDMdBDJaOstduAM4HfWGvnAe8uXqyCqHcdQERCo8Z1gMEYaHFXG2MmAWex/eRkqVNxi8hAlfRki10N6Bg38G/AncCD1tqlxpiZBDumlzIdKpG3mG5ee2Vsw1Ornquj2gabxopgs/WtkHAdY8AGenLyj8Afd/h6FfBPxQpVILWuA0jpaaSz9y+9N57c2me23RevW7EoHm9/qjY2rt2Y2Rgz0IGMlJ97XQcYjAH9RTXG1AKfAA5ih0K01l5QpFyF0O46gJSe5XbmLGvJjMCOOq2t4+jT2joAaDembUl93Yrb4/WtT9TWjmmNmAMxJlTHPWWv9LgOMBgDHWH8FniBYFOCfyPYZPP5YoUqkDdcB5DSY4lE1jP6xUm07LTcb9zahve2dxz53vagyLuM6XygrvYpvyGeebw2NjoTiRxIfr0eKUtlWdyzrLUfNsZ80Fp7gzHmRuCBYgYrABW39Ouh3CHtH6pastvvqbW27pSOzsNP6egEoNvQ9XBd3TI/Xr/10braUS1Bkes8SvnodB1gMAZa3L35j1uNMQcD64HxxYlUMJtcB5DSdEt2/pg9FfeuYpbaBR2d71yQL/Ie6Hmsrna53xDf8kht7Yg3qiIHYowu+gqvtOsAg2GstXv+JmMuJFgh8FDgP4EG4DJr7TXFjbd3vKTfgWaXyC6i9PW8GDs3Z0zhTmD3Qd8TtbEVfkN800N1tfGNVVWzMWZkoZ6/mFZcuoJIXQRjDFTBrKZZO/15tiPL2l+spXdLLzZr2ee9+zD6XaPpXtfNmmvWYLOWyedNpn5WPTZrSf8wzfRLphOJhWq5/9nNC5tfdB1ioAY6q+S6/Kf3AzOLF6fgNhOyxWOk+HqprskQf7qR9sMK9ZzVUD2vq/ugeV3dAGQh+1Qs9rzfUL/xwfq6uvVBkY8q1OsV2oyvzaB6RP91sPmezcQmx5j+pen0betj5ddXMmr+KLYs3sKkcyZRs08N636/jmmfn8aWe7fQOL8xbKWdJWQj7t0WtzHmX3b359baUr/s/TVU3NKPpbnZLadUPVm056+CqqO6u+cc1d09h80t5CDXHKtZ4TfE1y+pq4u9Xl11gA2WkCh5xhhyXTmsteS6c1TFqzARg6ky5Hpy5HpymCpDtj3LtmXb8C71XEcerDXNC5vL6uTkiPxHy1svVtjzMRb3ngLmuQ4hpcfPHtNQzOLeVQQih3X3zD6su2f2N2jBgn2upmbl7Q316xbX10XXVFfPssaMG7ZAOzKQ/kEagDH/MIYxC3b+eTLm5DG8esWrrPjiCnJdOaZ+diomYhhz8hjWXrsW2xccKtl4y0bGvX8cJhK665pech1gsHZb3NbabwMYY24ALrHWbs1/PRr4YfHj7bXh+5cpoXJP7vD9rSVnjJt9Vw2Yg3p69j9oS8/+X9myFYAV0eiq2xvir91bX1f1arR6v5wxw7Lw0cz/N5Po6Ch92/pIX54mNilGfPb286xtz7RRO60W72sePRt7SF+eZtbsWdSMrWHm14Mjp90buult6aV231rW/CI47j3hzAnEJoZiBmV5FfcODv17aQNYa1uMMYcXKVMhPeE6gJSmVuKjOomtqKe7ZLa3m93bO3N2y9aZX2oJ/qm9HK1O3x6Pr7knXhdJR6MzssbsW4zXjY4OlumoHlnNiCNG0Lmqc6fibnmghXGJcRhjiE2IUTOuhu513dTP3L4c0IY/b2DCmRPYfPdmxpw4hug+UTb8aQNTPxOKvUzKtrgjxpjR1toWgPzyrmG4PPgZgon1ugJO3uIZ622Ya1aUTHHvar/ePu/zWzPe57dmAFhdXb1mUUP96r/W1/NyTXR6nzF73Yq57hw2Z6mqqyLXnaPt2TbGn7bzTN+asTW0PddGfHacvkwf3eu6qRm3/Z9U+wvtRBujxCbGyPXkgoOqhuDzcHjOdYDBGuh0wHOBb7B9vZIPE+yGU+q74OAl/SeAI1znkNJzftWih78V/e181zmG6rXqqtfviMdfuStel1tZUzO11xhvsM/Rs7GHV698FQCbtYw6ZhTjTxvPlnu3ADDmpDH0tvSy9rq19GX6wMK4xDgaj20MHmMt6R+kmfrZqVQ3VNP1ehdrf7EWm7Xsu3Bf4vuX/NT2PmBM88LmVtdBBmNAxQ1gjHkHcFL+y3vzu+KUPC/pXwt80nUOKT0T2bLhkdrPhWoB/d1ZX1W1/q54/ao74vXZFbGayT3BKp6ye480L2we0g9vY8ypwBUEWyReZ61NFTTZbgz4cEe+qENR1ru4DxW39GM9Yyb02qrVUZOd7jpLIUzMZieeu6114rnbgsHjpqrIprvi9S/dGa/vfa6mZlK3MbOCq2xkB0NaFdAYUwVcBZwCrAWWGmNuGa4BbRiOU++tOwh+HaqE9yqDtNJOXvsO82pZFPeuxmVz487Z1jbunG1tAGyJRDb/NV6/8o54fXdzrGZilzH7Y0yorpQpgruG+Li5wEv5Ja4xxtwEfJBhGtyWfZmlU4kWL+n/DTjBdRYpPXfnjsy9I/Kq6xjDYkwuN/as1raxZ7UGRZ6JRLbeW1/34qKG+s6nY7HxHcYcQDCSrBRbgAeH+NjJwJodvl7LMF4zUvbFnfc/qLilH372mCmXVP+P6xhOjMrlGs9oa597RluwdH2rqbjNJW5vXticdR1iKMr5f8qO/gT8CG1VJbt40U6dkbNmU8RYN1ctlpAR1o6ssM0lbtmLx74G7Dgdc0r+vmEx4FklYecl/QeB41znkNKzpOaLj0yLbDzGdY5S12VM54N1tS/4DfHM0tpYYyYSmRPizSW2AlOaFzYPaacsE/wm8iJwMkFhLwU+aq19tnAR316ljLgBfoeKW/qxOHdY97mRu13HKHm11ta9u6Pz8HdvX5O8++G62qf9hnjLI+HbXOL6oZY2gLW2zxjzOYJN1KuA64ertKGyRtxxgpMJo11nkdIy1zz//H/HvjPHdY6wy28u8UIINpfIAbOaFza/4jrIUFVMcQN4ST8FfM11DiktEXLZl2Mf6zSGBtdZykkJby5xS/PC5g+6DrE3KulQCcDPgEupvPctu5EjUvUGo1aMI3Ok6yzlpL/NJZbtvLnEAdaYRgfRfurgNQuqokbcAF7SvxE423UOKS0/jV65+LSqhxe4zlFJ8ptLrBzmzSWebV7YfHCRX6PoKnHk+SNU3LKL27LzG0+reth1jIriaHOJKwv8fE5U3IgbwEv6S4B3uc4hpaOW7s7nY+dXG0PUdRbZLthcon7tffX11auj1TNzxkzci6fbAkxtXtjcUah8rlTiiBuCE5R/cx1CSkcXsbpW6p4ZSWfof40uJ8HmEpmZX2oJ1iRfFa1efXs8/uo98brIK9GolzVm8iCe7rvlUNpQoSNuAC/p/xfwEdc5pHTcEE3df2LV8hNd55CBW11dvXZRQ316AJtLvAy8I2ybAr+dSh1xQzDq/iAQlgsGpMj83Ly6E6uWu44hgzC9r2/KZ7Zum/KZrduA3W4ukSyX0oYKHnEDeEn/O8A3XeeQ0tBIa8tTsU83GqM1bcrF+qqq9X8ZEV900RfXXOA6SyFV+lq8KeB11yGkNGxlxOhuoqtc55DCmZjNjrto67afuM5RaBVd3OlUoh34uuscUjqet9OGbYU3GRZX0pQpu+NfFV3cAOlU4jfAItc5pDTckZ1bSRsJlLtVwGWuQxRDxRd33vnAJtchxL3bc/M81xmkILLAx2nKhGr39oFScQPpVGID8AnXOcS9NXb85D4b0XmP8Pv/NGXK9loNFXdeOpW4FbjGdQ5x7xU7cdiW+7zg5k7GX97KwVe37XT/lY/2cODP2jjo6ja+endXv4/98cPdHHR1Gwdf3cbZf+6gqy+YIXbOXzo49OdtfOOe7Y/79yXd/O8LvcV7I6VlKfBt1yGKScW9s0uBFa5DiFv35I4Ytn0Iz3tnlDs+Vr/Tffe90sfNK3p5+jNxnr2ogS8f+9bdwl7bluOnj/Xw+CfjPHNRA9kc3PRML8s3ZKmrNiz/bANLX8+S6bKsa83x6GtZTj+wIq7mbwfOoSnT5zpIMam4d5BOJTqAjwIVMzSRt7ote8y+w/VaJ0yvZkzdztPGf/54D8njY8Sqg/vHx/v/Z9qXg84+6MtZOnph3xERohHo7LPkrKU3C1URuOy+br69IKw7jA3axTRlVroOUWwq7l2kU4kngYtd5xB3nrEz9stZWly9/oubczywuo9517Vx4q/bWfraW38BmDwywpfn1zDtx61M+mEbo2rhPftVM2dcFePqIxzxi3Y+cEA1L23JkbNwxKSKmCzzQ5oyN7gOMRxU3P1IpxK/BK5wnUNcMWYdY52N2vpysKXT8sgn4lx+Si1n/amDXa9wbum03Lyij1cuaeD1f2mgvQd+tzy4ovsnp9ay7DMNXHpsjH+9r5vvnBTju0u6OeuPHfzyibK56ntXPvBV1yGGi4r77V0K3OE6hLjxYPaQTlevPWWk4cw5UYwxzJ1cRcTAGx07F/dfV/UxozHCuHiEaJXhzDnV/G3NziPzm1/o5chJEdp6LC+35PjvD9fzp+d76egtu2UungXOpimTcx1kuKi430Y6lcgCZwFPu84iw+/W3Pxi78Tytk4/MMp96eDc2oubs/RkYZ/6nY+DTxtleOS1LB29Fmst97ySZc4+2w+H9GYtP3m0h68eF6OzlzcXX8nmoGfYTr0Oi83AaeU6X/vtqLh3I51KtALvI9gdXirII7k5B1pL0UfdZ/+5g/m/amfF5hxTftTKr57s4YLDo6xqsRx8dRsf+VMnN5xehzGG11tzvO/3wXLS86ZU86E51Rzxi3YO+Xk7OQufOnL7rJGrlvaw8LAo9VHDoRMidPRZDvl5G0dOqqKxtmzW0OoEzqApU3Hry1T06oAD5SX9g4AHgNGus8jweTL26WVjTOs7XeeQfnUDH6Apc7frIC5oxD0A6VTiWeBkgl/LpEI8lpudcZ1B+tULfKhSSxtU3AOWTiWeAk4C3nCdRYbHbdn5Da4zyFtkCU5E3uY6iEsq7kFIpxLLgX8ANrrOIsV3X+6dB1hLeZ3KC7cccC5NmT+7DuKainuQ0qnEM8ACYL3jKFJk7dSN6CBW9lfhhUQvQWnf6DpIKVBxD0E6lXieoLy1ilyZW56bucF1BqENSNCU+b3rIKVCxT1E6VRiBXACWpSqrN2em1cxi3yUqA3Agko+EdkfFfdeSKcSLwPHAPpLVabuzB69n+sMFewl4FiaMk+4DlJqVNx7KZ1KbCW4SOcq11mk8DYyelyPrU67zlGBlhKUdsVdXDMQKu4CSKcSfelU4nPA50CzEMrNi3aKrpwdXjcAJ9KU0XaCb0PFXUDpVOIq4L2ALtwoI3dljyqba8RLXA/wWZoy59GUcbbIVxiouAssnUrcTXDc+1nXWaQw/Ny8aa4zVIC1wAk0ZbR94ACouIsgnUq8AByFjnuXhZft5GlZa3TRVfHcBxxJU+ZR10HCQsVdJOlUoit/3Ps0dJl86L1qx7/sOkMZ6gO+A5xCU0Y/GAdBxV1k+d3jD0VTBkNtce6d2oe0sFYQzBq5jKaMTugPkop7GKRTiXXAPwJfJjgBIyFza3b+eNcZyoQFfgocTlNmqeswYaX1uIeZl/QPB64HtM5ziBhyuVWxj7UZw0jXWULsVeB8mjL3ug4SdhpxD7P88rBHE2xs2uE4jgyQJRLZSKMWnBqaHHAtcIhKuzA04nbIS/ozgKuBU11nkT37cfSqxWdUPbTAdY6QeRy4mKbMY66DlBONuB1KpxKvpFOJ9wL/RPBrpJSwW7PztXXdwG0BPgvMU2kXnkbcJcJL+vXAN4FLgRrHcaQfMXq6XoidFzFG/392wxKcw0nSlNE02CJRcZcYL+lPB74NfBz9RlRyno5d2DzKdBziOkeJegD4ii6kKT4Vd4nykv47gO8Cp7vOIttdH/3+4pOqli1wnaPEPAN8vdL3gRxOGtGVqHQq8Vw6lTiDYN2T+1znkYCfPSbuOkMJeRk4FzhMpT28NOIOCS/pnwL8B8EaKOLISNoyT8c+NdIYKnnFwFcJLlX/NU2ZPtdhKpGKO0S8pG8INm24lGC3eXHg+dh5K+tMz/6uczjwDPAD4EaaMloCwCEVd0h5Sf8IggI/C6h2HKei/KmmaclRkRdPcJ1jGC0Gvk9TZtFQn8AYMxX4DTCBYObJtdbaKwoTr/KouEPOS/pTgUuAT4Iuxx4OF1b5f/tm9PfHus5RZFngz8DlNGUe39snM8ZMAiZZa580xowAngBOt9Y+t7fPXYlU3GXCS/ojCcr7C4AW/i+iyWxa91DtJZNc5yiSTcCvgWuKud+jMeZm4GfWWq2aOQQq7jLjJf0IcApwPsFUwpjbROVpZezja6MmO8V1jgKxwD0E64n8b7GPXxtjPGAJcLC1dlsxX6tcqbjLmJf0RwPnEJT4EY7jlJU7a7760OzI2uNc59hL64H/BK4brt3UjTENwP3Ad621fxmO1yxHKu4K4SX9w4ALCIp8rOM4ofeV6j88cHH1ze9ynWMI2oFbgJuA24dzOp8xJgrcBtxprf3RcL1uOVJxVxgv6dcQ7ET/YeAD6ITmkMwxq19eFPv6fq5zDFAn4AN/AHwXO6gbYwxwA7DFWvvF4X79cqPirmD5En8P8CHg/WgkPiirYudsjhhbqv/NuoE7Ccr6FpoybS7DGGOOJ1jLpJlgfW6Ab1hrb3eXKrxU3AKAl/SrgOMJNjf+IBCW0aQzD8a+8NgU88Zc1zl2sBpYBNwO3EtTpt1xHikSFbf0y0v6BwInAycBC4AxTgOVoP+ovu7+j1bfe6LDCD3AgwRFvYimjOZEVwgVt+xRforhOwlK/CTgXUCD01Al4JjIs8/eVPPdg4bxJXsJdpR5IH9b7Ppap8b0AAABzklEQVQQiLih4pZB85J+FJhLsF7KfIKFrypuF/Qqsn0vxT7eYwz1RXqJNuBhthf1oy5OLErpUXFLQeQvvT9qh9uRVMDJzidin35qrGk9vABP1Q4sB54ClgFPAstoymQL8NxSZlTcUjT5zZCPIrj450DgAGAWZbQ121XRKxYnqh5dMMiHvQ48S1DSfy/qF2nK5Hb7KJE8FbcMq/zslenAbIIi3/HjZAjXOtenRh598pqaK/q7KnU9sLKf20s0ZTqGMaKUIRW3lAwv6dcRlPdkYMoOn0/I3ybmP7rcbb0N2EBQzBtG0/rqU7Wf3gysBV7Lf1yjk4ZSTCpuCR0v6VcDI/K3ht18bGBg2/NZgqsLW/O3th0+3+m+dCrRU8j3IjIUKm4RkZDRZsEiIiGj4hYRCRkVt4hIyKi4RURCRsUtIhIyKm4RkZBRcYuIhIyKW0QkZFTcIiIho+IWEQkZFbeISMiouEVEQkbFLSISMipuEZGQUXGLiISMiltEJGRU3CIiIaPiFhEJGRW3iEjIqLhFREJGxS0iEjIqbhGRkFFxi4iEjIpbRCRkVNwiIiGj4hYRCRkVt4hIyKi4RURCRsUtIhIyKm4RkZBRcYuIhMz/AbvGyrK7E3xoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd57dd1cf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ploting percentage of each class\n",
    "df[\"class\"].value_counts().plot(kind='pie',autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHSLZ2oHJEfo"
   },
   "source": [
    "Finding out the Maximum length tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T11:25:15.769122Z",
     "start_time": "2022-03-17T11:25:15.720986Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "oHh5QglEIwlt",
    "outputId": "06a1c4ef-ae14-4b03-b582-15ba202a90d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['tweet'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T11:25:16.096468Z",
     "start_time": "2022-03-17T11:25:15.795541Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "0mNOMylQKD8C",
    "outputId": "cd1e1db0-f49d-40a1-a43d-6e83cfd76f20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f798f9fac18>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f798fae25f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['length'].plot(bins=50, kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxCU9bWANEWN"
   },
   "source": [
    "#Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTmX_141EwZR"
   },
   "source": [
    "Here copying the data from **df** to **dataF** and creating new variable with same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:40.450114Z",
     "start_time": "2022-04-11T11:13:40.445133Z"
    },
    "id": "a_fg58OnRuMx"
   },
   "outputs": [],
   "source": [
    "#copying data from original variable to another varible.\n",
    "\n",
    "dataF = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:41.601626Z",
     "start_time": "2022-04-11T11:13:41.588578Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "Y8nVur4mR5Hi",
    "outputId": "2b894f96-6e1a-4a66-e7bf-0353f5d0a86d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing top 3 rows in the data set\n",
    "\n",
    "dataF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T11:25:22.325854Z",
     "start_time": "2022-03-17T11:25:22.298404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:33:24.796694Z",
     "start_time": "2022-03-05T09:33:23.284203Z"
    },
    "id": "C13nkq6e5c4P"
   },
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords            ## it will check stop words present in data\n",
    "from nltk.stem.porter import PorterStemmer   ## it will use for stemmming the words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:28.490985Z",
     "start_time": "2022-03-05T05:04:28.486933Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:29.075152Z",
     "start_time": "2022-03-05T05:04:29.052599Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-1ee3083fd02b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m#For expanding Contractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "def con(words):\n",
    "    return [contractions.fix(word) for word in words.split()]    #For expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:30.271965Z",
     "start_time": "2022-03-05T05:04:30.267829Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataF['Contractions']=dataF['tweet'].apply(lambda x: con(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:30.995790Z",
     "start_time": "2022-03-05T05:04:30.977843Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:31.708535Z",
     "start_time": "2022-03-05T05:04:31.705355Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install TextBlob\n",
    "#from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.249394Z",
     "start_time": "2022-03-05T05:04:32.245934Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gingerit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.391937Z",
     "start_time": "2022-03-05T05:04:32.388591Z"
    }
   },
   "outputs": [],
   "source": [
    "#from gingerit.gingerit import GingerIt\n",
    "#parser = GingerIt()\n",
    "#tweet=parser.parse(dataF['tweet'][23])\n",
    "#tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.503268Z",
     "start_time": "2022-03-05T05:04:32.500517Z"
    }
   },
   "outputs": [],
   "source": [
    "#blob=TextBlob(dataF['tweet'][23])\n",
    "#blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:33.089007Z",
     "start_time": "2022-03-05T05:04:33.079920Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Expanding whatsapp language slangs\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"./HateSpeechNew/PreProcessing/sms_slang_translator-master/slang.txt\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if str(_str).upper() == row[0]:\n",
    "                    # If match found replace it with its appropriate phrase in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    return ' '.join(user_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.087982Z",
     "start_time": "2022-03-05T05:04:33.545921Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataF['ExpandedSlangs']=dataF['tweet'].apply(lambda x: translator(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.127335Z",
     "start_time": "2022-03-05T05:05:28.118138Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.175036Z",
     "start_time": "2022-03-05T05:05:28.152537Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'con' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-20407348bd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ExpandedSlangs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Contractions for expanded slangs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3192\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-20407348bd53>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ExpandedSlangs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Contractions for expanded slangs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'con' is not defined"
     ]
    }
   ],
   "source": [
    "dataF['Preprocessed_Initial']=dataF['ExpandedSlangs'].apply(lambda x: con(x)) #Contractions for expanded slangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:03.502948Z",
     "start_time": "2022-03-05T05:07:03.483677Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:03.996685Z",
     "start_time": "2022-03-05T05:07:03.993840Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJO5rARp8Bb2",
    "outputId": "fc26ef1f-29b7-4db9-db52-9bcc2a77cbd2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.121200Z",
     "start_time": "2022-03-05T05:07:04.118575Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctTSRXQR0vO4",
    "outputId": "1a59d48e-acec-4839-8f8e-bf1fc8180a6b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.244346Z",
     "start_time": "2022-03-05T05:07:04.241803Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iG6koGQnpfOF",
    "outputId": "e9572c08-72e0-4cdb-a447-f4b9fc6d3fbe"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.382959Z",
     "start_time": "2022-03-05T05:07:04.375813Z"
    },
    "id": "6146Ji8QjJPt"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the tweets.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer,word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    #for text in sentence:\n",
    "    #  if text in ['html','www','http','https','rt']:\n",
    "    #sentence= [sentence.replace(w,'') for w in sentence if w in ['html','www','http','https','rt']]\n",
    "    sentence=sentence.replace('rt',\"\")\n",
    "    sentence=sentence.replace('www',\"\")\n",
    "    sentence=sentence.replace('http',\"\")\n",
    "    sentence=sentence.replace('https',\"\")\n",
    "    sentence=sentence.replace('html',\"\")\n",
    "    sentence=sentence.replace('*',\"\")\n",
    "    sentence=sentence.replace('#',\"\")\n",
    "    cleanr = re.compile('<.?>')\n",
    "    cleantext1 = re.sub(cleanr, '', sentence)\n",
    "    cleantext = re.sub('@[^\\s]+','',cleantext1)\n",
    "    #rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', cleantext)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    tokens = tokenizer.tokenize(rem_num)\n",
    "    #tokens = word_tokenize(rem_num)\n",
    "    filtered_words = [w for w in tokens if len(w) > 2] # if not w in stopwords.words('english')]\n",
    "    #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    #lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.605980Z",
     "start_time": "2022-03-05T05:07:04.477566Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnDBnHcyLRZk",
    "outputId": "c3638e8c-646c-4c09-b7e2-0a8fe26246e4",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Preprocessed_Initial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Preprocessed_Initial'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-a01c6738a8ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PreprocessedTweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#dataF['tweet_prepro'] = dataF['tweet'].apply(lambda x: preprocess(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#dataF['tweet_prepro'].tail()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Preprocessed_Initial'"
     ]
    }
   ],
   "source": [
    "dataF['PreprocessedTweet']=dataF['Preprocessed_Initial'].map(lambda s:preprocess(s))\n",
    "#dataF['tweet_prepro'] = dataF['tweet'].apply(lambda x: preprocess(x))\n",
    "#dataF['tweet_prepro'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:05.904217Z",
     "start_time": "2022-03-05T05:07:05.886888Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:33:06.476959Z",
     "start_time": "2022-03-05T09:33:06.472222Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split(\" \",text) \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.187016Z",
     "start_time": "2022-03-05T05:19:23.336Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['final_tweet_tokens'] = dataF['PreprocessedTweet'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.188320Z",
     "start_time": "2022-03-05T05:19:23.644Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.189637Z",
     "start_time": "2022-03-05T05:19:24.031Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF['PreprocessedTweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.190956Z",
     "start_time": "2022-03-05T05:19:24.453Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.192175Z",
     "start_time": "2022-03-05T05:19:24.910Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "word_c = WordCloud(collocations=False,background_color='white',mode='RGB',scale=4).generate(str(dataF['PreprocessedTweet']))\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(18,8),facecolor='w')\n",
    "plt.imshow(word_c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.193460Z",
     "start_time": "2022-03-05T05:19:25.528Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatizing(words):\n",
    "    lemmatizer =WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.194684Z",
     "start_time": "2022-03-05T05:19:25.996Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['LemmaWords']=dataF['final_tweet_tokens'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.195891Z",
     "start_time": "2022-03-05T05:19:26.615Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.197110Z",
     "start_time": "2022-03-05T05:19:26.972Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF['LemmaWords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.198387Z",
     "start_time": "2022-03-05T05:19:27.407Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.to_csv('Hate_spech_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.199640Z",
     "start_time": "2022-03-05T05:19:27.920Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.200844Z",
     "start_time": "2022-03-05T05:19:30.381Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.202042Z",
     "start_time": "2022-03-05T05:19:31.178Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.203266Z",
     "start_time": "2022-03-05T05:19:31.370Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['KerasTokens']=dataF['PreprocessedTweet'].apply(text_to_word_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.204501Z",
     "start_time": "2022-03-05T05:19:31.537Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.205742Z",
     "start_time": "2022-03-05T05:19:32.391Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "word_c = WordCloud(collocations=False,background_color='white',mode='RGB',scale=4).generate(str(dataF['LemmaWords']))\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(18,8),facecolor='w')\n",
    "plt.imshow(word_c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.206953Z",
     "start_time": "2022-03-05T05:19:32.619Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['Lemma_Preprocessed']=dataF['LemmaWords'].map(lambda s:preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.208149Z",
     "start_time": "2022-03-05T05:19:32.830Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T08:36:34.150352Z",
     "start_time": "2022-03-15T08:36:33.698741Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF = pd.read_csv(\"/home/coea/Hatespeech/preprocessing/Hate_spech_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T08:36:34.250170Z",
     "start_time": "2022-03-15T08:36:34.223693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "      <th>Preprocessed_Initial</th>\n",
       "      <th>PreprocessedTweet</th>\n",
       "      <th>final_tweet_tokens</th>\n",
       "      <th>LemmaWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>['!!!', 'RT', '@mayasolovely:', 'As', 'a', 'wo...</td>\n",
       "      <td>woman you should not complain about cleaning y...</td>\n",
       "      <td>['woman', 'you', 'should', 'not', 'complain', ...</td>\n",
       "      <td>['woman', 'you', 'should', 'not', 'complain', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "      <td>['!!!!!', 'RT', '@mleew17:', 'boy', 'That Is',...</td>\n",
       "      <td>boy that cold tyga down bad for cuffin that ho...</td>\n",
       "      <td>['boy', 'that', 'cold', 'tyga', 'down', 'bad',...</td>\n",
       "      <td>['boy', 'that', 'cold', 'tyga', 'down', 'bad',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "      <td>['!!!!!!!', 'RT', '@UrKindOfBrand', 'Dog', 'RT...</td>\n",
       "      <td>dog you ever fuck bitch and she sta cry you co...</td>\n",
       "      <td>['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...</td>\n",
       "      <td>['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>['!!!!!!!!!', 'RT', '@C_G_Anderson:', '@viva_b...</td>\n",
       "      <td>she look like tranny</td>\n",
       "      <td>['she', 'look', 'like', 'tranny']</td>\n",
       "      <td>['she', 'look', 'like', 'tranny']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>['!!!!!!!!!!!!!', 'RT', '@ShenikaRoberts:', 'T...</td>\n",
       "      <td>the shit you hear about might true might faker...</td>\n",
       "      <td>['the', 'shit', 'you', 'hear', 'about', 'might...</td>\n",
       "      <td>['the', 'shit', 'you', 'hear', 'about', 'might...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "\n",
       "                                Preprocessed_Initial  \\\n",
       "0  ['!!!', 'RT', '@mayasolovely:', 'As', 'a', 'wo...   \n",
       "1  ['!!!!!', 'RT', '@mleew17:', 'boy', 'That Is',...   \n",
       "2  ['!!!!!!!', 'RT', '@UrKindOfBrand', 'Dog', 'RT...   \n",
       "3  ['!!!!!!!!!', 'RT', '@C_G_Anderson:', '@viva_b...   \n",
       "4  ['!!!!!!!!!!!!!', 'RT', '@ShenikaRoberts:', 'T...   \n",
       "\n",
       "                                   PreprocessedTweet  \\\n",
       "0  woman you should not complain about cleaning y...   \n",
       "1  boy that cold tyga down bad for cuffin that ho...   \n",
       "2  dog you ever fuck bitch and she sta cry you co...   \n",
       "3                               she look like tranny   \n",
       "4  the shit you hear about might true might faker...   \n",
       "\n",
       "                                  final_tweet_tokens  \\\n",
       "0  ['woman', 'you', 'should', 'not', 'complain', ...   \n",
       "1  ['boy', 'that', 'cold', 'tyga', 'down', 'bad',...   \n",
       "2  ['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...   \n",
       "3                  ['she', 'look', 'like', 'tranny']   \n",
       "4  ['the', 'shit', 'you', 'hear', 'about', 'might...   \n",
       "\n",
       "                                          LemmaWords  \n",
       "0  ['woman', 'you', 'should', 'not', 'complain', ...  \n",
       "1  ['boy', 'that', 'cold', 'tyga', 'down', 'bad',...  \n",
       "2  ['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...  \n",
       "3                  ['she', 'look', 'like', 'tranny']  \n",
       "4  ['the', 'shit', 'you', 'hear', 'about', 'might...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:33:35.291436Z",
     "start_time": "2022-03-05T09:33:35.287464Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T08:36:39.761456Z",
     "start_time": "2022-03-15T08:36:38.002766Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T03:37:56.594816Z",
     "start_time": "2022-03-07T03:37:56.228628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 590)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=50, max_features=1000, stop_words=None,tokenizer=tokenize,analyzer='word')\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataF['PreprocessedTweet'].values.astype('U'))\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In 86 start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:55.753511Z",
     "start_time": "2022-04-11T11:13:53.665076Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.read_csv('/home/coea/Hatespeech/TF-IDF/TF-IDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:55.814961Z",
     "start_time": "2022-04-11T11:13:55.812305Z"
    }
   },
   "outputs": [],
   "source": [
    "# tfidf_df = pd.DataFrame(tfidf_df.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:55.862500Z",
     "start_time": "2022-04-11T11:13:55.859810Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:55.886778Z",
     "start_time": "2022-04-11T11:13:55.883428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 589)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 1.06 s, total: 3min 34s\n",
      "Wall time: 17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3628394, 5058200)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_tweet = dataF['Lemma_Preprocessed'].apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            vector_size=500, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 32, # no.of cores\n",
    "            seed = 34\n",
    ") \n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(dataF['Lemma_Preprocessed']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 500)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 500)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 500 )\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pandas==0.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/dstuser/anaconda3/lib/python3.7/site-packages (4.32.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LabeledSentence' from 'gensim.models.doc2vec' (/home/dstuser/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-8d3be766ad86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#tqdm.pandas(desc=\"progress-bar\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabeledSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LabeledSentence' from 'gensim.models.doc2vec' (/home/dstuser/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "#tqdm.pandas(desc=\"progress-bar\") \n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(TaggedDocument(s, [\"tweet_\" + str(i)]))\n",
    "    return output\n",
    "\n",
    "labeled_tweets = add_label(tokenized_tweet) # label all the tweets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time \n",
    "model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for â€˜distributed memoryâ€™ model\n",
    "                                  dm_mean=1, # dm_mean = 1 for using mean of the context word vectors\n",
    "                                  vector_size=1000, # no. of desired features\n",
    "                                  window=5, # width of the context window                                  \n",
    "                                  negative=7, # if > 0 then negative sampling will be used\n",
    "                                  min_count=5, # Ignores all words with total frequency lower than 5.                                  \n",
    "                                  workers=32, # no. of cores                                  \n",
    "                                  alpha=0.1, # learning rate                                  \n",
    "                                  seed = 23, # for reproducibility\n",
    "                                 ) \n",
    "\n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n",
    "\n",
    "model_d2v.train(labeled_tweets, total_examples= len(dataF['Lemma_Preprocessed']), epochs=15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docvec_arrays = np.zeros((len(tokenized_tweet), 1000)) \n",
    "for i in range(len(dataF)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,1000))    \n",
    "\n",
    "docvec_df = pd.DataFrame(docvec_arrays) \n",
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:14:02.632607Z",
     "start_time": "2022-04-11T11:14:02.304976Z"
    },
    "id": "53bWppiTiWGx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, dataF['class'].values, \\\n",
    "                                                    stratify = dataF['class'].values,shuffle = True,\\\n",
    "                                                    random_state=1,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:14:19.707681Z",
     "start_time": "2022-04-11T11:14:19.614096Z"
    },
    "id": "Obk37BzJsRwW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing necessary librarys machine learning algorthms\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# PNN algorithm not imported\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#importing all necessary metrics\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:14:22.763244Z",
     "start_time": "2022-04-11T11:14:22.759145Z"
    }
   },
   "outputs": [],
   "source": [
    "technique='TF-IDF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T06:51:13.210645Z",
     "start_time": "2022-03-16T06:48:47.899923Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.898, Val_Acc: 0.887, est=0.888, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8981238652410732\n",
      "Test_macro_f1: 0.7193030454149679\n",
      "Fold: 2, acc=0.898, Val_Acc: 0.888, est=0.889, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8977203954004438\n",
      "Test_macro_f1: 0.7165124659190477\n",
      "Fold: 3, acc=0.899, Val_Acc: 0.891, est=0.887, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8993342747629616\n",
      "Test_macro_f1: 0.7212807980756742\n",
      "Fold: 4, acc=0.900, Val_Acc: 0.893, est=0.887, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8997377446035909\n",
      "Test_macro_f1: 0.7170804613551636\n",
      "Fold: 5, acc=0.899, Val_Acc: 0.891, est=0.888, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8991325398426467\n",
      "Test_macro_f1: 0.7178905090925659\n",
      "Fold: 6, acc=0.896, Val_Acc: 0.895, est=0.887, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8957030461972968\n",
      "Test_macro_f1: 0.7107190991825804\n",
      "Fold: 7, acc=0.899, Val_Acc: 0.886, est=0.887, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8985273350817027\n",
      "Test_macro_f1: 0.7183322255403467\n",
      "Fold: 8, acc=0.899, Val_Acc: 0.897, est=0.888, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8993342747629616\n",
      "Test_macro_f1: 0.7196749909181065\n",
      "Fold: 9, acc=0.897, Val_Acc: 0.881, est=0.887, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.8967117207988703\n",
      "Test_macro_f1: 0.715397912123867\n",
      "Fold:10, acc=0.899, Val_Acc: 0.901, est=0.887, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.898930804922332\n",
      "Test_macro_f1: 0.7191592311831174\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    LR_model = LogisticRegression(random_state=1,C=100)\n",
    "    \n",
    "    # define gridsearch CV\n",
    "    p_grid = {'penalty': ['l1', 'l2']}\n",
    "    \n",
    "    clf = GridSearchCV(estimator = LR_model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:45:08.321833Z",
     "start_time": "2022-03-05T09:45:08.310319Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.899 +/- 0.006\n",
      "\n",
      "CV accuracy: 0.901 +/- 0.001\n",
      "\n",
      "CV micro F1: 0.901 +/- 0.001\n",
      "\n",
      "CV macro F1: 0.706 +/- 0.009\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.578 +/- 0.016\n",
      "\n",
      "CV Class : Hate speech Recall: 0.230 +/- 0.021\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.329 +/- 0.024\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.937 +/- 0.002\n",
      "\n",
      "CV Class : Offensive language Recall: 0.950 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.943 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.795 +/- 0.003\n",
      "\n",
      "CV Class : Neither Recall: 0.908 +/- 0.005\n",
      "\n",
      "CV Class : Neither F1 score: 0.848 +/- 0.002\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T12:34:11.451620Z",
     "start_time": "2022-02-28T12:34:11.308636Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF_LR.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.904983</td>\n",
       "      <td>0.904983</td>\n",
       "      <td>0.725584</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.936012</td>\n",
       "      <td>0.952840</td>\n",
       "      <td>0.944351</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.936012</td>\n",
       "      <td>0.902761</td>\n",
       "      <td>0.853091</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.717650</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.251748</td>\n",
       "      <td>0.357320</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.935723</td>\n",
       "      <td>0.952058</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.935723</td>\n",
       "      <td>0.903962</td>\n",
       "      <td>0.851810</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.902764</td>\n",
       "      <td>0.902764</td>\n",
       "      <td>0.715340</td>\n",
       "      <td>0.612069</td>\n",
       "      <td>0.248252</td>\n",
       "      <td>0.353234</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.935690</td>\n",
       "      <td>0.951537</td>\n",
       "      <td>0.943547</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.935690</td>\n",
       "      <td>0.902761</td>\n",
       "      <td>0.849238</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.903571</td>\n",
       "      <td>0.715922</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.244755</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.936137</td>\n",
       "      <td>0.951016</td>\n",
       "      <td>0.943518</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.936137</td>\n",
       "      <td>0.911164</td>\n",
       "      <td>0.854249</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.904781</td>\n",
       "      <td>0.904781</td>\n",
       "      <td>0.716139</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.244755</td>\n",
       "      <td>0.349127</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.937692</td>\n",
       "      <td>0.952840</td>\n",
       "      <td>0.945205</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.937692</td>\n",
       "      <td>0.909964</td>\n",
       "      <td>0.854085</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.904983  0.904983  0.725584               0.641667            0.269231   \n",
       "1  0.903571  0.903571  0.717650               0.615385            0.251748   \n",
       "2  0.902764  0.902764  0.715340               0.612069            0.248252   \n",
       "3  0.903571  0.903571  0.715922               0.614035            0.244755   \n",
       "4  0.904781  0.904781  0.716139               0.608696            0.244755   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.379310                286.0                      0.936012   \n",
       "1             0.357320                286.0                      0.935723   \n",
       "2             0.353234                286.0                      0.935690   \n",
       "3             0.350000                286.0                      0.936137   \n",
       "4             0.349127                286.0                      0.937692   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.952840                    0.944351   \n",
       "1                   0.952058                    0.943820   \n",
       "2                   0.951537                    0.943547   \n",
       "3                   0.951016                    0.943518   \n",
       "4                   0.952840                    0.945205   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.936012        0.902761   \n",
       "1                      3838.0           0.935723        0.903962   \n",
       "2                      3838.0           0.935690        0.902761   \n",
       "3                      3838.0           0.936137        0.911164   \n",
       "4                      3838.0           0.937692        0.909964   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.853091            833.0  \n",
       "1         0.851810            833.0  \n",
       "2         0.849238            833.0  \n",
       "3         0.854249            833.0  \n",
       "4         0.854085            833.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"LR.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHwC6nvnrbkq"
   },
   "source": [
    "## Training SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T20:06:25.011870Z",
     "start_time": "2022-02-28T12:34:49.360042Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.901, Val_Acc: 0.899, est=0.898, cfg={'C': 1.5, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.9011498890457939\n",
      "Test_macro_f1: 0.668201953997292\n",
      "Fold: 2, acc=0.901, Val_Acc: 0.897, est=0.899, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.9005446842848497\n",
      "Test_macro_f1: 0.6530421797297578\n",
      "Fold: 3, acc=0.901, Val_Acc: 0.903, est=0.899, cfg={'C': 2.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.9011498890457939\n",
      "Test_macro_f1: 0.676610513609346\n",
      "Fold: 4, acc=0.900, Val_Acc: 0.900, est=0.899, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.8999394795239056\n",
      "Test_macro_f1: 0.6647199093626323\n",
      "Fold: 5, acc=0.900, Val_Acc: 0.896, est=0.899, cfg={'C': 2.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.9001412144442204\n",
      "Test_macro_f1: 0.662273233100421\n",
      "Fold: 6, acc=0.901, Val_Acc: 0.901, est=0.899, cfg={'C': 1.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.9007464192051644\n",
      "Test_macro_f1: 0.6630573950097846\n",
      "Fold: 7, acc=0.902, Val_Acc: 0.897, est=0.900, cfg={'C': 2.0, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.9021585636473674\n",
      "Test_macro_f1: 0.6705676764607696\n",
      "Fold: 8, acc=0.900, Val_Acc: 0.900, est=0.899, cfg={'C': 1.5, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.900342949364535\n",
      "Test_macro_f1: 0.6668695897667272\n",
      "Fold: 9, acc=0.900, Val_Acc: 0.895, est=0.900, cfg={'C': 2.5, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.9001412144442204\n",
      "Test_macro_f1: 0.66623503573352\n",
      "Fold:10, acc=0.900, Val_Acc: 0.910, est=0.898, cfg={'C': 1.5, 'kernel': 'linear'}\n",
      "Test_micro_f1: 0.9001412144442204\n",
      "Test_macro_f1: 0.6670125133481521\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = SVC(random_state=42,max_iter=-1)\n",
    "    param_range = [0.1,0.5,1.0,1.5,2.0,2.5,3.0]\n",
    "    \n",
    "    p_grid =  {'C': param_range, \n",
    "               #'loss':['hinge', 'squared_hinge'],\n",
    "               'kernel':['linear','poly', 'rbf', 'sigmoid']\n",
    "               }   \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T20:06:25.064089Z",
     "start_time": "2022-02-28T20:06:25.055149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.900 +/- 0.004\n",
      "\n",
      "CV accuracy: 0.901 +/- 0.001\n",
      "\n",
      "CV micro F1: 0.901 +/- 0.001\n",
      "\n",
      "CV macro F1: 0.666 +/- 0.006\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.487 +/- 0.032\n",
      "\n",
      "CV Class : Hate speech Recall: 0.123 +/- 0.013\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.196 +/- 0.018\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.935 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Recall: 0.951 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.943 +/- 0.000\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.795 +/- 0.004\n",
      "\n",
      "CV Class : Neither Recall: 0.934 +/- 0.003\n",
      "\n",
      "CV Class : Neither F1 score: 0.859 +/- 0.002\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T20:06:25.132344Z",
     "start_time": "2022-02-28T20:06:25.103353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF_SVM.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.901150</td>\n",
       "      <td>0.901150</td>\n",
       "      <td>0.668202</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.125874</td>\n",
       "      <td>0.200557</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.934271</td>\n",
       "      <td>0.951798</td>\n",
       "      <td>0.942953</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.934271</td>\n",
       "      <td>0.933974</td>\n",
       "      <td>0.861096</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.900545</td>\n",
       "      <td>0.900545</td>\n",
       "      <td>0.653042</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.094406</td>\n",
       "      <td>0.154286</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.935005</td>\n",
       "      <td>0.952058</td>\n",
       "      <td>0.943455</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.935005</td>\n",
       "      <td>0.939976</td>\n",
       "      <td>0.861386</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.901150</td>\n",
       "      <td>0.901150</td>\n",
       "      <td>0.676611</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.146853</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.936793</td>\n",
       "      <td>0.949974</td>\n",
       "      <td>0.943338</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.936793</td>\n",
       "      <td>0.935174</td>\n",
       "      <td>0.856986</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.899939</td>\n",
       "      <td>0.899939</td>\n",
       "      <td>0.664720</td>\n",
       "      <td>0.479452</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>0.194986</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.936072</td>\n",
       "      <td>0.949974</td>\n",
       "      <td>0.942972</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.936072</td>\n",
       "      <td>0.936375</td>\n",
       "      <td>0.856202</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.900141</td>\n",
       "      <td>0.900141</td>\n",
       "      <td>0.662273</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.933555</td>\n",
       "      <td>0.951798</td>\n",
       "      <td>0.942588</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.933555</td>\n",
       "      <td>0.931573</td>\n",
       "      <td>0.859358</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.901150  0.901150  0.668202               0.493151            0.125874   \n",
       "1  0.900545  0.900545  0.653042               0.421875            0.094406   \n",
       "2  0.901150  0.901150  0.676611               0.525000            0.146853   \n",
       "3  0.899939  0.899939  0.664720               0.479452            0.122378   \n",
       "4  0.900141  0.900141  0.662273               0.464789            0.115385   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.200557                286.0                      0.934271   \n",
       "1             0.154286                286.0                      0.935005   \n",
       "2             0.229508                286.0                      0.936793   \n",
       "3             0.194986                286.0                      0.936072   \n",
       "4             0.184874                286.0                      0.933555   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.951798                    0.942953   \n",
       "1                   0.952058                    0.943455   \n",
       "2                   0.949974                    0.943338   \n",
       "3                   0.949974                    0.942972   \n",
       "4                   0.951798                    0.942588   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.934271        0.933974   \n",
       "1                      3838.0           0.935005        0.939976   \n",
       "2                      3838.0           0.936793        0.935174   \n",
       "3                      3838.0           0.936072        0.936375   \n",
       "4                      3838.0           0.933555        0.931573   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.861096            833.0  \n",
       "1         0.861386            833.0  \n",
       "2         0.856986            833.0  \n",
       "3         0.856202            833.0  \n",
       "4         0.859358            833.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"SVM.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIXIwRTzeUiD"
   },
   "source": [
    "## Training DescisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T10:52:01.241515Z",
     "start_time": "2022-03-01T03:34:37.934628Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.867, Val_Acc: 0.856, est=0.863, cfg={'max_features': 'sqrt', 'min_samples_split': 9, 'criterion': 'entropy', 'max_depth': 110, 'min_samples_leaf': 3}\n",
      "Test_micro_f1: 0.8672584224329231\n",
      "Test_macro_f1: 0.6252220560978636\n",
      "Fold: 2, acc=0.859, Val_Acc: 0.867, est=0.869, cfg={'max_features': 'sqrt', 'min_samples_split': 2, 'criterion': 'entropy', 'max_depth': 86, 'min_samples_leaf': 7}\n",
      "Test_micro_f1: 0.8587855557797056\n",
      "Test_macro_f1: 0.6425770739563639\n",
      "Fold: 3, acc=0.855, Val_Acc: 0.854, est=0.864, cfg={'max_features': 'sqrt', 'min_samples_split': 8, 'criterion': 'entropy', 'max_depth': 90, 'min_samples_leaf': 3}\n",
      "Test_micro_f1: 0.8547508573734114\n",
      "Test_macro_f1: 0.622248327350648\n",
      "Fold: 4, acc=0.870, Val_Acc: 0.870, est=0.863, cfg={'max_features': 'sqrt', 'min_samples_split': 9, 'criterion': 'entropy', 'max_depth': 104, 'min_samples_leaf': 4}\n",
      "Test_micro_f1: 0.8696792414766996\n",
      "Test_macro_f1: 0.64767940332273\n",
      "Fold: 5, acc=0.861, Val_Acc: 0.846, est=0.867, cfg={'max_features': 'sqrt', 'min_samples_split': 7, 'criterion': 'entropy', 'max_depth': 72, 'min_samples_leaf': 3}\n",
      "Test_micro_f1: 0.8614081097437967\n",
      "Test_macro_f1: 0.6611301456026135\n",
      "Fold: 6, acc=0.855, Val_Acc: 0.870, est=0.864, cfg={'max_features': 'sqrt', 'min_samples_split': 9, 'criterion': 'entropy', 'max_depth': 84, 'min_samples_leaf': 4}\n",
      "Test_micro_f1: 0.8553560621343554\n",
      "Test_macro_f1: 0.6378075783313712\n",
      "Fold: 7, acc=0.851, Val_Acc: 0.850, est=0.864, cfg={'max_features': 'sqrt', 'min_samples_split': 9, 'criterion': 'entropy', 'max_depth': 92, 'min_samples_leaf': 4}\n",
      "Test_micro_f1: 0.8505144240468026\n",
      "Test_macro_f1: 0.6206337959317368\n",
      "Fold: 8, acc=0.858, Val_Acc: 0.854, est=0.859, cfg={'max_features': 'sqrt', 'min_samples_split': 7, 'criterion': 'entropy', 'max_depth': 98, 'min_samples_leaf': 3}\n",
      "Test_micro_f1: 0.8581803510187613\n",
      "Test_macro_f1: 0.6244073280839905\n",
      "Fold: 9, acc=0.854, Val_Acc: 0.848, est=0.863, cfg={'max_features': 'sqrt', 'min_samples_split': 7, 'criterion': 'entropy', 'max_depth': 102, 'min_samples_leaf': 3}\n",
      "Test_micro_f1: 0.8537421827718377\n",
      "Test_macro_f1: 0.6359382834477523\n",
      "Fold:10, acc=0.863, Val_Acc: 0.872, est=0.861, cfg={'max_features': 'sqrt', 'min_samples_split': 2, 'criterion': 'entropy', 'max_depth': 90, 'min_samples_leaf': 5}\n",
      "Test_micro_f1: 0.8634254589469437\n",
      "Test_macro_f1: 0.6214103265475126\n",
      "CPU times: user 3h 2min 18s, sys: 18min 9s, total: 3h 20min 27s\n",
      "Wall time: 7h 17min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "#writing a loop for nested 10-fold cross-validation & parameter tuning\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    model = DecisionTreeClassifier(random_state=1)\n",
    "    \n",
    "    params_range = np.arange(2, 20, 2)\n",
    "    min_split_param = np.arange(2, 10)\n",
    "    \n",
    "    # define gridsearch CV\n",
    "    p_grid={\n",
    "            'min_samples_split': min_split_param,\n",
    "            'criterion':['gini','entropy'], \n",
    "            'max_depth': np.arange(2, 250, 2),\n",
    "            'max_features':['sqrt','log2'],\n",
    "            'min_samples_leaf': np.arange(2, 10)\n",
    "               \n",
    "               } \n",
    "\n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T10:52:01.346930Z",
     "start_time": "2022-03-01T10:52:01.334115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.859 +/- 0.010\n",
      "\n",
      "CV accuracy: 0.859 +/- 0.006\n",
      "\n",
      "CV micro F1: 0.859 +/- 0.006\n",
      "\n",
      "CV macro F1: 0.634 +/- 0.013\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.381 +/- 0.031\n",
      "\n",
      "CV Class : Hate speech Recall: 0.167 +/- 0.034\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.231 +/- 0.036\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.917 +/- 0.007\n",
      "\n",
      "CV Class : Offensive language Recall: 0.919 +/- 0.009\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.918 +/- 0.004\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.695 +/- 0.023\n",
      "\n",
      "CV Class : Neither Recall: 0.822 +/- 0.026\n",
      "\n",
      "CV Class : Neither F1 score: 0.753 +/- 0.017\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-01T10:52:01.459434Z",
     "start_time": "2022-03-01T10:52:01.432934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF_DT.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.867258</td>\n",
       "      <td>0.867258</td>\n",
       "      <td>0.625222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.118881</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.916710</td>\n",
       "      <td>0.929130</td>\n",
       "      <td>0.922878</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.916710</td>\n",
       "      <td>0.839136</td>\n",
       "      <td>0.777531</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.858786</td>\n",
       "      <td>0.858786</td>\n",
       "      <td>0.642577</td>\n",
       "      <td>0.420635</td>\n",
       "      <td>0.185315</td>\n",
       "      <td>0.257282</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.916297</td>\n",
       "      <td>0.915581</td>\n",
       "      <td>0.915939</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.916297</td>\n",
       "      <td>0.828331</td>\n",
       "      <td>0.754511</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.854751</td>\n",
       "      <td>0.854751</td>\n",
       "      <td>0.622248</td>\n",
       "      <td>0.341270</td>\n",
       "      <td>0.150350</td>\n",
       "      <td>0.208738</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.914093</td>\n",
       "      <td>0.917665</td>\n",
       "      <td>0.915876</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.914093</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.742131</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.869679</td>\n",
       "      <td>0.869679</td>\n",
       "      <td>0.647679</td>\n",
       "      <td>0.380597</td>\n",
       "      <td>0.178322</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.918829</td>\n",
       "      <td>0.931996</td>\n",
       "      <td>0.925365</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.918829</td>\n",
       "      <td>0.819928</td>\n",
       "      <td>0.774816</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.861408</td>\n",
       "      <td>0.861408</td>\n",
       "      <td>0.661130</td>\n",
       "      <td>0.391813</td>\n",
       "      <td>0.234266</td>\n",
       "      <td>0.293217</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.934970</td>\n",
       "      <td>0.902814</td>\n",
       "      <td>0.918611</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.934970</td>\n",
       "      <td>0.885954</td>\n",
       "      <td>0.771563</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.867258  0.867258  0.625222               0.333333            0.118881   \n",
       "1  0.858786  0.858786  0.642577               0.420635            0.185315   \n",
       "2  0.854751  0.854751  0.622248               0.341270            0.150350   \n",
       "3  0.869679  0.869679  0.647679               0.380597            0.178322   \n",
       "4  0.861408  0.861408  0.661130               0.391813            0.234266   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.175258                286.0                      0.916710   \n",
       "1             0.257282                286.0                      0.916297   \n",
       "2             0.208738                286.0                      0.914093   \n",
       "3             0.242857                286.0                      0.918829   \n",
       "4             0.293217                286.0                      0.934970   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.929130                    0.922878   \n",
       "1                   0.915581                    0.915939   \n",
       "2                   0.917665                    0.915876   \n",
       "3                   0.931996                    0.925365   \n",
       "4                   0.902814                    0.918611   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.916710        0.839136   \n",
       "1                      3838.0           0.916297        0.828331   \n",
       "2                      3838.0           0.914093        0.806723   \n",
       "3                      3838.0           0.918829        0.819928   \n",
       "4                      3838.0           0.934970        0.885954   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.777531            833.0  \n",
       "1         0.754511            833.0  \n",
       "2         0.742131            833.0  \n",
       "3         0.774816            833.0  \n",
       "4         0.771563            833.0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"DT.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfzInD9wC1jl"
   },
   "source": [
    "## Training RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T22:51:26.402980Z",
     "start_time": "2022-04-11T11:14:34.188292Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.892, Val_Acc: 0.893, est=0.893, cfg={'criterion': 'entropy', 'max_depth': 172, 'min_samples_split': 9, 'n_estimators': 200, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8922735525519467\n",
      "Test_macro_f1: 0.6529699264654975\n",
      "Fold: 2, acc=0.892, Val_Acc: 0.888, est=0.893, cfg={'criterion': 'entropy', 'max_depth': 182, 'min_samples_split': 9, 'n_estimators': 100, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8922735525519467\n",
      "Test_macro_f1: 0.6572266248349027\n",
      "Fold: 3, acc=0.893, Val_Acc: 0.896, est=0.893, cfg={'criterion': 'entropy', 'max_depth': 117, 'min_samples_split': 9, 'n_estimators': 50, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8928787573128909\n",
      "Test_macro_f1: 0.6552646742500361\n",
      "Fold: 4, acc=0.888, Val_Acc: 0.895, est=0.893, cfg={'criterion': 'entropy', 'max_depth': 82, 'min_samples_split': 8, 'n_estimators': 200, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8884405890659673\n",
      "Test_macro_f1: 0.6487868394318691\n",
      "Fold: 5, acc=0.891, Val_Acc: 0.884, est=0.893, cfg={'criterion': 'entropy', 'max_depth': 87, 'min_samples_split': 9, 'n_estimators': 200, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8912648779503732\n",
      "Test_macro_f1: 0.6565160835181449\n",
      "Fold: 6, acc=0.891, Val_Acc: 0.898, est=0.891, cfg={'criterion': 'entropy', 'max_depth': 122, 'min_samples_split': 8, 'n_estimators': 100, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8912648779503732\n",
      "Test_macro_f1: 0.647578040665735\n",
      "Fold: 7, acc=0.890, Val_Acc: 0.890, est=0.894, cfg={'criterion': 'entropy', 'max_depth': 112, 'min_samples_split': 7, 'n_estimators': 200, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8896509985878556\n",
      "Test_macro_f1: 0.6512951011300268\n",
      "Fold: 8, acc=0.890, Val_Acc: 0.890, est=0.892, cfg={'criterion': 'entropy', 'max_depth': 137, 'min_samples_split': 9, 'n_estimators': 200, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8904579382691143\n",
      "Test_macro_f1: 0.6465631634268383\n",
      "Fold: 9, acc=0.890, Val_Acc: 0.897, est=0.892, cfg={'criterion': 'entropy', 'max_depth': 117, 'min_samples_split': 5, 'n_estimators': 200, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.8902562033487996\n",
      "Test_macro_f1: 0.660310830866318\n",
      "Fold:10, acc=0.890, Val_Acc: 0.901, est=0.890, cfg={'criterion': 'entropy', 'max_depth': 112, 'min_samples_split': 5, 'n_estimators': 200, 'max_features': 'sqrt'}\n",
      "Test_micro_f1: 0.890054468428485\n",
      "Test_macro_f1: 0.655459249315603\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    #cv_inner = StratifiedKFold(n_splits=5, random_state=42)\n",
    "    p_grid = {'n_estimators': [50,100,200],\n",
    "              'max_depth' : np.arange(2, 200, 5),\n",
    "              'min_samples_split': np.arange(2, 10),\n",
    "              'max_features': ['sqrt', 'log2'],\n",
    "              'criterion' :['gini', 'entropy'],\n",
    "              \n",
    "                }\n",
    "    \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:26:37.001156Z",
     "start_time": "2022-04-13T03:26:36.978670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.893 +/- 0.005\n",
      "\n",
      "CV accuracy: 0.891 +/- 0.001\n",
      "\n",
      "CV micro F1: 0.891 +/- 0.001\n",
      "\n",
      "CV macro F1: 0.653 +/- 0.004\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.532 +/- 0.029\n",
      "\n",
      "CV Class : Hate speech Recall: 0.123 +/- 0.009\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.199 +/- 0.013\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.917 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Recall: 0.957 +/- 0.002\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.937 +/- 0.001\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.800 +/- 0.004\n",
      "\n",
      "CV Class : Neither Recall: 0.848 +/- 0.006\n",
      "\n",
      "CV Class : Neither F1 score: 0.824 +/- 0.004\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T03:26:42.943850Z",
     "start_time": "2022-04-13T03:26:42.904849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF_RF.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.892274</td>\n",
       "      <td>0.892274</td>\n",
       "      <td>0.652970</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.118881</td>\n",
       "      <td>0.192090</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.917248</td>\n",
       "      <td>0.958833</td>\n",
       "      <td>0.937580</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.917248</td>\n",
       "      <td>0.851140</td>\n",
       "      <td>0.829240</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.892274</td>\n",
       "      <td>0.892274</td>\n",
       "      <td>0.657227</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.129371</td>\n",
       "      <td>0.208451</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.916625</td>\n",
       "      <td>0.959614</td>\n",
       "      <td>0.937627</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.916625</td>\n",
       "      <td>0.843938</td>\n",
       "      <td>0.825602</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.892879</td>\n",
       "      <td>0.892879</td>\n",
       "      <td>0.655265</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>0.202312</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.918393</td>\n",
       "      <td>0.958833</td>\n",
       "      <td>0.938177</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.918393</td>\n",
       "      <td>0.853541</td>\n",
       "      <td>0.825305</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.888441</td>\n",
       "      <td>0.888441</td>\n",
       "      <td>0.648787</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.118881</td>\n",
       "      <td>0.194842</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.914321</td>\n",
       "      <td>0.956488</td>\n",
       "      <td>0.934929</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.914321</td>\n",
       "      <td>0.839136</td>\n",
       "      <td>0.816589</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.891265</td>\n",
       "      <td>0.891265</td>\n",
       "      <td>0.656516</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.129371</td>\n",
       "      <td>0.209632</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.916708</td>\n",
       "      <td>0.957791</td>\n",
       "      <td>0.936799</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.916708</td>\n",
       "      <td>0.846339</td>\n",
       "      <td>0.823117</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.892274  0.892274  0.652970               0.500000            0.118881   \n",
       "1  0.892274  0.892274  0.657227               0.536232            0.129371   \n",
       "2  0.892879  0.892879  0.655265               0.583333            0.122378   \n",
       "3  0.888441  0.888441  0.648787               0.539683            0.118881   \n",
       "4  0.891265  0.891265  0.656516               0.552239            0.129371   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.192090                286.0                      0.917248   \n",
       "1             0.208451                286.0                      0.916625   \n",
       "2             0.202312                286.0                      0.918393   \n",
       "3             0.194842                286.0                      0.914321   \n",
       "4             0.209632                286.0                      0.916708   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.958833                    0.937580   \n",
       "1                   0.959614                    0.937627   \n",
       "2                   0.958833                    0.938177   \n",
       "3                   0.956488                    0.934929   \n",
       "4                   0.957791                    0.936799   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.917248        0.851140   \n",
       "1                      3838.0           0.916625        0.843938   \n",
       "2                      3838.0           0.918393        0.853541   \n",
       "3                      3838.0           0.914321        0.839136   \n",
       "4                      3838.0           0.916708        0.846339   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.829240            833.0  \n",
       "1         0.825602            833.0  \n",
       "2         0.825305            833.0  \n",
       "3         0.816589            833.0  \n",
       "4         0.823117            833.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"RF.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwmNqojeGz5s"
   },
   "source": [
    "## Training Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T06:37:45.569266Z",
     "start_time": "2022-03-16T06:12:39.899422Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8aff0970b8f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m                        refit=True, n_jobs=20)\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# fitting model on parameter search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mgd_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Validating model which is trained parameter search \"inner_cv\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "  \n",
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = MLPClassifier(random_state=1,max_iter=1000)\n",
    "\n",
    "    p_grid = { 'solver':['sgd', 'adam'],\n",
    "               'activation':['logistic', 'tanh', 'relu'],\n",
    "               'alpha':[0.0001, 0.01, 0.1],\n",
    "               'hidden_layer_sizes':[(50,),(100,),(150,),(200,)]\n",
    "             }    \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=20)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-15T08:52:17.641Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T03:47:21.351671Z",
     "start_time": "2022-02-22T13:06:50.878Z"
    }
   },
   "outputs": [],
   "source": [
    "model=\"MLP.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-22T05:47:15.289Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    model = GradientBoostingClassifier(random_state=1)\n",
    "    \n",
    "    p_grid = {  \"learning_rate\": [0.001, 0.01,0.1],\n",
    "                \"min_samples_split\": np.arange(2, 10),\n",
    "                \"max_depth\":np.arange(2, 200, 5),\n",
    "                \"n_estimators\": [50, 100, 200],\n",
    "                }\n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"GBT.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T05:18:42.866076Z",
     "start_time": "2022-03-21T06:13:23.490Z"
    }
   },
   "outputs": [],
   "source": [
    "# t_test = pd.DataFrame(t_test_tfidf_accuracy_gds,columns = ['Fold1','Fold2','Fold3','Fold4','Fold5','Fold6','Fold7','Fold8','Fold9','Fold10',])\n",
    "# res_metrics = pd.DataFrame(tfidf_result_metrics_gds, columns = ['accuracy','micro_f1','macro_f1'])#,'Specificity','Precision','Sensitivity'])\n",
    "\n",
    "# t_test.to_csv('t_test_tfidf_accuracy_10fold_grid_cv.csv', index = False)\n",
    "# res_metrics.to_csv('tfidf_result_metrics_10fold_grid_cv.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T05:26:13.829888Z",
     "start_time": "2022-04-13T05:25:42.682729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.389, Val_Acc: 0.370, est=0.355, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.3889449263667541\n",
      "Test_macro_f1: 0.4120145762315741\n",
      "Fold: 2, acc=0.391, Val_Acc: 0.381, est=0.355, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.3905588057292718\n",
      "Test_macro_f1: 0.40964608601244296\n",
      "Fold: 3, acc=0.399, Val_Acc: 0.395, est=0.359, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.3988299374621747\n",
      "Test_macro_f1: 0.4176212208957768\n",
      "Fold: 4, acc=0.412, Val_Acc: 0.399, est=0.371, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.41174097236231594\n",
      "Test_macro_f1: 0.4257840541219536\n",
      "Fold: 5, acc=0.395, Val_Acc: 0.391, est=0.362, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.39540044381682465\n",
      "Test_macro_f1: 0.41634780939323135\n",
      "Fold: 6, acc=0.394, Val_Acc: 0.396, est=0.357, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.39439176921525115\n",
      "Test_macro_f1: 0.41228906686036243\n",
      "Fold: 7, acc=0.382, Val_Acc: 0.376, est=0.353, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.38188420415573937\n",
      "Test_macro_f1: 0.4050109266061029\n",
      "Fold: 8, acc=0.396, Val_Acc: 0.378, est=0.357, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.3956021787371394\n",
      "Test_macro_f1: 0.41523744591150824\n",
      "Fold: 9, acc=0.380, Val_Acc: 0.387, est=0.348, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.3800685898729069\n",
      "Test_macro_f1: 0.4045043808989677\n",
      "Fold:10, acc=0.378, Val_Acc: 0.363, est=0.351, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.3778495057494452\n",
      "Test_macro_f1: 0.4049369962391334\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "#writing a loop for nested 10-fold cross-validation & parameter tuning\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = GaussianNB()\n",
    "    #cv_inner = StratifiedKFold(n_splits=5, random_state=42)\n",
    "    p_grid = [\n",
    "      {'var_smoothing':[1e-9,1e-8,1e-7,1e-6,1e-5]\n",
    "       }]\n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T05:26:14.003181Z",
     "start_time": "2022-04-13T05:26:13.973048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.384 +/- 0.012\n",
      "\n",
      "CV accuracy: 0.392 +/- 0.010\n",
      "\n",
      "CV micro F1: 0.392 +/- 0.010\n",
      "\n",
      "CV macro F1: 0.412 +/- 0.006\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.075 +/- 0.002\n",
      "\n",
      "CV Class : Hate speech Recall: 0.741 +/- 0.015\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.137 +/- 0.003\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.931 +/- 0.004\n",
      "\n",
      "CV Class : Offensive language Recall: 0.309 +/- 0.011\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.464 +/- 0.013\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.621 +/- 0.008\n",
      "\n",
      "CV Class : Neither Recall: 0.652 +/- 0.010\n",
      "\n",
      "CV Class : Neither F1 score: 0.636 +/- 0.006\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T05:26:14.153898Z",
     "start_time": "2022-04-13T05:26:14.093157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF_NB.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.388945</td>\n",
       "      <td>0.388945</td>\n",
       "      <td>0.412015</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>0.744755</td>\n",
       "      <td>0.137154</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.936893</td>\n",
       "      <td>0.301720</td>\n",
       "      <td>0.456445</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.936893</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.642445</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.390559</td>\n",
       "      <td>0.390559</td>\n",
       "      <td>0.409646</td>\n",
       "      <td>0.075234</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.136423</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.927445</td>\n",
       "      <td>0.306410</td>\n",
       "      <td>0.460635</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.927445</td>\n",
       "      <td>0.661465</td>\n",
       "      <td>0.631881</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.398830</td>\n",
       "      <td>0.398830</td>\n",
       "      <td>0.417621</td>\n",
       "      <td>0.075736</td>\n",
       "      <td>0.737762</td>\n",
       "      <td>0.137370</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.932978</td>\n",
       "      <td>0.319177</td>\n",
       "      <td>0.475636</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.932978</td>\n",
       "      <td>0.649460</td>\n",
       "      <td>0.639858</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.411741</td>\n",
       "      <td>0.411741</td>\n",
       "      <td>0.425784</td>\n",
       "      <td>0.075959</td>\n",
       "      <td>0.720280</td>\n",
       "      <td>0.137425</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.934450</td>\n",
       "      <td>0.334289</td>\n",
       "      <td>0.492420</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.934450</td>\n",
       "      <td>0.662665</td>\n",
       "      <td>0.647507</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.416348</td>\n",
       "      <td>0.077417</td>\n",
       "      <td>0.758741</td>\n",
       "      <td>0.140499</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.933646</td>\n",
       "      <td>0.311621</td>\n",
       "      <td>0.467279</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.933646</td>\n",
       "      <td>0.656663</td>\n",
       "      <td>0.641266</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.388945  0.388945  0.412015               0.075532            0.744755   \n",
       "1  0.390559  0.390559  0.409646               0.075234            0.730769   \n",
       "2  0.398830  0.398830  0.417621               0.075736            0.737762   \n",
       "3  0.411741  0.411741  0.425784               0.075959            0.720280   \n",
       "4  0.395400  0.395400  0.416348               0.077417            0.758741   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.137154                286.0                      0.936893   \n",
       "1             0.136423                286.0                      0.927445   \n",
       "2             0.137370                286.0                      0.932978   \n",
       "3             0.137425                286.0                      0.934450   \n",
       "4             0.140499                286.0                      0.933646   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.301720                    0.456445   \n",
       "1                   0.306410                    0.460635   \n",
       "2                   0.319177                    0.475636   \n",
       "3                   0.334289                    0.492420   \n",
       "4                   0.311621                    0.467279   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.936893        0.668667   \n",
       "1                      3838.0           0.927445        0.661465   \n",
       "2                      3838.0           0.932978        0.649460   \n",
       "3                      3838.0           0.934450        0.662665   \n",
       "4                      3838.0           0.933646        0.656663   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.642445            833.0  \n",
       "1         0.631881            833.0  \n",
       "2         0.639858            833.0  \n",
       "3         0.647507            833.0  \n",
       "4         0.641266            833.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"NB.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hate_speech_TF-IDF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "211px",
    "width": "228px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
