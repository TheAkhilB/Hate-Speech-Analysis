{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1\">TF-IDF</a></span></li><li><span><a href=\"#In-86-start-here\" data-toc-modified-id=\"In-86-start-here-2\">In 86 start here</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3\">Word2Vec</a></span></li><li><span><a href=\"#Doc2Vec\" data-toc-modified-id=\"Doc2Vec-4\">Doc2Vec</a></span></li><li><span><a href=\"#LIWC\" data-toc-modified-id=\"LIWC-5\">LIWC</a></span><ul class=\"toc-item\"><li><span><a href=\"#Splitting-the-data\" data-toc-modified-id=\"Splitting-the-data-5.1\">Splitting the data</a></span></li></ul></li><li><span><a href=\"#Training-Logistic-Regression\" data-toc-modified-id=\"Training-Logistic-Regression-6\">Training Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-SVC\" data-toc-modified-id=\"Training-SVC-6.1\">Training SVC</a></span></li><li><span><a href=\"#Training-DescisionTree\" data-toc-modified-id=\"Training-DescisionTree-6.2\">Training DescisionTree</a></span></li><li><span><a href=\"#Training-RandomForest\" data-toc-modified-id=\"Training-RandomForest-6.3\">Training RandomForest</a></span></li><li><span><a href=\"#Training-Multilayer-Perceptron\" data-toc-modified-id=\"Training-Multilayer-Perceptron-6.4\">Training Multilayer Perceptron</a></span></li><li><span><a href=\"#Training-Gradient-Boosting\" data-toc-modified-id=\"Training-Gradient-Boosting-6.5\">Training Gradient Boosting</a></span></li></ul></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-7\">Naive Bayes</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:20.703262Z",
     "start_time": "2022-04-20T03:47:17.130608Z"
    },
    "id": "qZmuEjZft6HC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:20.731485Z",
     "start_time": "2022-04-20T03:47:20.727560Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:20.998436Z",
     "start_time": "2022-04-20T03:47:20.754467Z"
    },
    "id": "2g-tlAV2xeg4"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/coea/Hatespeech/labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:21.082120Z",
     "start_time": "2022-04-20T03:47:21.074818Z"
    },
    "id": "DiarcwG6MxR9"
   },
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0',axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:21.177274Z",
     "start_time": "2022-04-20T03:47:21.154636Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "id": "7kT-aKnpzmz-",
    "outputId": "3be1c8ed-c133-4d9d-b2e7-cafd8bf75fde"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:21.257735Z",
     "start_time": "2022-04-20T03:47:21.251715Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GSFMzROzsym",
    "outputId": "11572627-7167-48d5-ab0f-a7aa63c8f654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['count', 'hate_speech', 'offensive_language', 'neither', 'class',\n",
       "       'tweet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:21.388282Z",
     "start_time": "2022-04-20T03:47:21.332929Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "PPI6y2mU3P9S",
    "outputId": "2dd30c6e-1363-4dfc-b93a-7d3b16970c9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.243473</td>\n",
       "      <td>0.280515</td>\n",
       "      <td>2.413711</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>1.110277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.883060</td>\n",
       "      <td>0.631851</td>\n",
       "      <td>1.399459</td>\n",
       "      <td>1.113299</td>\n",
       "      <td>0.462089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count   hate_speech  offensive_language       neither  \\\n",
       "count  24783.000000  24783.000000        24783.000000  24783.000000   \n",
       "mean       3.243473      0.280515            2.413711      0.549247   \n",
       "std        0.883060      0.631851            1.399459      1.113299   \n",
       "min        3.000000      0.000000            0.000000      0.000000   \n",
       "25%        3.000000      0.000000            2.000000      0.000000   \n",
       "50%        3.000000      0.000000            3.000000      0.000000   \n",
       "75%        3.000000      0.000000            3.000000      0.000000   \n",
       "max        9.000000      7.000000            9.000000      9.000000   \n",
       "\n",
       "              class  \n",
       "count  24783.000000  \n",
       "mean       1.110277  \n",
       "std        0.462089  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        2.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:21.617627Z",
     "start_time": "2022-04-20T03:47:21.459088Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "zoaS7B0m3fug",
    "outputId": "6d8f8e97-aebe-4a09-ce79-8c25f129d899"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">hate_speech</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">neither</th>\n",
       "      <th colspan=\"8\" halign=\"left\">offensive_language</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>3.108392</td>\n",
       "      <td>0.648084</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>2.256643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>0.755944</td>\n",
       "      <td>0.487653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19190.0</td>\n",
       "      <td>3.268890</td>\n",
       "      <td>0.923024</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19190.0</td>\n",
       "      <td>0.180459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19190.0</td>\n",
       "      <td>3.003544</td>\n",
       "      <td>0.954097</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4163.0</td>\n",
       "      <td>3.172712</td>\n",
       "      <td>0.746097</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4163.0</td>\n",
       "      <td>0.062935</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4163.0</td>\n",
       "      <td>0.264233</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         count                                              hate_speech  \\\n",
       "         count      mean       std  min  25%  50%  75%  max       count   \n",
       "class                                                                     \n",
       "0       1430.0  3.108392  0.648084  3.0  3.0  3.0  3.0  9.0      1430.0   \n",
       "1      19190.0  3.268890  0.923024  3.0  3.0  3.0  3.0  9.0     19190.0   \n",
       "2       4163.0  3.172712  0.746097  3.0  3.0  3.0  3.0  9.0      4163.0   \n",
       "\n",
       "                ...  neither      offensive_language                           \\\n",
       "           mean ...      75%  max              count      mean       std  min   \n",
       "class           ...                                                             \n",
       "0      2.256643 ...      0.0  4.0             1430.0  0.755944  0.487653  0.0   \n",
       "1      0.180459 ...      0.0  3.0            19190.0  3.003544  0.954097  2.0   \n",
       "2      0.062935 ...      3.0  9.0             4163.0  0.264233  0.461737  0.0   \n",
       "\n",
       "                           \n",
       "       25%  50%  75%  max  \n",
       "class                      \n",
       "0      0.0  1.0  1.0  4.0  \n",
       "1      3.0  3.0  3.0  9.0  \n",
       "2      0.0  0.0  1.0  4.0  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"class\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:21.711935Z",
     "start_time": "2022-04-20T03:47:21.702712Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlrXEEjfRpU-",
    "outputId": "b2d7f10c-544b-4008-c75f-b1610f707312"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1430"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.groupby(\"class\").describe()[\"class\"==0]\n",
    "sum(df[\"class\"]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:21.788916Z",
     "start_time": "2022-04-20T03:47:21.780440Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kd3x0b6kTR2C",
    "outputId": "0fd8fb36-a08a-4437-bc22-29dcb4bde58b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    19190\n",
       "2     4163\n",
       "0     1430\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:21.863827Z",
     "start_time": "2022-04-20T03:47:21.858365Z"
    },
    "id": "aQWMCpzRFbA4"
   },
   "outputs": [],
   "source": [
    "def percentage_class(idx):\n",
    "    Percentage = 100 * float(sum(df[\"class\"]==idx))/float(len(df[\"class\"]))\n",
    "    return Percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:21.948517Z",
     "start_time": "2022-04-20T03:47:21.935484Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIY1NFjBRJKU",
    "outputId": "ed2113a7-0550-437f-d8ba-09c6b50e785b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the percenatge of '0' class 5.770084332001775\n",
      "the percenatge of '1' class 77.43211072105879\n",
      "the percenatge of '2' class 16.797804946939433\n"
     ]
    }
   ],
   "source": [
    "print(\"the percenatge of '0' class\", percentage_class(0))\n",
    "print(\"the percenatge of '1' class\", percentage_class(1))\n",
    "print(\"the percenatge of '2' class\", percentage_class(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:22.204975Z",
     "start_time": "2022-04-20T03:47:22.018475Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "GtRcXmguS0zE",
    "outputId": "e97f821f-5f7a-40f2-c045-039d5561314d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3c39f87358>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c39f87e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ploting percentage of each class\n",
    "df[\"class\"].value_counts().plot(kind='pie',autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHSLZ2oHJEfo"
   },
   "source": [
    "Finding out the Maximum length tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:22.340976Z",
     "start_time": "2022-04-20T03:47:22.291943Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "oHh5QglEIwlt",
    "outputId": "06a1c4ef-ae14-4b03-b582-15ba202a90d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['tweet'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:22.741852Z",
     "start_time": "2022-04-20T03:47:22.343905Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "0mNOMylQKD8C",
    "outputId": "cd1e1db0-f49d-40a1-a43d-6e83cfd76f20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3c39f28cf8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c39fd3048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['length'].plot(bins=50, kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxCU9bWANEWN"
   },
   "source": [
    "#Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTmX_141EwZR"
   },
   "source": [
    "Here copying the data from **df** to **dataF** and creating new variable with same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:22.931534Z",
     "start_time": "2022-04-20T03:47:22.924428Z"
    },
    "id": "a_fg58OnRuMx"
   },
   "outputs": [],
   "source": [
    "#copying data from original variable to another varible.\n",
    "\n",
    "dataF = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:23.074140Z",
     "start_time": "2022-04-20T03:47:23.059599Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "Y8nVur4mR5Hi",
    "outputId": "2b894f96-6e1a-4a66-e7bf-0353f5d0a86d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing top 3 rows in the data set\n",
    "\n",
    "dataF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:47:23.091812Z",
     "start_time": "2022-04-20T03:47:23.076616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:28:46.519321Z",
     "start_time": "2022-04-19T12:28:44.902919Z"
    },
    "id": "C13nkq6e5c4P"
   },
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords            ## it will check stop words present in data\n",
    "from nltk.stem.porter import PorterStemmer   ## it will use for stemmming the words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T07:06:14.892260Z",
     "start_time": "2022-04-16T07:06:14.890173Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T07:06:15.585003Z",
     "start_time": "2022-04-16T07:06:14.943569Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1ee3083fd02b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m#For expanding Contractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "def con(words):\n",
    "    return [contractions.fix(word) for word in words.split()]    #For expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:30.271965Z",
     "start_time": "2022-03-05T05:04:30.267829Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataF['Contractions']=dataF['tweet'].apply(lambda x: con(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:30.995790Z",
     "start_time": "2022-03-05T05:04:30.977843Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:31.708535Z",
     "start_time": "2022-03-05T05:04:31.705355Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install TextBlob\n",
    "#from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.249394Z",
     "start_time": "2022-03-05T05:04:32.245934Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gingerit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.391937Z",
     "start_time": "2022-03-05T05:04:32.388591Z"
    }
   },
   "outputs": [],
   "source": [
    "#from gingerit.gingerit import GingerIt\n",
    "#parser = GingerIt()\n",
    "#tweet=parser.parse(dataF['tweet'][23])\n",
    "#tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:32.503268Z",
     "start_time": "2022-03-05T05:04:32.500517Z"
    }
   },
   "outputs": [],
   "source": [
    "#blob=TextBlob(dataF['tweet'][23])\n",
    "#blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:04:33.089007Z",
     "start_time": "2022-03-05T05:04:33.079920Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Expanding whatsapp language slangs\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"./HateSpeechNew/PreProcessing/sms_slang_translator-master/slang.txt\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if str(_str).upper() == row[0]:\n",
    "                    # If match found replace it with its appropriate phrase in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    return ' '.join(user_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.087982Z",
     "start_time": "2022-03-05T05:04:33.545921Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataF['ExpandedSlangs']=dataF['tweet'].apply(lambda x: translator(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.127335Z",
     "start_time": "2022-03-05T05:05:28.118138Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.175036Z",
     "start_time": "2022-03-05T05:05:28.152537Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'con' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-20407348bd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ExpandedSlangs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Contractions for expanded slangs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3192\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-20407348bd53>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ExpandedSlangs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Contractions for expanded slangs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'con' is not defined"
     ]
    }
   ],
   "source": [
    "dataF['Preprocessed_Initial']=dataF['ExpandedSlangs'].apply(lambda x: con(x)) #Contractions for expanded slangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:03.502948Z",
     "start_time": "2022-03-05T05:07:03.483677Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:03.996685Z",
     "start_time": "2022-03-05T05:07:03.993840Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJO5rARp8Bb2",
    "outputId": "fc26ef1f-29b7-4db9-db52-9bcc2a77cbd2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.121200Z",
     "start_time": "2022-03-05T05:07:04.118575Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctTSRXQR0vO4",
    "outputId": "1a59d48e-acec-4839-8f8e-bf1fc8180a6b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.244346Z",
     "start_time": "2022-03-05T05:07:04.241803Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iG6koGQnpfOF",
    "outputId": "e9572c08-72e0-4cdb-a447-f4b9fc6d3fbe"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.382959Z",
     "start_time": "2022-03-05T05:07:04.375813Z"
    },
    "id": "6146Ji8QjJPt"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the tweets.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer,word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    #for text in sentence:\n",
    "    #  if text in ['html','www','http','https','rt']:\n",
    "    #sentence= [sentence.replace(w,'') for w in sentence if w in ['html','www','http','https','rt']]\n",
    "    sentence=sentence.replace('rt',\"\")\n",
    "    sentence=sentence.replace('www',\"\")\n",
    "    sentence=sentence.replace('http',\"\")\n",
    "    sentence=sentence.replace('https',\"\")\n",
    "    sentence=sentence.replace('html',\"\")\n",
    "    sentence=sentence.replace('*',\"\")\n",
    "    sentence=sentence.replace('#',\"\")\n",
    "    cleanr = re.compile('<.?>')\n",
    "    cleantext1 = re.sub(cleanr, '', sentence)\n",
    "    cleantext = re.sub('@[^\\s]+','',cleantext1)\n",
    "    #rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', cleantext)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    tokens = tokenizer.tokenize(rem_num)\n",
    "    #tokens = word_tokenize(rem_num)\n",
    "    filtered_words = [w for w in tokens if len(w) > 2] # if not w in stopwords.words('english')]\n",
    "    #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    #lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:04.605980Z",
     "start_time": "2022-03-05T05:07:04.477566Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnDBnHcyLRZk",
    "outputId": "c3638e8c-646c-4c09-b7e2-0a8fe26246e4",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Preprocessed_Initial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Preprocessed_Initial'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-a01c6738a8ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PreprocessedTweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessed_Initial'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#dataF['tweet_prepro'] = dataF['tweet'].apply(lambda x: preprocess(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#dataF['tweet_prepro'].tail()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Preprocessed_Initial'"
     ]
    }
   ],
   "source": [
    "dataF['PreprocessedTweet']=dataF['Preprocessed_Initial'].map(lambda s:preprocess(s))\n",
    "#dataF['tweet_prepro'] = dataF['tweet'].apply(lambda x: preprocess(x))\n",
    "#dataF['tweet_prepro'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:07:05.904217Z",
     "start_time": "2022-03-05T05:07:05.886888Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:33:06.476959Z",
     "start_time": "2022-03-05T09:33:06.472222Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split(\" \",text) \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.187016Z",
     "start_time": "2022-03-05T05:19:23.336Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['final_tweet_tokens'] = dataF['PreprocessedTweet'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.188320Z",
     "start_time": "2022-03-05T05:19:23.644Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.189637Z",
     "start_time": "2022-03-05T05:19:24.031Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF['PreprocessedTweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.190956Z",
     "start_time": "2022-03-05T05:19:24.453Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.192175Z",
     "start_time": "2022-03-05T05:19:24.910Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "word_c = WordCloud(collocations=False,background_color='white',mode='RGB',scale=4).generate(str(dataF['PreprocessedTweet']))\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(18,8),facecolor='w')\n",
    "plt.imshow(word_c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.193460Z",
     "start_time": "2022-03-05T05:19:25.528Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatizing(words):\n",
    "    lemmatizer =WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.194684Z",
     "start_time": "2022-03-05T05:19:25.996Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['LemmaWords']=dataF['final_tweet_tokens'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.195891Z",
     "start_time": "2022-03-05T05:19:26.615Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.197110Z",
     "start_time": "2022-03-05T05:19:26.972Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataF['LemmaWords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.198387Z",
     "start_time": "2022-03-05T05:19:27.407Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.to_csv('Hate_spech_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.199640Z",
     "start_time": "2022-03-05T05:19:27.920Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.200844Z",
     "start_time": "2022-03-05T05:19:30.381Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.202042Z",
     "start_time": "2022-03-05T05:19:31.178Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.203266Z",
     "start_time": "2022-03-05T05:19:31.370Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['KerasTokens']=dataF['PreprocessedTweet'].apply(text_to_word_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.204501Z",
     "start_time": "2022-03-05T05:19:31.537Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.205742Z",
     "start_time": "2022-03-05T05:19:32.391Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "word_c = WordCloud(collocations=False,background_color='white',mode='RGB',scale=4).generate(str(dataF['LemmaWords']))\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(18,8),facecolor='w')\n",
    "plt.imshow(word_c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.206953Z",
     "start_time": "2022-03-05T05:19:32.619Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF['Lemma_Preprocessed']=dataF['LemmaWords'].map(lambda s:preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T05:05:28.208149Z",
     "start_time": "2022-03-05T05:19:32.830Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T08:36:34.150352Z",
     "start_time": "2022-03-15T08:36:33.698741Z"
    }
   },
   "outputs": [],
   "source": [
    "dataF = pd.read_csv(\"/home/coea/Hatespeech/preprocessing/Hate_spech_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T08:36:34.250170Z",
     "start_time": "2022-03-15T08:36:34.223693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>ExpandedSlangs</th>\n",
       "      <th>Preprocessed_Initial</th>\n",
       "      <th>PreprocessedTweet</th>\n",
       "      <th>final_tweet_tokens</th>\n",
       "      <th>LemmaWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>140</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>['!!!', 'RT', '@mayasolovely:', 'As', 'a', 'wo...</td>\n",
       "      <td>woman you should not complain about cleaning y...</td>\n",
       "      <td>['woman', 'you', 'should', 'not', 'complain', ...</td>\n",
       "      <td>['woman', 'you', 'should', 'not', 'complain', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>85</td>\n",
       "      <td>!!!!! RT @mleew17: boy Thats cold...tyga Down ...</td>\n",
       "      <td>['!!!!!', 'RT', '@mleew17:', 'boy', 'That Is',...</td>\n",
       "      <td>boy that cold tyga down bad for cuffin that ho...</td>\n",
       "      <td>['boy', 'that', 'cold', 'tyga', 'down', 'bad',...</td>\n",
       "      <td>['boy', 'that', 'cold', 'tyga', 'down', 'bad',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>120</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...</td>\n",
       "      <td>['!!!!!!!', 'RT', '@UrKindOfBrand', 'Dog', 'RT...</td>\n",
       "      <td>dog you ever fuck bitch and she sta cry you co...</td>\n",
       "      <td>['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...</td>\n",
       "      <td>['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>62</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>['!!!!!!!!!', 'RT', '@C_G_Anderson:', '@viva_b...</td>\n",
       "      <td>she look like tranny</td>\n",
       "      <td>['she', 'look', 'like', 'tranny']</td>\n",
       "      <td>['she', 'look', 'like', 'tranny']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>137</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>['!!!!!!!!!!!!!', 'RT', '@ShenikaRoberts:', 'T...</td>\n",
       "      <td>the shit you hear about might true might faker...</td>\n",
       "      <td>['the', 'shit', 'you', 'hear', 'about', 'might...</td>\n",
       "      <td>['the', 'shit', 'you', 'hear', 'about', 'might...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  length  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...     140   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      85   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     120   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      62   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     137   \n",
       "\n",
       "                                      ExpandedSlangs  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1  !!!!! RT @mleew17: boy Thats cold...tyga Down ...   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dog RT @80sbaby4life...   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "\n",
       "                                Preprocessed_Initial  \\\n",
       "0  ['!!!', 'RT', '@mayasolovely:', 'As', 'a', 'wo...   \n",
       "1  ['!!!!!', 'RT', '@mleew17:', 'boy', 'That Is',...   \n",
       "2  ['!!!!!!!', 'RT', '@UrKindOfBrand', 'Dog', 'RT...   \n",
       "3  ['!!!!!!!!!', 'RT', '@C_G_Anderson:', '@viva_b...   \n",
       "4  ['!!!!!!!!!!!!!', 'RT', '@ShenikaRoberts:', 'T...   \n",
       "\n",
       "                                   PreprocessedTweet  \\\n",
       "0  woman you should not complain about cleaning y...   \n",
       "1  boy that cold tyga down bad for cuffin that ho...   \n",
       "2  dog you ever fuck bitch and she sta cry you co...   \n",
       "3                               she look like tranny   \n",
       "4  the shit you hear about might true might faker...   \n",
       "\n",
       "                                  final_tweet_tokens  \\\n",
       "0  ['woman', 'you', 'should', 'not', 'complain', ...   \n",
       "1  ['boy', 'that', 'cold', 'tyga', 'down', 'bad',...   \n",
       "2  ['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...   \n",
       "3                  ['she', 'look', 'like', 'tranny']   \n",
       "4  ['the', 'shit', 'you', 'hear', 'about', 'might...   \n",
       "\n",
       "                                          LemmaWords  \n",
       "0  ['woman', 'you', 'should', 'not', 'complain', ...  \n",
       "1  ['boy', 'that', 'cold', 'tyga', 'down', 'bad',...  \n",
       "2  ['dog', 'you', 'ever', 'fuck', 'bitch', 'and',...  \n",
       "3                  ['she', 'look', 'like', 'tranny']  \n",
       "4  ['the', 'shit', 'you', 'hear', 'about', 'might...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-05T09:33:35.291436Z",
     "start_time": "2022-03-05T09:33:35.287464Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T08:36:39.761456Z",
     "start_time": "2022-03-15T08:36:38.002766Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T03:37:56.594816Z",
     "start_time": "2022-03-07T03:37:56.228628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 590)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=50, max_features=1000, stop_words=None,tokenizer=tokenize,analyzer='word')\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataF['PreprocessedTweet'].values.astype('U'))\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In 86 start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:55.753511Z",
     "start_time": "2022-04-11T11:13:53.665076Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.read_csv('/home/coea/Hatespeech/TF-IDF/TF-IDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:55.814961Z",
     "start_time": "2022-04-11T11:13:55.812305Z"
    }
   },
   "outputs": [],
   "source": [
    "# tfidf_df = pd.DataFrame(tfidf_df.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:55.862500Z",
     "start_time": "2022-04-11T11:13:55.859810Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T11:13:55.886778Z",
     "start_time": "2022-04-11T11:13:55.883428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 589)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 33s, sys: 1.06 s, total: 3min 34s\n",
      "Wall time: 17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3628394, 5058200)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_tweet = dataF['Lemma_Preprocessed'].apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            vector_size=500, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 32, # no.of cores\n",
    "            seed = 34\n",
    ") \n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(dataF['Lemma_Preprocessed']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 500)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 500)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 500 )\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pandas==0.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/dstuser/anaconda3/lib/python3.7/site-packages (4.32.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LabeledSentence' from 'gensim.models.doc2vec' (/home/dstuser/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-8d3be766ad86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#tqdm.pandas(desc=\"progress-bar\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabeledSentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LabeledSentence' from 'gensim.models.doc2vec' (/home/dstuser/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "#tqdm.pandas(desc=\"progress-bar\") \n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(TaggedDocument(s, [\"tweet_\" + str(i)]))\n",
    "    return output\n",
    "\n",
    "labeled_tweets = add_label(tokenized_tweet) # label all the tweets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time \n",
    "model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model\n",
    "                                  dm_mean=1, # dm_mean = 1 for using mean of the context word vectors\n",
    "                                  vector_size=1000, # no. of desired features\n",
    "                                  window=5, # width of the context window                                  \n",
    "                                  negative=7, # if > 0 then negative sampling will be used\n",
    "                                  min_count=5, # Ignores all words with total frequency lower than 5.                                  \n",
    "                                  workers=32, # no. of cores                                  \n",
    "                                  alpha=0.1, # learning rate                                  \n",
    "                                  seed = 23, # for reproducibility\n",
    "                                 ) \n",
    "\n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n",
    "\n",
    "model_d2v.train(labeled_tweets, total_examples= len(dataF['Lemma_Preprocessed']), epochs=15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docvec_arrays = np.zeros((len(tokenized_tweet), 1000)) \n",
    "for i in range(len(dataF)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,1000))    \n",
    "\n",
    "docvec_df = pd.DataFrame(docvec_arrays) \n",
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:48:24.277743Z",
     "start_time": "2022-04-20T03:47:31.643040Z"
    }
   },
   "outputs": [],
   "source": [
    "lw = pd.read_excel('LIWC2015 Results (HateSpeech)-Apr13-2022.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:48:24.620419Z",
     "start_time": "2022-04-20T03:48:24.597533Z"
    }
   },
   "outputs": [],
   "source": [
    "lw.drop('Source (A)',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:48:24.924967Z",
     "start_time": "2022-04-20T03:48:24.920306Z"
    }
   },
   "outputs": [],
   "source": [
    "lw = lw.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:48:25.229621Z",
     "start_time": "2022-04-20T03:48:25.223312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 93)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:48:25.648666Z",
     "start_time": "2022-04-20T03:48:25.541545Z"
    },
    "id": "53bWppiTiWGx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(lw.values, dataF['class'].values, \\\n",
    "                                                    stratify = dataF['class'].values,shuffle = True,\\\n",
    "                                                    random_state=1,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T03:48:26.501059Z",
     "start_time": "2022-04-20T03:48:25.943346Z"
    },
    "id": "Obk37BzJsRwW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing necessary librarys machine learning algorthms\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# PNN algorithm not imported\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#importing all necessary metrics\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T12:29:44.522510Z",
     "start_time": "2022-04-19T12:29:44.518685Z"
    }
   },
   "outputs": [],
   "source": [
    "technique='LIWC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T08:43:52.273204Z",
     "start_time": "2022-04-13T08:37:54.766431Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.774, Val_Acc: 0.775, est=0.774, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 2, acc=0.774, Val_Acc: 0.775, est=0.774, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 3, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 4, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 5, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 6, acc=0.774, Val_Acc: 0.775, est=0.774, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 7, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 8, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 9, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold:10, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'penalty': 'l2'}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    LR_model = LogisticRegression(random_state=1,C=100)\n",
    "    \n",
    "    # define gridsearch CV\n",
    "    p_grid = {'penalty': ['l1', 'l2']}\n",
    "    \n",
    "    clf = GridSearchCV(estimator = LR_model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T08:43:59.771691Z",
     "start_time": "2022-04-13T08:43:59.736351Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.774 +/- 0.000\n",
      "\n",
      "CV accuracy: 0.774 +/- 0.000\n",
      "\n",
      "CV micro F1: 0.774 +/- 0.000\n",
      "\n",
      "CV macro F1: 0.291 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Hate speech Recall: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.774 +/- 0.000\n",
      "\n",
      "CV Class : Offensive language Recall: 1.000 +/- 0.000\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.873 +/- 0.000\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Neither Recall: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Neither F1 score: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T08:44:05.309411Z",
     "start_time": "2022-04-13T08:44:05.233877Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIWC_LR.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "1  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "2  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "3  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "4  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0                  0.0                286.0                      0.774259   \n",
       "1                  0.0                286.0                      0.774259   \n",
       "2                  0.0                286.0                      0.774259   \n",
       "3                  0.0                286.0                      0.774259   \n",
       "4                  0.0                286.0                      0.774259   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                        1.0                    0.872769   \n",
       "1                        1.0                    0.872769   \n",
       "2                        1.0                    0.872769   \n",
       "3                        1.0                    0.872769   \n",
       "4                        1.0                    0.872769   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.774259             0.0   \n",
       "1                      3838.0           0.774259             0.0   \n",
       "2                      3838.0           0.774259             0.0   \n",
       "3                      3838.0           0.774259             0.0   \n",
       "4                      3838.0           0.774259             0.0   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0              0.0            833.0  \n",
       "1              0.0            833.0  \n",
       "2              0.0            833.0  \n",
       "3              0.0            833.0  \n",
       "4              0.0            833.0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"LR.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHwC6nvnrbkq"
   },
   "source": [
    "## Training SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-20T04:03:55.076Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = SVC(random_state=42,max_iter=-1)\n",
    "    param_range = [0.1,0.5,1.0]\n",
    "    \n",
    "    p_grid =  {'C': param_range, \n",
    "               #'loss':['hinge', 'squared_hinge'],\n",
    "               'kernel':['linear','poly', 'rbf', 'sigmoid']\n",
    "               }   \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-20T04:03:55.613Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-20T04:03:56.151Z"
    }
   },
   "outputs": [],
   "source": [
    "model=\"SVM.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIXIwRTzeUiD"
   },
   "source": [
    "## Training DescisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T15:40:30.619496Z",
     "start_time": "2022-04-13T08:46:24.670535Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.774, Val_Acc: 0.775, est=0.774, cfg={'min_samples_leaf': 2, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'log2', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 2, acc=0.774, Val_Acc: 0.775, est=0.774, cfg={'min_samples_leaf': 2, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 3, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'min_samples_leaf': 8, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 4, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'min_samples_leaf': 2, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 5, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'min_samples_leaf': 8, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 6, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'min_samples_leaf': 2, 'criterion': 'entropy', 'min_samples_split': 2, 'max_features': 'log2', 'max_depth': 4}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909559548176787\n",
      "Fold: 7, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'min_samples_leaf': 2, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'log2', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 8, acc=0.775, Val_Acc: 0.774, est=0.774, cfg={'min_samples_leaf': 2, 'criterion': 'entropy', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7748638289287876\n",
      "Test_macro_f1: 0.29344473215587735\n",
      "Fold: 9, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'min_samples_leaf': 8, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold:10, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'min_samples_leaf': 5, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "CPU times: user 1h 48min 12s, sys: 9min 24s, total: 1h 57min 37s\n",
      "Wall time: 6h 54min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "#writing a loop for nested 10-fold cross-validation & parameter tuning\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    model = DecisionTreeClassifier(random_state=1)\n",
    "    \n",
    "    params_range = np.arange(2, 20, 2)\n",
    "    min_split_param = np.arange(2, 10)\n",
    "    \n",
    "    # define gridsearch CV\n",
    "    p_grid={\n",
    "            'min_samples_split': min_split_param,\n",
    "            'criterion':['gini','entropy'], \n",
    "            'max_depth': np.arange(2, 250, 2),\n",
    "            'max_features':['sqrt','log2'],\n",
    "            'min_samples_leaf': np.arange(2, 10)\n",
    "               \n",
    "               } \n",
    "\n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T15:40:30.802654Z",
     "start_time": "2022-04-13T15:40:30.778822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.774 +/- 0.000\n",
      "\n",
      "CV accuracy: 0.774 +/- 0.000\n",
      "\n",
      "CV micro F1: 0.774 +/- 0.000\n",
      "\n",
      "CV macro F1: 0.291 +/- 0.001\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Hate speech Recall: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.774 +/- 0.000\n",
      "\n",
      "CV Class : Offensive language Recall: 1.000 +/- 0.000\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.873 +/- 0.000\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.075 +/- 0.225\n",
      "\n",
      "CV Class : Neither Recall: 0.000 +/- 0.001\n",
      "\n",
      "CV Class : Neither F1 score: 0.001 +/- 0.002\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T15:40:30.999066Z",
     "start_time": "2022-04-13T15:40:30.954876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIWC_DT.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "1  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "2  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "3  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "4  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0                  0.0                286.0                      0.774259   \n",
       "1                  0.0                286.0                      0.774259   \n",
       "2                  0.0                286.0                      0.774259   \n",
       "3                  0.0                286.0                      0.774259   \n",
       "4                  0.0                286.0                      0.774259   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                        1.0                    0.872769   \n",
       "1                        1.0                    0.872769   \n",
       "2                        1.0                    0.872769   \n",
       "3                        1.0                    0.872769   \n",
       "4                        1.0                    0.872769   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.774259             0.0   \n",
       "1                      3838.0           0.774259             0.0   \n",
       "2                      3838.0           0.774259             0.0   \n",
       "3                      3838.0           0.774259             0.0   \n",
       "4                      3838.0           0.774259             0.0   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0              0.0            833.0  \n",
       "1              0.0            833.0  \n",
       "2              0.0            833.0  \n",
       "3              0.0            833.0  \n",
       "4              0.0            833.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"DT.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfzInD9wC1jl"
   },
   "source": [
    "## Training RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T07:27:48.850553Z",
     "start_time": "2022-04-13T15:40:31.142855Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.774, Val_Acc: 0.775, est=0.774, cfg={'n_estimators': 50, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 2, acc=0.774, Val_Acc: 0.775, est=0.774, cfg={'n_estimators': 50, 'criterion': 'gini', 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 17}\n",
      "Test_micro_f1: 0.7738551543272141\n",
      "Test_macro_f1: 0.2908374085446757\n",
      "Fold: 3, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'n_estimators': 50, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 4, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 9, 'max_features': 'sqrt', 'max_depth': 17}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 5, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'n_estimators': 50, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 6, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'n_estimators': 50, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 7, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'n_estimators': 200, 'criterion': 'gini', 'min_samples_split': 7, 'max_features': 'log2', 'max_depth': 22}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 8, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'n_estimators': 50, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold: 9, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'n_estimators': 50, 'criterion': 'gini', 'min_samples_split': 7, 'max_features': 'sqrt', 'max_depth': 17}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n",
      "Fold:10, acc=0.774, Val_Acc: 0.774, est=0.774, cfg={'n_estimators': 50, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'max_depth': 2}\n",
      "Test_micro_f1: 0.7742586241678434\n",
      "Test_macro_f1: 0.2909228728444192\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    #cv_inner = StratifiedKFold(n_splits=5, random_state=42)\n",
    "    p_grid = {'n_estimators': [50,100,200],\n",
    "              'max_depth' : np.arange(2, 200, 5),\n",
    "              'min_samples_split': np.arange(2, 10),\n",
    "              'max_features': ['sqrt', 'log2'],\n",
    "              'criterion' :['gini', 'entropy'],\n",
    "              \n",
    "                }\n",
    "    \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T07:27:51.415366Z",
     "start_time": "2022-04-15T07:27:49.017739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.774 +/- 0.000\n",
      "\n",
      "CV accuracy: 0.774 +/- 0.000\n",
      "\n",
      "CV micro F1: 0.774 +/- 0.000\n",
      "\n",
      "CV macro F1: 0.291 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Hate speech Recall: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.774 +/- 0.000\n",
      "\n",
      "CV Class : Offensive language Recall: 1.000 +/- 0.000\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.873 +/- 0.000\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Neither Recall: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Neither F1 score: 0.000 +/- 0.000\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-15T07:27:51.606923Z",
     "start_time": "2022-04-15T07:27:51.564445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIWC_RF.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.773855</td>\n",
       "      <td>0.773855</td>\n",
       "      <td>0.290837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774168</td>\n",
       "      <td>0.999479</td>\n",
       "      <td>0.872512</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872769</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "1  0.773855  0.773855  0.290837                    0.0                 0.0   \n",
       "2  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "3  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "4  0.774259  0.774259  0.290923                    0.0                 0.0   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0                  0.0                286.0                      0.774259   \n",
       "1                  0.0                286.0                      0.774168   \n",
       "2                  0.0                286.0                      0.774259   \n",
       "3                  0.0                286.0                      0.774259   \n",
       "4                  0.0                286.0                      0.774259   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   1.000000                    0.872769   \n",
       "1                   0.999479                    0.872512   \n",
       "2                   1.000000                    0.872769   \n",
       "3                   1.000000                    0.872769   \n",
       "4                   1.000000                    0.872769   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.774259             0.0   \n",
       "1                      3838.0           0.774168             0.0   \n",
       "2                      3838.0           0.774259             0.0   \n",
       "3                      3838.0           0.774259             0.0   \n",
       "4                      3838.0           0.774259             0.0   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0              0.0            833.0  \n",
       "1              0.0            833.0  \n",
       "2              0.0            833.0  \n",
       "3              0.0            833.0  \n",
       "4              0.0            833.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"RF.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwmNqojeGz5s"
   },
   "source": [
    "## Training Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-19T08:36:37.571Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = MLPClassifier(random_state=1,max_iter=1000)\n",
    "\n",
    "    p_grid = { 'solver':['sgd', 'adam'],\n",
    "               'activation':['logistic', 'tanh', 'relu'],\n",
    "               'alpha':[0.01, 0.1],\n",
    "               'hidden_layer_sizes':[(50,),(100,)]\n",
    "             }    \n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=20)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-19T08:36:38.192Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-19T08:36:39.054Z"
    }
   },
   "outputs": [],
   "source": [
    "model=\"MLP.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-03-22T05:47:15.289Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "    \n",
    "    model = GradientBoostingClassifier(random_state=1)\n",
    "    \n",
    "    p_grid = {  \"learning_rate\": [0.001, 0.01,0.1],\n",
    "                \"min_samples_split\": np.arange(2, 10),\n",
    "                \"max_depth\":np.arange(2, 200, 5),\n",
    "                \"n_estimators\": [50, 100, 200],\n",
    "                }\n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"GBT.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T05:18:42.866076Z",
     "start_time": "2022-03-21T06:13:23.490Z"
    }
   },
   "outputs": [],
   "source": [
    "# t_test = pd.DataFrame(t_test_tfidf_accuracy_gds,columns = ['Fold1','Fold2','Fold3','Fold4','Fold5','Fold6','Fold7','Fold8','Fold9','Fold10',])\n",
    "# res_metrics = pd.DataFrame(tfidf_result_metrics_gds, columns = ['accuracy','micro_f1','macro_f1'])#,'Specificity','Precision','Sensitivity'])\n",
    "\n",
    "# t_test.to_csv('t_test_tfidf_accuracy_10fold_grid_cv.csv', index = False)\n",
    "# res_metrics.to_csv('tfidf_result_metrics_10fold_grid_cv.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T08:45:38.766674Z",
     "start_time": "2022-04-13T08:45:32.677229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, acc=0.521, Val_Acc: 0.530, est=0.511, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.5208795642525721\n",
      "Test_macro_f1: 0.312221453946851\n",
      "Fold: 2, acc=0.510, Val_Acc: 0.501, est=0.497, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.5101876134758927\n",
      "Test_macro_f1: 0.3087772428320242\n",
      "Fold: 3, acc=0.503, Val_Acc: 0.514, est=0.493, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.5027234214242485\n",
      "Test_macro_f1: 0.3145747680778053\n",
      "Fold: 4, acc=0.496, Val_Acc: 0.495, est=0.493, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.4956626992132338\n",
      "Test_macro_f1: 0.30366699408265313\n",
      "Fold: 5, acc=0.521, Val_Acc: 0.523, est=0.509, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.5212830340932015\n",
      "Test_macro_f1: 0.3067506446574266\n",
      "Fold: 6, acc=0.501, Val_Acc: 0.482, est=0.490, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.5011095420617309\n",
      "Test_macro_f1: 0.3074325888700853\n",
      "Fold: 7, acc=0.496, Val_Acc: 0.522, est=0.478, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.49606616905386325\n",
      "Test_macro_f1: 0.30526250075829436\n",
      "Fold: 8, acc=0.533, Val_Acc: 0.546, est=0.524, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.5333871293120839\n",
      "Test_macro_f1: 0.31321997977375804\n",
      "Fold: 9, acc=0.505, Val_Acc: 0.517, est=0.485, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.505144240468025\n",
      "Test_macro_f1: 0.3100409480776292\n",
      "Fold:10, acc=0.499, Val_Acc: 0.526, est=0.490, cfg={'var_smoothing': 1e-05}\n",
      "Test_micro_f1: 0.4992939277788985\n",
      "Test_macro_f1: 0.3054267394661419\n"
     ]
    }
   ],
   "source": [
    "#Define both cross-validation objects (inner & outer)\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    \n",
    "vali_scores = []\n",
    "accuracy = []\n",
    "micro_f1 = []\n",
    "macro_f1 = []\n",
    "cm_score = []\n",
    "\n",
    "rac_precision=[]\n",
    "rac_recall=[]\n",
    "rac_f1score=[]\n",
    "rac_support=[]\n",
    "\n",
    "se_precision=[]\n",
    "se_recall=[]\n",
    "se_f1score=[]\n",
    "se_support=[]\n",
    "\n",
    "nei_precision=[]\n",
    "nei_recall=[]\n",
    "nei_f1score=[]\n",
    "nei_support=[]\n",
    "\n",
    "#writing a loop for nested 10-fold cross-validation & parameter tuning\n",
    "    \n",
    "for k,(train, test) in enumerate(outer_cv.split(X_train, y_train)):\n",
    "        \n",
    "    # define gridsearch CV\n",
    "    model = GaussianNB()\n",
    "    #cv_inner = StratifiedKFold(n_splits=5, random_state=42)\n",
    "    p_grid = [\n",
    "      {'var_smoothing':[1e-9,1e-8,1e-7,1e-6,1e-5]\n",
    "       }]\n",
    "    \n",
    "    clf = GridSearchCV(estimator = model, param_grid=p_grid, scoring='f1_micro',\n",
    "                       refit=True, n_jobs=-1)\n",
    "    # fitting model on parameter search\n",
    "    gd_search = clf.fit(X_train[train], y_train[train])\n",
    "\n",
    "    # Validating model which is trained parameter search \"inner_cv\".\n",
    "    score = gd_search.score(X_train[test], y_train[test])\n",
    "    vali_scores.append(score)\n",
    "\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = gd_search.best_estimator_\n",
    "    \n",
    "    best_param = gd_search.best_params_\n",
    "    # evaluate model on the hold out dataset\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # model \"accuracy score\" from ground truth and predicted values \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    " \n",
    "    # micro_F1, marco_F1 \n",
    "    target_names=['Hate speech','Offensive language','neither']\n",
    "    report = classification_report(y_test, y_pred,target_names=target_names,output_dict=True)\n",
    "    \n",
    "    ### individual class metrics\n",
    "    \n",
    "    \n",
    "    rac_precision.append(report['Hate speech']['precision'])\n",
    "    rac_recall.append(report['Hate speech']['recall'])\n",
    "    rac_f1score.append(report['Hate speech']['f1-score'])\n",
    "    rac_support.append(report['Hate speech']['support'])\n",
    "    \n",
    "    se_precision.append(report['Offensive language']['precision'])\n",
    "    se_recall.append(report['Offensive language']['recall'])\n",
    "    se_f1score.append(report['Offensive language']['f1-score'])\n",
    "    se_support.append(report['Offensive language']['support'])\n",
    "    \n",
    "    nei_precision.append(report['neither']['precision'])\n",
    "    nei_recall.append(report['neither']['recall'])\n",
    "    nei_f1score.append(report['neither']['f1-score'])\n",
    "    nei_support.append(report['neither']['support'])\n",
    "    \n",
    "    \n",
    "    # Since it is multiclass classification printing \"micro_F1_score\"\n",
    "    micro_F1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Since it is multiclass classification printing \"macro_F1_score\"\n",
    "    macro_F1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # taking \"confusion metrics\" from true and predicted values\n",
    "    cm = confusion_matrix(y_test, y_pred)               ## Confusion Matrix\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    micro_f1.append(micro_F1)\n",
    "    macro_f1.append(macro_F1)\n",
    "    cm_score.append(cm)\n",
    "          \n",
    "    print('Fold:%2d, acc=%.3f, Val_Acc: %.3f, est=%.3f, cfg=%s' % (k+1,\n",
    "                            acc, score, gd_search.best_score_, gd_search.best_params_))\n",
    "    print('Test_micro_f1:', micro_F1)\n",
    "    print('Test_macro_f1:', macro_F1)\n",
    "    \n",
    "    #report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "    \n",
    "    #print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T08:45:38.943917Z",
     "start_time": "2022-04-13T08:45:38.918062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV vali_accuracy: 0.516 +/- 0.018\n",
      "\n",
      "CV accuracy: 0.509 +/- 0.012\n",
      "\n",
      "CV micro F1: 0.509 +/- 0.012\n",
      "\n",
      "CV macro F1: 0.309 +/- 0.003\n",
      "\n",
      "---------------------- CLASS : Hate speech Metrics ------------------------\n",
      "\n",
      "CV Class : Hate speech precision: 0.067 +/- 0.002\n",
      "\n",
      "CV Class : Hate speech Recall: 0.312 +/- 0.024\n",
      "\n",
      "CV Class : Hate speech F1 score: 0.110 +/- 0.004\n",
      "\n",
      "CV Class : Hate speech Support: 286.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Offensive language Metrics ------------------------\n",
      "\n",
      "CV Class : Offensive language precision: 0.773 +/- 0.002\n",
      "\n",
      "CV Class : Offensive language Recall: 0.608 +/- 0.018\n",
      "\n",
      "CV Class : Offensive language F1 score: 0.681 +/- 0.011\n",
      "\n",
      "CV Class : Offensive language Support: 3838.000 +/- 0.000\n",
      "\n",
      "---------------------- CLASS : Neither Metrics ------------------------\n",
      "\n",
      "CV Class : Neither precision: 0.161 +/- 0.006\n",
      "\n",
      "CV Class : Neither Recall: 0.117 +/- 0.010\n",
      "\n",
      "CV Class : Neither F1 score: 0.135 +/- 0.008\n",
      "\n",
      "CV Class : Neither Support: 833.000 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "# summarize the estimated performance of the model            \n",
    "print('\\nCV vali_accuracy: %.3f +/- %.3f' % (np.mean(vali_scores), np.std(vali_scores)))\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "print('\\nCV micro F1: %.3f +/- %.3f' % (np.mean(micro_f1), np.std(micro_f1)))\n",
    "print('\\nCV macro F1: %.3f +/- %.3f' % (np.mean(macro_f1), np.std(macro_f1)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Hate speech Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Hate speech precision: %.3f +/- %.3f' % (np.mean(rac_precision), np.std(rac_precision)))\n",
    "print('\\nCV Class : Hate speech Recall: %.3f +/- %.3f' % (np.mean(rac_recall), np.std(rac_recall)))\n",
    "print('\\nCV Class : Hate speech F1 score: %.3f +/- %.3f' % (np.mean(rac_f1score), np.std(rac_f1score)))\n",
    "print('\\nCV Class : Hate speech Support: %.3f +/- %.3f' % (np.mean(rac_support), np.std(rac_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Offensive language Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Offensive language precision: %.3f +/- %.3f' % (np.mean(se_precision), np.std(se_precision)))\n",
    "print('\\nCV Class : Offensive language Recall: %.3f +/- %.3f' % (np.mean(se_recall), np.std(se_recall)))\n",
    "print('\\nCV Class : Offensive language F1 score: %.3f +/- %.3f' % (np.mean(se_f1score), np.std(se_f1score)))\n",
    "print('\\nCV Class : Offensive language Support: %.3f +/- %.3f' % (np.mean(se_support), np.std(se_support)))\n",
    "\n",
    "print(\"\\n---------------------- CLASS : Neither Metrics ------------------------\")\n",
    "\n",
    "print('\\nCV Class : Neither precision: %.3f +/- %.3f' % (np.mean(nei_precision), np.std(nei_precision)))\n",
    "print('\\nCV Class : Neither Recall: %.3f +/- %.3f' % (np.mean(nei_recall), np.std(nei_recall)))\n",
    "print('\\nCV Class : Neither F1 score: %.3f +/- %.3f' % (np.mean(nei_f1score), np.std(nei_f1score)))\n",
    "print('\\nCV Class : Neither Support: %.3f +/- %.3f' % (np.mean(nei_support), np.std(nei_support)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T08:45:39.133399Z",
     "start_time": "2022-04-13T08:45:39.084843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIWC_NB.csv\n",
      "(10, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>microF1</th>\n",
       "      <th>macro F1</th>\n",
       "      <th>Hate speech-Precision</th>\n",
       "      <th>Hate speech-Recall</th>\n",
       "      <th>Hate speech-F1score</th>\n",
       "      <th>Hate speech-Support</th>\n",
       "      <th>Offensive language-Precision</th>\n",
       "      <th>Offensive language-Recall</th>\n",
       "      <th>Offensive language-F1score</th>\n",
       "      <th>Offensive language-Support</th>\n",
       "      <th>Neither-Precision</th>\n",
       "      <th>Neither-Recall</th>\n",
       "      <th>Neither-F1score</th>\n",
       "      <th>Neither-Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.520880</td>\n",
       "      <td>0.520880</td>\n",
       "      <td>0.312221</td>\n",
       "      <td>0.069615</td>\n",
       "      <td>0.297203</td>\n",
       "      <td>0.112807</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.772947</td>\n",
       "      <td>0.625326</td>\n",
       "      <td>0.691344</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.772947</td>\n",
       "      <td>0.116447</td>\n",
       "      <td>0.132514</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.510188</td>\n",
       "      <td>0.510188</td>\n",
       "      <td>0.308777</td>\n",
       "      <td>0.064996</td>\n",
       "      <td>0.290210</td>\n",
       "      <td>0.106206</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.772832</td>\n",
       "      <td>0.610735</td>\n",
       "      <td>0.682288</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.772832</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.137838</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.502723</td>\n",
       "      <td>0.502723</td>\n",
       "      <td>0.314575</td>\n",
       "      <td>0.068862</td>\n",
       "      <td>0.321678</td>\n",
       "      <td>0.113440</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.774533</td>\n",
       "      <td>0.594320</td>\n",
       "      <td>0.672564</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.774533</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.157720</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.495663</td>\n",
       "      <td>0.495663</td>\n",
       "      <td>0.303667</td>\n",
       "      <td>0.064449</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>0.107577</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.771584</td>\n",
       "      <td>0.591454</td>\n",
       "      <td>0.669617</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.771584</td>\n",
       "      <td>0.112845</td>\n",
       "      <td>0.133808</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.521283</td>\n",
       "      <td>0.521283</td>\n",
       "      <td>0.306751</td>\n",
       "      <td>0.063049</td>\n",
       "      <td>0.276224</td>\n",
       "      <td>0.102664</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.770064</td>\n",
       "      <td>0.630016</td>\n",
       "      <td>0.693035</td>\n",
       "      <td>3838.0</td>\n",
       "      <td>0.770064</td>\n",
       "      <td>0.104442</td>\n",
       "      <td>0.124553</td>\n",
       "      <td>833.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy   microF1  macro F1  Hate speech-Precision  Hate speech-Recall  \\\n",
       "0  0.520880  0.520880  0.312221               0.069615            0.297203   \n",
       "1  0.510188  0.510188  0.308777               0.064996            0.290210   \n",
       "2  0.502723  0.502723  0.314575               0.068862            0.321678   \n",
       "3  0.495663  0.495663  0.303667               0.064449            0.325175   \n",
       "4  0.521283  0.521283  0.306751               0.063049            0.276224   \n",
       "\n",
       "   Hate speech-F1score  Hate speech-Support  Offensive language-Precision  \\\n",
       "0             0.112807                286.0                      0.772947   \n",
       "1             0.106206                286.0                      0.772832   \n",
       "2             0.113440                286.0                      0.774533   \n",
       "3             0.107577                286.0                      0.771584   \n",
       "4             0.102664                286.0                      0.770064   \n",
       "\n",
       "   Offensive language-Recall  Offensive language-F1score  \\\n",
       "0                   0.625326                    0.691344   \n",
       "1                   0.610735                    0.682288   \n",
       "2                   0.594320                    0.672564   \n",
       "3                   0.591454                    0.669617   \n",
       "4                   0.630016                    0.693035   \n",
       "\n",
       "   Offensive language-Support  Neither-Precision  Neither-Recall  \\\n",
       "0                      3838.0           0.772947        0.116447   \n",
       "1                      3838.0           0.772832        0.122449   \n",
       "2                      3838.0           0.774533        0.142857   \n",
       "3                      3838.0           0.771584        0.112845   \n",
       "4                      3838.0           0.770064        0.104442   \n",
       "\n",
       "   Neither-F1score  Neither-Support  \n",
       "0         0.132514            833.0  \n",
       "1         0.137838            833.0  \n",
       "2         0.157720            833.0  \n",
       "3         0.133808            833.0  \n",
       "4         0.124553            833.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=\"NB.csv\"\n",
    "filename=technique+\"_\"+model\n",
    "print(filename)\n",
    "\n",
    "lr_op=[]\n",
    "\n",
    "lr_op=[accuracy,micro_f1,macro_f1,rac_precision,rac_recall,rac_f1score,rac_support,\\\n",
    "       se_precision,se_recall,se_f1score,se_support,\\\n",
    "      se_precision,nei_recall,nei_f1score,nei_support]\n",
    "\n",
    "lr_op=np.array(lr_op).T\n",
    "\n",
    "print(lr_op.shape)\n",
    "\n",
    "lr_df=pd.DataFrame(lr_op,columns=[\"accuracy\",\"microF1\",\"macro F1\",\\\n",
    "                                  \"Hate speech-Precision\",\"Hate speech-Recall\",\"Hate speech-F1score\",\"Hate speech-Support\",\\\n",
    "                                 \"Offensive language-Precision\",\"Offensive language-Recall\",\"Offensive language-F1score\",\"Offensive language-Support\",\\\n",
    "                                 \"Neither-Precision\",\"Neither-Recall\",\"Neither-F1score\",\"Neither-Support\"])\n",
    "lr_df.to_csv(filename,index=True)\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hate_speech_TF-IDF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "211px",
    "width": "228px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
